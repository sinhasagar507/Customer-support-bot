{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Classification With PyTorch\n",
    "Previously, my focus in the notebooks was on obtaining labeled data for my chatbot. However, this current notebook is centered around utilizing PyTorch for the classification of intents within fresh, unseen user-generated data. The model has transitioned to a supervised learning approach, leveraging the labels derived from the unsupervised learning conducted in the preceding notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RASA Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasa trains this intent classification step with SVM and GridsearchCV because they can try different configurations ([source](https://medium.com/bhavaniravi/intent-classification-demystifying-rasanlu-part-4-685fc02f5c1d)). When deploying preprocessing pipeline should remain same between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.2.2\n",
      "Numpy: 1.26.4\n",
      "Sklearn: 1.4.2\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'objects/train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# from torchtext.data.utils import get_tokenizer\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# from torch.nn.utils.rnn import pad_sequence\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# from torch.utils.data import Dataset, DataLoader, TensorDataset\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Reading in training data\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobjects/train.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'objects/train.pkl'"
     ]
    }
   ],
   "source": [
    "# Standard \n",
    "import collections\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "import numpy as np\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "\n",
    "# Machine Learning\n",
    "import sklearn\n",
    "print(f\"Sklearn: {sklearn.__version__}\")\n",
    "\n",
    "# Deep Learning\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Preprocessing and Torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# from torchtext.data import get_tokenizer\n",
    "\n",
    "# Reading in training data\n",
    "train = pd.read_pickle('objects/train.pkl')\n",
    "print(f'Training data: {train.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train' is a DataFrame containing 'Utterance' and 'Intent' columns\n",
    "\n",
    "# Tokenize the text data using PyTorch's tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "X_train = [tokenizer(text) for text in train['Utterance']]\n",
    "y_train = train['Intent']\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, \n",
    "                                                  shuffle=True, stratify=y_train, random_state=7)\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_val = torch.tensor(y_val.values)\n",
    "\n",
    "print(f'\\nShape checks:\\nX_train: {len(X_train)} X_val: {len(X_val)}\\ny_train: {len(y_train)} y_val: {len(y_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchtext tokenizer \n",
    "- Add description later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of Action\n",
    "- Prepare the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Sagar Study\\\\ML and Learning\\\\Projects\\\\customer-support-bot\\\\amazon_customer_support\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps taken\n",
    "    -   the words would involve creating a vocabulary dictionary to map words to indices \n",
    "    -   For each sequence, the words are converted into their corresponding indices based on the word dictionary \n",
    "    - When feeding sentences into the model, ensure a consistent sequence length is crucial \n",
    "    - To achieve this, sequences are padded with zeros until they reach the length of the longest sequence \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize tge text data using PyTorch's tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Tokenize and encode the text data  \n",
    "X_train = [tokenizer(text) for text in train_df[\"Utterance\"]]\n",
    "y_train = train_df[\"Intent\"]\n",
    "\n",
    "# Split the data into train and validation sets \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size=0.3, shuffle=True, stratify = y_train, random_state = SEED_VALUE)\n",
    "\n",
    "# Convert labels to PyTorch Tensors \n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_val = torch.tensor(y_val.values)\n",
    "\n",
    "print(f\"\\nShape checks:\\nX_train: {len(X_train)} X_val: {len(X_val)} y_train: {len(y_train)} y_val: {len(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Write a function to average all of the word vectors in a given paragraph\n",
    "def get_sentence_vector(sentence, word_vectors):\n",
    "    \n",
    "    # Initialize the vector as all zeros\n",
    "    vector = np.zeros(word_vectors.vector_size)\n",
    "    \n",
    "    num_words = 0\n",
    "    # Loop over each word in the sentence\n",
    "    for word in sentence:\n",
    "        if word in word_vectors:\n",
    "            vector += word_vectors[word]\n",
    "            num_words += 1\n",
    "    # Average the vector\n",
    "    if num_words:\n",
    "        vector /= num_words\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocab mapping for words \n",
    "word_to_idx = {word: index+1 for index, (word, _) in enumerate(t.word_index.items())}\n",
    "index_to_word = {index+1: word for index, (word, _) in enumerate(t.word_index.items())}\n",
    "\n",
    "# Convert documents to sequences of indices \n",
    "indexed_X_train = [[word_to_index[word] for word in doc] for doc in X_train]\n",
    "indexed_X_val = [[word_to_idx[word] for word in doc] for doc in X_val]\n",
    "\n",
    "# Pad sequences to a common length \n",
    "max_length = min(max(len(doc) for doc in indexed_X_train), 100)\n",
    "padded_X_train = pad_sequence([torch.tensor(doc[ :max_length]) + [0]*(max_length - len(doc) for doc in indexed_X_train], batch_first=True)\n",
    "padded_X_val = pad_sequence([torch.tensor(doc[: max_length] + [0] * (max_length - len(doc)) for doc in indexed_X_val], batch_first=True)\n",
    "                            \n",
    "# Define vocabulary size \n",
    "vocab_size = len(word_to_index) + 1 \n",
    "\n",
    "print(f\"Vocab size:\\n{vocab_size}\")\n",
    "print(f\"Max length:\\n{max_length}\")\n",
    "\n",
    "\n",
    "print(f\"padded_X_train\\n{padded_X_train}\")\n",
    "print(f\"padded_X_val\\n{padded_X_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example fits the Tokenizer with 5 small documents. The details of the fit Tokenizer are printed. Then the 5 documents are encoded using a word count \n",
    "\n",
    "Each document is encoded as a 9-element vector with one position for each word and the chosen encoding scheme value for each word position. In this case, a simple word count is used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models involve dealing with one-hot encoding for multiclass classification and using embeddings for document representations. Below are PyTorch specific of the provided statements\n",
    "\n",
    "If we are using Doc2Vec embeddings, how will we pass in our tweets? We may have to pass it in as full tweets. Check how we pass in the tweets. We may have to perform tokenization at a tweet level. If we pass it in, if it's Twweet 57, it will activate the node such that it gets multiplied out by the embeddings for the 57th document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are 4 different dimensionality options \n",
    "!ls models/glove.twitter.27B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we compute an index mapping of words to known embeddings by parsing the data dump of pre-trained embeddings \n",
    "- I use 50D because my X_train has a max_length of 32 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just include `weights and biases tracking` as a part of training mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Glove Embeddings \n",
    "embeddings_index = {} \n",
    "\n",
    "with open(\"models/glove.twitter.27B/glove.twitter.27B.50d.txt\", encoding=\"utf8\") as f: \n",
    "    for line in f: \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "print(\"Found %s word vectors.\"%len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can leverage our embeddings_index dictionary and our word_to_index dictionary to create an embedding matrix that we can use to initialize our embedding layer. We will use the same dimensionality as our GloVe embeddings (50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing required objects \n",
    "word_index = t.word_index \n",
    "EMBEDDING_DIM = 50 # Because we are using the 50D glove embeddings \n",
    "\n",
    "# Getting my embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, idx in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all zeros\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape, embeddings_index.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can start the modeling \n",
    "\n",
    "In a regular word embedding, the order of the embeddings in the matrix has to be setup so that it matches how the words \n",
    "\n",
    "I also made sure the order of the embeddings are the same order of the words as they are in the model \n",
    "\n",
    "Here, I also made sure that domain-specific words like customer support is in my Twitter embeddings. One example of this is 'customer' and you can clearly see that it is in indeed in my embeddings file, which is cool \n",
    "\n",
    "<img src=\"visualizations/amazon.png\" alt=\"Drawing\" style=\"width: 400px; \"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target variable \n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_valid = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the text data\n",
    "X_train = [tokenizer(text) for text in train[\"Utterance\"]]\n",
    "y_train = train[\"Intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df[\"Utterance\"], train_df[\"Intent\"], test_size=0.3,    shuffle=True, stratify=train_df[\"Intent\"], random_state=SEED_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build train and test dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch datasets from train and test sets \n",
    "train = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "valid = torch.utils.data.TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architect the Neural Network  \n",
    "- I will create a neural network with PyTorch with the output layer having the same no. of nodes as they are intents. The following is the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassificationModel(nn.Module): \n",
    "    def __init__(sel): \n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "\n",
    "        # Bidirectional LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, bidirectional=True)\n",
    "\n",
    "        # Dense layers\n",
    "        self.dense1 = nn.Linear(hidden_size * 2, dense_size)\n",
    "        self.dense2 = nn.Linear(dense_size, dense_size)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(dense_size, num_intents)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        embedded = self.embedding(input_data)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = torch.cat((lstm_out[:, -1, :hidden_size], lstm_out[:, 0, hidden_size:]), dim=1)\n",
    "        dense1_out = nn.functional.relu(self.dense1(lstm_out))\n",
    "        dense2_out = nn.functional.relu(self.dense2(dense1_out))\n",
    "        dropped_out = self.dropout(dense2_out)\n",
    "        output = nn.functional.softmax(self.output_layer(dropped_out), dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat the model with 32 as the max token length \n",
    "model = IntentClassificationModel(vocab_size, torch.Tensor(embedding_matrix), hidden_state=128, dense_size=600, num_intents=10, dropout_rate=0.5)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 \n",
    "\n",
    "# Initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # Set initial \"min\" to infinity\n",
    "\n",
    "# Some lists to keep track of loss and accuracy during each epoch\n",
    "epoch_lst = []\n",
    "train_loss_lst = []\n",
    "valid_loss_lst = []\n",
    "train_acc_lst = []\n",
    "valid_acc_lst = []\n",
    "\n",
    "# Start looping through epochs\n",
    "for epoch in range(epochs): \n",
    "    \n",
    "    # Monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # Train the model \n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Load train images with labels(targets)\n",
    "    for data, target in train_loader: \n",
    "        \n",
    "        # Clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        pred_labels = torch.argmax(output, 1)\n",
    "        \n",
    "        # Compute total no. of labels \n",
    "        total += len(target)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        correct += (pred_labels == target).sum()\n",
    "        \n",
    "        # Calculate the loss \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        # Calculate average training loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        \n",
    "    # Average accuracy \n",
    "    accuracy = 100 * correct/float(total)\n",
    "    \n",
    "    # Append them to a list for plotting and printing purposes\n",
    "    train_loss_lst.append(train_loss)\n",
    "    train_acc_lst.append(accuracy)\n",
    "    \n",
    "    \n",
    "    # Set the model to evaluation mode \n",
    "    model.eval()\n",
    "    \n",
    "    # Caclulate validation accuracy\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            pred_labels = torch.argmax(output, 1)\n",
    "            \n",
    "            # Compute total no. of labels \n",
    "            total += len(target)\n",
    "            \n",
    "            # Total correct predictions\n",
    "            correct += (pred_labels == target).sum()\n",
    "            \n",
    "            # Calculate the loss \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Update running validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "            # Calculate average validation loss over an epoch\n",
    "            valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "            \n",
    "            # Total no. of labels\n",
    "            total += len(target)    \n",
    "            \n",
    "            # Total correct predictions\n",
    "            correct += (pred_labels == target).sum()\n",
    "            \n",
    "# Calculate average validaton loss and accuracy over an epoch\n",
    "val_loss = val_loss / len(valid_loader.dataset)\n",
    "accuracy = 100 * correct/float(total)\n",
    "\n",
    "# Put them in their list \n",
    "valid_acc_lst.append(accuracy)\n",
    "val_loss_lst.append(val_loss)      \n",
    "\n",
    "# Print the epoch and training loss details with validation accuracy \n",
    "print(f\"Epoch: {epoch+1}/{epochs}.. Training loss: {train_loss:.3f}.. Validation Loss: {val_loss:.3f}.. Validation Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Save the model if validation loss has decreased  \n",
    "if val_loss <= valid_loss_min: \n",
    "    print(f\"Validation loss decreased ({valid_loss_min:.3f} --> {val_loss:.3f}). Saving model...\")\n",
    "    torch.save(model.state_dict(), \"models/intent_classification_model.pt\")\n",
    "    valid_loss_min = val_loss\n",
    "# Move to next epoch \n",
    "epoch_lst.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the model with the lowest validation loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"models/intent_classification_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training loss vs validation loss (the loss is how bad the model is during training)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss_lst, label=\"Training Loss\", color=\"cyan\")\n",
    "plt.plot(valid_loss_lst, label=\"Validation Loss\", color=\"magenta\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize training accuracy vs validation accuracy \n",
    "plt.figure(figsize=(10, 7)) \n",
    "plt.plot(train_acc_lst, label=\"Training Accuracy\", color=\"magenta\")   \n",
    "plt.plot(valid_acc_lst, label=\"Validation Accuracy\", color=\"cyan\")\n",
    "plt.title(\"Training accuracy vs validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At after 20 epochs, the slope becomes a flat line, not really much change in the loss. Floor effect is you can't get any lower than 0 loss. It really quickly learns from the training data what it needs to learn. If you continue to train, you 're basically overfitting to the training data, you are fitting to the unimportant signal. \n",
    "\n",
    "For example, in the context of images, if the model learns to recognize what a cat is, it might now be too detailed and learn that cats have to be the color black as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model improvements\n",
    "The model overfits at a low epoch. Model is significantly overfitting. Plot out accuracies.\n",
    "\n",
    "Don't need 100 training epoch.\n",
    "\n",
    "Look at learning rate scheduling, after certain number of epochs, decrease learning rate.\n",
    "* Learning rate scheduling\n",
    "* Early stopping or reducing epochs\n",
    "* Dropout layers\n",
    "* Regularization\n",
    "* Improve distinctiveness between intent data\n",
    "\n",
    "After I have applied these improvements, my accuracy went up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_intent(user_input): \n",
    "    # Tokenize the input \n",
    "    user_input = tokenizer(user_input)\n",
    "    \n",
    "    # Convert the input to indices \n",
    "    indexed_input = [word_to_idx[word] for word in user_input]\n",
    "    \n",
    "    # Pad the input \n",
    "    padded_input = pad_sequence([torch.tensor(indexed_input)], batch_first=True)\n",
    "    \n",
    "    # Make predictions \n",
    "    output = model(padded_input)\n",
    "    \n",
    "    # Get the predicted label \n",
    "    pred_label = torch.argmax(output, 1)\n",
    "    \n",
    "    # Return the predicted label \n",
    "    return le.inverse_transform(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing checkpoint settings to view progress and save model \n",
    "filename = 'models/intent_classification_model.h5'\n",
    "\n",
    "# Learning Rate Scheduling \n",
    "# This function keeps the initial learning rate for the first ten epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target variable \n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_valid = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io # for encoding\n",
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line.strip.split()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use Torch's tokenizer API \n",
    "# Train-test split of 95% train and 5% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for training\n",
    "# Change all of the following configurations as per the specifications in the original repo \n",
    "# Set a seed value \n",
    "seed_value = 12321 \n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "    \n",
    "class Config: \n",
    " \n",
    "    VOCAB_SIZE = 0\n",
    "    BATCH_SIZE = 512 \n",
    "    EMB_SIZE = 300 \n",
    "    OUT_SIZE = 2\n",
    "    NUM_FOLDS = 5 #  \n",
    "    NUM_EPOCHS = 10 \n",
    "    NUM_WORKERS = 8\n",
    "    \n",
    "# I want to update the pretrained embedding weights during training process \n",
    "# I want to use a pretrained embedding\n",
    "    EMB_WT_UPDATE = True\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False \n",
    "    PATIENCE = 6 \n",
    "    IS_BIDIRECTIONAL = True \n",
    "    \n",
    "    # Model hyperparameters\n",
    "    MODEL_PARAMS = {\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"drop_out\": 0.4258,\n",
    "        \"lr\": 0.000366,\n",
    "        \"weight_decay\": 0.00001\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset class for CSV/TSV files \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, vocab, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.data[idx][0]\n",
    "        text = self.data[idx][1]\n",
    "        tokens = self.tokenizer(text)[:self.max_length]\n",
    "        tokens = [self.vocab[token] for token in tokens]\n",
    "        return (torch.tensor(tokens), torch.tensor(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix \n",
    "def create_embedding_matrix(word_index, embedding_dict=None, dim=100): \n",
    "    num_words = len(word_index) + 1 # the word_index dictionary start from 1, not 0, since 0 is reserved for padding\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, idx in word_index.items(): \n",
    "        embedding_vector = embedding_dict.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and validation data\n",
    "def create_data(train_df, valid_df): \n",
    "    X_train = train_df[\"text\"].values\n",
    "    y_train = train_df[\"label\"].values\n",
    "    X_valid = valid_df[\"text\"].values\n",
    "    y_valid = valid_df[\"label\"].values\n",
    "    \n",
    "    ds_train = CustomDataset(X_train, tokenizer, vocab, max_length=100)\n",
    "    ds_valid = CustomDataset(X_valid, tokenizer, vocab, max_length=100)\n",
    "    \n",
    "    torch_train = DataLoader(ds_train, batch_size=CONFIG.batch_size, collate_fn = pad_collate, num_workers=Config.NUM_WORKERS, shuffle=True)\n",
    "    \n",
    "    torch_valid = DataLoader(ds_valid, batch_size=CONFIG.batch_size, collate_fn = pad_collate, num_workers=Config.NUM_WORKERS, shuffle=True)\n",
    "    \n",
    "    return torch_train, torch_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the Input Sequence.  If the goal is to train with mini-batches, one @ needs to pad the sequences in batch. \n",
    "# In other words, given a mini-batch of size N, if the length of the largest sequence is L, \n",
    "# one needs to pad every sequence with a length of smaller than L with zeros and make their \n",
    "# lengths equal to L. Moreover, it is important that the sequences in the batch are in the \n",
    "# descending order.\n",
    "\n",
    "from cProfile import label\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # Each element in the batch is a tuple (token_tensor, label) \n",
    "    # Sort the batch (based on word count) in descending order \n",
    "    \n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Also need to store the length of each sequence. This is later needed in order to unpad the sequences\n",
    "    seq_len = torch.Tensor([x[0].shape[0] for x in sorted_batch])\n",
    "    labels = torch.LongTensor([x[1] for x in sorted_batch]) \n",
    "    \n",
    "    return sequences_padded, seq_len, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the input data into a TensorDataset (see what other types of data are availabel as well)\n",
    "dataset = TensorDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture \n",
    "- Create a neural network in Torch for intent classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import lstm\n",
    "\n",
    "# Enhance the architecture later \n",
    "class IntentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim, output_dim, n_layers, dropout): \n",
    "        super().__init__() # In Python3, class specification ain't required in the super() call anymore \n",
    "        \n",
    "        # Emebdding layer with pretrained weights \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM layer \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=True, \n",
    "                            dropout=dropout)\n",
    "        \n",
    "        # Dense layers \n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 600)  # 2 for bidirectional \n",
    "        self.fc2 = nn.Linear(600, 600)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "        # Output layer \n",
    "        self.out = nn.Linear(600, output_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # text = [batch_size, embed_length]\n",
    "        \n",
    "        embeddings = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        # embedded = [batch_size, sent_length, emb_dim]\n",
    "        assert embeddings.shape == (inputs.shape[0], inputs.shape[1], self.embedding_dim)\n",
    "         \n",
    "        # pack_padded_sequence before feeding to the LSTM. This is required so PyTorch knows \n",
    "        # which elements of the sequence are padded and ignores them in the computation \n",
    "        # Accomplished only after the embedding step \n",
    "        embeds_pack = pack_padded_sequence(embeddings, inputs_lengths, batch_first=True)\n",
    "        \n",
    "        # Get the dimensions of the packed sequence \n",
    "        dimensions = embeds_pack.data.size()\n",
    "        _, (hidden, _) = self.lstm(embeds_pack)\n",
    "        # Ours task being a classification model, we are only interested in the final hidden state and not the LSTM output \n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        final_hidden_forward = hidden[-2,:,:] # [batch_size, hidden_dim]\n",
    "        final_hidden_backward = hidden[-1,:,:] # [bacth_size, hidden_dim]\n",
    "        \n",
    "        # Concat the final forward and hidden backward states \n",
    "        hidden = torch.cat((final_hidden_forward, final_hidden_backward), dim=1)\n",
    "                \n",
    "        # Dense Linear Layers \n",
    "        dense_outputs_1 = F.relu(self.fc1(hidden))\n",
    "        dense_outputs_2 = self.dropout(F.relu(self.fc2(dense_outputs_1)))\n",
    "        \n",
    "        # Final output classification layer\n",
    "        final_output = F.soft(self.out(dense_outputs_2))\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Instantiate the model\n",
    "intent_model = IntentClassifier(vocab_size=Config.VOCAB_SIZE, embedding_dim=Config.EMB_SIZE, embedding_matrix=embedding_matrix, hidden_dim=Config.MODEL_PARAMS[\"hidden_size\"], output_dim=Config.OUT_SIZE, n_layers=Config.MODEL_PARAMS[\"num_layers\"], dropout=Config.MODEL_PARAMS[\"drop_out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import lstm\n",
    "\n",
    "# Enhance the architecture later \n",
    "class IntentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim, output_dim, n_layers, dropout): \n",
    "        super().__init__() # In Python3, class specification ain't required in the super() call anymore \n",
    "        \n",
    "        # Emebdding layer with pretrained weights \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM layer \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=True, \n",
    "                            dropout=dropout)\n",
    "        \n",
    "        # Dense layers \n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 600)  # 2 for bidirectional \n",
    "        self.fc2 = nn.Linear(600, 600)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "        # Output layer \n",
    "        self.out = nn.Linear(600, output_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # text = [batch_size, embed_length]\n",
    "        \n",
    "        embeddings = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        # embedded = [batch_size, sent_length, emb_dim]\n",
    "        assert embeddings.shape == (inputs.shape[0], inputs.shape[1], self.embedding_dim)\n",
    "         \n",
    "        # pack_padded_sequence before feeding to the LSTM. This is required so PyTorch knows \n",
    "        # which elements of the sequence are padded and ignores them in the computation \n",
    "        # Accomplished only after the embedding step \n",
    "        embeds_pack = pack_padded_sequence(embeddings, inputs_lengths, batch_first=True)\n",
    "        \n",
    "        # Get the dimensions of the packed sequence \n",
    "        dimensions = embeds_pack.data.size()\n",
    "        _, (hidden, _) = self.lstm(embeds_pack)\n",
    "        # Ours task being a classification model, we are only interested in the final hidden state and not the LSTM output \n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        final_hidden_forward = hidden[-2,:,:] # [batch_size, hidden_dim]\n",
    "        final_hidden_backward = hidden[-1,:,:] # [bacth_size, hidden_dim]\n",
    "        \n",
    "        # Concat the final forward and hidden backward states \n",
    "        hidden = torch.cat((final_hidden_forward, final_hidden_backward), dim=1)\n",
    "                \n",
    "        # Dense Linear Layers \n",
    "        dense_outputs_1 = F.relu(self.fc1(hidden))\n",
    "        dense_outputs_2 = self.dropout(F.relu(self.fc2(dense_outputs_1)))\n",
    "        \n",
    "        # Final output classification layer\n",
    "        final_output = F.soft(self.out(dense_outputs_2))\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Instantiate the model\n",
    "intent_model = IntentClassifier(vocab_size=Config.VOCAB_SIZE, embedding_dim=Config.EMB_SIZE, embedding_matrix=embedding_matrix, hidden_dim=Config.MODEL_PARAMS[\"hidden_size\"], output_dim=Config.OUT_SIZE, n_layers=Config.MODEL_PARAMS[\"num_layers\"], dropout=Config.MODEL_PARAMS[\"drop_out\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
