{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Classification With PyTorch\n",
    "Previously, my focus in the notebooks was on obtaining labeled data for my chatbot. However, this current notebook is centered around utilizing PyTorch for the classification of intents within fresh, unseen user-generated data. The model has transitioned to a supervised learning approach, leveraging the labels derived from the unsupervised learning conducted in the preceding notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RASA Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasa trains this intent classification step with SVM and GridsearchCV because they can try different configurations ([source](https://medium.com/bhavaniravi/intent-classification-demystifying-rasanlu-part-4-685fc02f5c1d)). When deploying preprocessing pipeline should remain same between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.17.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/saggysimmba/Library/Python/3.12/lib/python/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /Users/saggysimmba/Library/Python/3.12/lib/python/site-packages (from wandb) (4.2.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/saggysimmba/Library/Python/3.12/lib/python/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (2.7.1)\n",
      "Requirement already satisfied: setproctitle in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/saggysimmba/Library/Python/3.12/lib/python/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext-langdetect\n",
      "  Using cached fasttext-langdetect-1.0.5.tar.gz (6.8 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fasttext>=0.9.1 (from fasttext-langdetect)\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fasttext-langdetect) (2.31.0)\n",
      "Collecting pybind11>=2.2 (from fasttext>=0.9.1->fasttext-langdetect)\n",
      "  Using cached pybind11-2.13.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fasttext>=0.9.1->fasttext-langdetect) (69.5.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fasttext>=0.9.1->fasttext-langdetect) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.22.0->fasttext-langdetect) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.22.0->fasttext-langdetect) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.22.0->fasttext-langdetect) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.22.0->fasttext-langdetect) (2024.2.2)\n",
      "Using cached pybind11-2.13.1-py3-none-any.whl (238 kB)\n",
      "Building wheels for collected packages: fasttext-langdetect, fasttext\n",
      "  Building wheel for fasttext-langdetect (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext-langdetect: filename=fasttext_langdetect-1.0.5-py3-none-any.whl size=7503 sha256=8453bc8e4db4965ecb10b58a6cb9410e60739e3475daff3820aaa2cb9cb0c5c6\n",
      "  Stored in directory: /Users/saggysimmba/Library/Caches/pip/wheels/02/98/3e/91a2527cd5ff03e889b6089faf95681f8cc6310eb72a0c636a\n",
      "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.3-cp312-cp312-macosx_14_0_universal2.whl size=623348 sha256=371ce65bfa348b6e93f752bea8e314e5a5e3fd7a40d06ae12bdc5ff09d0094b2\n",
      "  Stored in directory: /Users/saggysimmba/Library/Caches/pip/wheels/20/27/95/a7baf1b435f1cbde017cabdf1e9688526d2b0e929255a359c6\n",
      "Successfully built fasttext-langdetect fasttext\n",
      "Installing collected packages: pybind11, fasttext, fasttext-langdetect\n",
      "Successfully installed fasttext-0.9.3 fasttext-langdetect-1.0.5 pybind11-2.13.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fasttext-langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import wandb\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.2.2\n",
      "Numpy: 1.26.4\n",
      "Sklearn: 1.4.2\n",
      "Training data:                                              support  \\\n",
      "0  [very, poor, feedback, very, disappointing, se...   \n",
      "1  [already, done, i, am, frankly, fed, up, with,...   \n",
      "2  [very, poor, feedback, very, disappointing, se...   \n",
      "3  [can, see, you, have, replied, to, others, who...   \n",
      "4  [my, issue, is, not, resolved, really, should,...   \n",
      "\n",
      "                                         account           greeting  \\\n",
      "0                      [email, account, details]    [good, morning]   \n",
      "1                      [email, account, details]  [good, afternoon]   \n",
      "2                      [email, account, details]    [good, evening]   \n",
      "3  [the, credit, card, information, is, correct]      [good, night]   \n",
      "4                        [account, email, email]        [good, day]   \n",
      "\n",
      "            goodbye     speak_representative            challenge_robot  \\\n",
      "0         [goodbye]    [talk, human, please]            [are, you, bot]   \n",
      "1      [thank, you]     [let, talk, support]           [are, you, real]   \n",
      "2  [thanks, a, lot]   [speak, agent, person]             [are, you, AI]   \n",
      "3            [done]         [connect, human]       [are, you, computer]   \n",
      "4        [see, you]  [speak, representative]  [who, am, I, talking, to]   \n",
      "\n",
      "                                  quality                       track  \\\n",
      "0               [product, quality, issue]          [track, my, order]   \n",
      "1  [quality, concerns, product, received]    [where, is, my, package]   \n",
      "2      [received, item, quality, problem]      [locate, my, shipment]   \n",
      "3    [need, help, with, product, quality]        [find, my, delivery]   \n",
      "4   [product, not, as, expected, quality]  [check, my, order, status]   \n",
      "\n",
      "                discount  \n",
      "0      [offer, discount]  \n",
      "1       [sale, discount]  \n",
      "2      [price, discount]  \n",
      "3  [promotion, discount]  \n",
      "4       [deal, discount]  \n"
     ]
    }
   ],
   "source": [
    "# Standard \n",
    "import collections\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "import numpy as np\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "\n",
    "# Machine Learning\n",
    "import sklearn\n",
    "print(f\"Sklearn: {sklearn.__version__}\")\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Preprocessing and Torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# from torchtext.data import get_tokenizer\n",
    "\n",
    "# Warnings \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reading in training data\n",
    "train = pd.read_pickle('../objects/train.pkl')\n",
    "print(f'Training data: {train.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for training\n",
    "# Change all of the following configurations as per the specifications in the original repo \n",
    "# Set a seed value \n",
    "seed_value = 12321 \n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hva8b3hu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d889ce785f1249db83be1f371bb37c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▇▇███</td></tr><tr><td>Training Loss</td><td>█▂▂▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▆▇███</td></tr><tr><td>Validation F1 Score</td><td>▁▆████</td></tr><tr><td>Validation Loss</td><td>█▄▂▂▁▁</td></tr><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>84.63763</td></tr><tr><td>Training Loss</td><td>4212.7321</td></tr><tr><td>Validation Accuracy</td><td>82.76762</td></tr><tr><td>Validation F1 Score</td><td>0.82519</td></tr><tr><td>Validation Loss</td><td>1190.43005</td></tr><tr><td>epoch</td><td>5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-butterfly-84</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/hva8b3hu' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/hva8b3hu</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240712_102619-hva8b3hu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hva8b3hu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68e17508bfb4a82b646dcffb525acad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011147703232968018, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240712_102925-6v3xr3v6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/6v3xr3v6' target=\"_blank\">pleasant-mountain-85</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/6v3xr3v6' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/6v3xr3v6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sinhasagar507/intent-classification/runs/6v3xr3v6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x31ebbcbf0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"intent-classification\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 32, \n",
    "    \"embedding_size\": 200,\n",
    "    \"hidden_size\": 128,\n",
    "    \"output_size\": 9,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"eval_metric\": \"accuracy\", \n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"scheduler_lambda_epoch_threshold\": 10,\n",
    "    \"scheduler_decay_rate\": -0.1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.melt(train)\n",
    "train.columns = [\"intent\", \"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>challenge_robot</td>\n",
       "      <td>[is, this, computer, program]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>discount</td>\n",
       "      <td>[do, you, ve, plans, to, bring, this, item, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quality</td>\n",
       "      <td>[defective, item, replacement, process, details]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speak_representative</td>\n",
       "      <td>[talk, support, agent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>discount</td>\n",
       "      <td>[discounted, price, discount]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13495</th>\n",
       "      <td>support</td>\n",
       "      <td>[in, english, it, may, be, called, something, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[thanks, so, much, see, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13497</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[appreciate, that]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13498</th>\n",
       "      <td>speak_representative</td>\n",
       "      <td>[let, talk, support]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13499</th>\n",
       "      <td>greeting</td>\n",
       "      <td>[hello, everyone,, how, is, it, going]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     intent                                             tokens\n",
       "0           challenge_robot                      [is, this, computer, program]\n",
       "1                  discount  [do, you, ve, plans, to, bring, this, item, in...\n",
       "2                   quality   [defective, item, replacement, process, details]\n",
       "3      speak_representative                             [talk, support, agent]\n",
       "4                  discount                      [discounted, price, discount]\n",
       "...                     ...                                                ...\n",
       "13495               support  [in, english, it, may, be, called, something, ...\n",
       "13496               goodbye                       [thanks, so, much, see, you]\n",
       "13497               goodbye                                 [appreciate, that]\n",
       "13498  speak_representative                               [let, talk, support]\n",
       "13499              greeting             [hello, everyone,, how, is, it, going]\n",
       "\n",
       "[13500 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df = train.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [very, poor, feedback, very, disappointing, se...\n",
       "1        [already, done, i, am, frankly, fed, up, with,...\n",
       "2        [very, poor, feedback, very, disappointing, se...\n",
       "3        [can, see, you, have, replied, to, others, who...\n",
       "4        [my, issue, is, not, resolved, really, should,...\n",
       "                               ...                        \n",
       "13495    [please, unblock, my, account, i, need, to, pu...\n",
       "13496    [i, know, that, amazon, did, not, manufacture,...\n",
       "13497    [friend, do, not, use, amazon, pay, its, not, ...\n",
       "13498                                              [india]\n",
       "13499    [can, you, see, which, type, of, service, the,...\n",
       "Name: tokens, Length: 13500, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['very', 'poor', 'feedback', 'very', 'disappointing', 'services']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>challenge_robot</td>\n",
       "      <td>[is, this, computer, program]</td>\n",
       "      <td>[computer, program]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>discount</td>\n",
       "      <td>[do, you, ve, plans, to, bring, this, item, in...</td>\n",
       "      <td>[ve, plans, bring, item, prime, trust, sellers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quality</td>\n",
       "      <td>[defective, item, replacement, process, details]</td>\n",
       "      <td>[defective, item, replacement, process, details]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speak_representative</td>\n",
       "      <td>[talk, support, agent]</td>\n",
       "      <td>[talk, support, agent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>discount</td>\n",
       "      <td>[discounted, price, discount]</td>\n",
       "      <td>[discounted, price, discount]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 intent                                             tokens  \\\n",
       "0       challenge_robot                      [is, this, computer, program]   \n",
       "1              discount  [do, you, ve, plans, to, bring, this, item, in...   \n",
       "2               quality   [defective, item, replacement, process, details]   \n",
       "3  speak_representative                             [talk, support, agent]   \n",
       "4              discount                      [discounted, price, discount]   \n",
       "\n",
       "                                     cleaned_tokens  \n",
       "0                               [computer, program]  \n",
       "1   [ve, plans, bring, item, prime, trust, sellers]  \n",
       "2  [defective, item, replacement, process, details]  \n",
       "3                            [talk, support, agent]  \n",
       "4                     [discounted, price, discount]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of common stopwords\n",
    "manual_stopwords = {\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', \n",
    "    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', \n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', \n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def basic_preprocess_tokens(tokens):\n",
    "    \n",
    "    # Convert string representation of list to actual list\n",
    "    # tokens = ast.literal_eval(tokens)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in manual_stopwords]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply basic preprocessing to the tokens column\n",
    "shuffled_df['cleaned_tokens'] = shuffled_df['tokens'].apply(basic_preprocess_tokens)\n",
    "\n",
    "shuffled_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13402, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wherever the length of the tokens is 0, we will remove those rows\n",
    "shuffled_df = shuffled_df[shuffled_df['cleaned_tokens'].apply(len) > 0]\n",
    "shuffled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intent\n",
       "challenge_robot         1500\n",
       "speak_representative    1500\n",
       "goodbye                 1500\n",
       "track                   1500\n",
       "discount                1499\n",
       "account                 1495\n",
       "support                 1494\n",
       "quality                 1489\n",
       "greeting                1425\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df[\"intent\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_lens = [len(tokens) for tokens in shuffled_df['cleaned_tokens']]\n",
    "sns.histplot(seq_lens, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df.to_csv(\"../data/processed/train_intents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent            object\n",
      "tokens            object\n",
      "cleaned_tokens    object\n",
      "dtype: object\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Print the data types of the columns\n",
    "print(shuffled_df.dtypes)\n",
    "\n",
    "# Check the data types of each row in the \"tokens\" column and if its not a list, highlight the the error \n",
    "# Don't print it, log it \n",
    "print(\" \")\n",
    "for index, row in shuffled_df.iterrows():\n",
    "    if not isinstance(row[\"tokens\"], list):\n",
    "        print(f\"Error: {row['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [token_lst for token_lst in shuffled_df['cleaned_tokens']]\n",
    "X = [*X]\n",
    "y = [*shuffled_df['intent'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saggysimmba/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saggysimmba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchtext tokenizer \n",
    "- Add description later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of Action\n",
    "- Prepare the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Applications/saggydev/projects_learning/amazon_support/notebooks'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps taken\n",
    "    -   the words would involve creating a vocabulary dictionary to map words to indices \n",
    "    -   For each sequence, the words are converted into their corresponding indices based on the word dictionary \n",
    "    - When feeding sentences into the model, ensure a consistent sequence length is crucial \n",
    "    - To achieve this, sequences are padded with zeros until they reach the length of the longest sequence \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Use glove word embeddings \n",
    "embeddings_index = {}\n",
    "f = open(\"../models/glove.twitter.27B/glove.twitter.27B.200d.txt\", \"r\", encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build a vocabulary: This is something I hadve just added \n",
    "from collections import Counter\n",
    "word_counts = Counter(token for sentence in X for token in sentence)\n",
    "vocabulary = {word: i+1 for i, (word, _) in enumerate(word_counts.items())}  # Reserve 0 for padding \n",
    "vocabulary[\"<unknown>\"] = len(vocabulary) + 1  # Reserve for unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20198  ,  0.41916  , -1.0054   , -0.84391  , -0.68951  ,\n",
       "        0.18279  , -0.23453  , -0.0052121,  0.45239  , -0.92015  ,\n",
       "       -0.13365  ,  0.040719 ,  0.072168 , -0.062865 , -1.3087   ,\n",
       "        0.27299  ,  0.52128  ,  0.57118  ,  0.29222  ,  0.33075  ,\n",
       "       -0.24459  , -0.37486  ,  1.1441   , -0.46274  , -0.076162 ,\n",
       "        0.82587  , -0.24807  , -0.944    ,  0.61853  , -0.39438  ,\n",
       "        0.59014  , -0.29744  , -0.35508  ,  0.53303  ,  0.44923  ,\n",
       "       -0.63778  , -0.15895  , -0.86348  , -0.49895  , -0.041426 ,\n",
       "        0.43123  ,  0.29367  , -1.2033   , -0.45566  , -0.34636  ,\n",
       "       -0.02048  , -1.7236   ,  0.5066   , -0.81875  , -0.21657  ,\n",
       "       -0.26766  ,  0.35248  ,  0.078784 , -0.66945  ,  1.1626   ,\n",
       "        0.0051163, -1.0655   ,  0.096867 ,  0.33572  ,  0.52233  ,\n",
       "        0.40273  ,  0.027763 , -0.58058  , -0.42969  , -0.22255  ,\n",
       "        0.51217  ,  0.011956 ,  0.043467 ,  0.3244   ,  1.0959   ,\n",
       "        0.11997  ,  0.60929  ,  0.068    ,  0.19848  , -0.46239  ,\n",
       "        0.7439   , -0.75058  , -0.018753 ,  0.58308  ,  0.058709 ,\n",
       "       -1.6619   ,  0.5988   ,  0.59503  , -0.90967  , -0.23844  ,\n",
       "        0.1871   ,  0.2298   , -0.61659  ,  0.72868  , -0.044096 ,\n",
       "       -0.093553 ,  1.0485   ,  0.43222  , -0.6701   ,  0.099832 ,\n",
       "       -0.57062  , -1.1275   , -0.25786  , -0.067018 ,  0.99043  ,\n",
       "       -0.16494  , -0.074319 , -0.20347  ,  0.50117  ,  0.99839  ,\n",
       "        0.027985 , -0.26679  ,  0.10414  , -0.1975   ,  0.53277  ,\n",
       "        0.59072  , -0.98284  , -0.41028  ,  0.86478  , -0.56606  ,\n",
       "        0.50508  ,  0.48191  , -0.83698  , -0.68231  ,  0.71701  ,\n",
       "        0.20375  ,  0.24835  ,  0.61261  , -0.82918  ,  0.1744   ,\n",
       "        0.51417  , -0.94267  , -0.22483  ,  0.82227  ,  0.64526  ,\n",
       "        0.77582  ,  0.081498 ,  0.14157  ,  0.57915  , -0.37718  ,\n",
       "        0.30407  ,  0.025211 ,  0.45323  , -0.033279 , -0.14454  ,\n",
       "       -0.16226  ,  0.44186  , -0.40837  ,  0.43568  ,  0.97894  ,\n",
       "       -0.36031  ,  0.051556 , -0.66336  ,  0.28641  , -0.018247 ,\n",
       "        0.25104  , -0.47485  ,  0.33419  ,  0.77673  ,  0.29789  ,\n",
       "       -0.097057 , -0.36122  ,  0.37737  ,  0.62324  , -0.47451  ,\n",
       "       -0.78616  ,  0.41789  ,  0.64583  , -0.89144  , -0.60993  ,\n",
       "       -0.43233  ,  0.525    ,  0.58204  , -0.32864  ,  0.39671  ,\n",
       "        0.74224  , -0.73897  ,  2.0081   ,  0.4678   , -1.4246   ,\n",
       "       -0.36199  ,  0.13764  , -0.18797  ,  0.23071  ,  0.56834  ,\n",
       "        0.072789 ,  0.49499  ,  1.0639   ,  0.67527  ,  0.98661  ,\n",
       "        0.76382  ,  0.96887  , -1.1163   ,  0.34554  ,  0.38176  ,\n",
       "        0.91244  , -0.52999  , -0.17728  ,  0.18612  ,  1.0075   ,\n",
       "       -0.29716  ,  0.27487  , -0.3182   ,  0.21931  , -0.64215  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"<unknown>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20198  ,  0.41916  , -1.0054   , -0.84391  , -0.68951  ,\n",
       "        0.18279  , -0.23453  , -0.0052121,  0.45239  , -0.92015  ,\n",
       "       -0.13365  ,  0.040719 ,  0.072168 , -0.062865 , -1.3087   ,\n",
       "        0.27299  ,  0.52128  ,  0.57118  ,  0.29222  ,  0.33075  ,\n",
       "       -0.24459  , -0.37486  ,  1.1441   , -0.46274  , -0.076162 ,\n",
       "        0.82587  , -0.24807  , -0.944    ,  0.61853  , -0.39438  ,\n",
       "        0.59014  , -0.29744  , -0.35508  ,  0.53303  ,  0.44923  ,\n",
       "       -0.63778  , -0.15895  , -0.86348  , -0.49895  , -0.041426 ,\n",
       "        0.43123  ,  0.29367  , -1.2033   , -0.45566  , -0.34636  ,\n",
       "       -0.02048  , -1.7236   ,  0.5066   , -0.81875  , -0.21657  ,\n",
       "       -0.26766  ,  0.35248  ,  0.078784 , -0.66945  ,  1.1626   ,\n",
       "        0.0051163, -1.0655   ,  0.096867 ,  0.33572  ,  0.52233  ,\n",
       "        0.40273  ,  0.027763 , -0.58058  , -0.42969  , -0.22255  ,\n",
       "        0.51217  ,  0.011956 ,  0.043467 ,  0.3244   ,  1.0959   ,\n",
       "        0.11997  ,  0.60929  ,  0.068    ,  0.19848  , -0.46239  ,\n",
       "        0.7439   , -0.75058  , -0.018753 ,  0.58308  ,  0.058709 ,\n",
       "       -1.6619   ,  0.5988   ,  0.59503  , -0.90967  , -0.23844  ,\n",
       "        0.1871   ,  0.2298   , -0.61659  ,  0.72868  , -0.044096 ,\n",
       "       -0.093553 ,  1.0485   ,  0.43222  , -0.6701   ,  0.099832 ,\n",
       "       -0.57062  , -1.1275   , -0.25786  , -0.067018 ,  0.99043  ,\n",
       "       -0.16494  , -0.074319 , -0.20347  ,  0.50117  ,  0.99839  ,\n",
       "        0.027985 , -0.26679  ,  0.10414  , -0.1975   ,  0.53277  ,\n",
       "        0.59072  , -0.98284  , -0.41028  ,  0.86478  , -0.56606  ,\n",
       "        0.50508  ,  0.48191  , -0.83698  , -0.68231  ,  0.71701  ,\n",
       "        0.20375  ,  0.24835  ,  0.61261  , -0.82918  ,  0.1744   ,\n",
       "        0.51417  , -0.94267  , -0.22483  ,  0.82227  ,  0.64526  ,\n",
       "        0.77582  ,  0.081498 ,  0.14157  ,  0.57915  , -0.37718  ,\n",
       "        0.30407  ,  0.025211 ,  0.45323  , -0.033279 , -0.14454  ,\n",
       "       -0.16226  ,  0.44186  , -0.40837  ,  0.43568  ,  0.97894  ,\n",
       "       -0.36031  ,  0.051556 , -0.66336  ,  0.28641  , -0.018247 ,\n",
       "        0.25104  , -0.47485  ,  0.33419  ,  0.77673  ,  0.29789  ,\n",
       "       -0.097057 , -0.36122  ,  0.37737  ,  0.62324  , -0.47451  ,\n",
       "       -0.78616  ,  0.41789  ,  0.64583  , -0.89144  , -0.60993  ,\n",
       "       -0.43233  ,  0.525    ,  0.58204  , -0.32864  ,  0.39671  ,\n",
       "        0.74224  , -0.73897  ,  2.0081   ,  0.4678   , -1.4246   ,\n",
       "       -0.36199  ,  0.13764  , -0.18797  ,  0.23071  ,  0.56834  ,\n",
       "        0.072789 ,  0.49499  ,  1.0639   ,  0.67527  ,  0.98661  ,\n",
       "        0.76382  ,  0.96887  , -1.1163   ,  0.34554  ,  0.38176  ,\n",
       "        0.91244  , -0.52999  , -0.17728  ,  0.18612  ,  1.0075   ,\n",
       "       -0.29716  ,  0.27487  , -0.3182   ,  0.21931  , -0.64215  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get(\"<unknown>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('computer', 1), ('program', 2), ('ve', 3), ('plans', 4), ('bring', 5), ('item', 6), ('prime', 7), ('trust', 8), ('sellers', 9), ('defective', 10), ('replacement', 11), ('process', 12), ('details', 13), ('talk', 14), ('support', 15), ('agent', 16), ('discounted', 17), ('price', 18), ('discount', 19), ('im', 20), ('done', 21), ('see', 22), ('given', 23), ('options', 24), ('track', 25), ('order', 26), ('info', 27), ('available', 28), ('emailed', 29), ('customer', 30), ('service', 31), ('contact', 32), ('number', 33), ('thanks', 34), ('sorry', 35), ('hassle', 36), ('virtual', 37), ('assistant', 38), ('find', 39), ('shipping', 40), ('speak', 41), ('someone', 42), ('fall', 43), ('sale', 44), ('information', 45), ('apply', 46), ('uk', 47), ('residents', 48), ('hello', 49), ('everybody', 50), ('link', 51), ('anything', 52), ('message', 53), ('sent', 54), ('know', 55), ('minor', 56), ('thing', 57), ('opt', 58), ('general', 59), ('account', 60), ('hard', 61), ('remember', 62), ('smile', 63), ('apparently', 64), ('parcel', 65), ('due', 66), ('delivery', 67), ('yesterday', 68), ('still', 69), ('arrived', 70), ('days', 71), ('wasted', 72), ('site', 73), ('seriously', 74), ('first', 75), ('time', 76), ('facing', 77), ('keep', 78), ('happening', 79), ('always', 80), ('promise', 81), ('something', 82), ('come', 83), ('excuses', 84), ('promised', 85), ('noon', 86), ('today', 87), ('let', 88), ('us', 89), ('clear', 90), ('want', 91), ('refund', 92), ('compensation', 93), ('teach', 94), ('idiot', 95), ('people', 96), ('response', 97), ('reported', 98), ('amazon', 99), ('unable', 100), ('solution', 101), ('check', 102), ('bin', 103), ('series', 104), ('indeed', 105), ('treated', 106), ('international', 107), ('vcc', 108), ('life', 109), ('event', 110), ('deal', 111), ('nice', 112), ('day', 113), ('representative', 114), ('much', 115), ('lots', 116), ('apologies', 117), ('promises', 118), ('investigations', 119), ('feel', 120), ('like', 121), ('interested', 122), ('solving', 123), ('problem', 124), ('folks', 125), ('status', 126), ('hey', 127), ('please', 128), ('pay', 129), ('seller', 130), ('funds', 131), ('held', 132), ('email', 133), ('really', 134), ('understand', 135), ('concern', 136), ('tell', 137), ('eta', 138), ('resolve', 139), ('fix', 140), ('issue', 141), ('customers', 142), ('face', 143), ('clearance', 144), ('tricks', 145), ('delivered', 146), ('refurbished', 147), ('device', 148), ('center', 149), ('claiming', 150), ('warranty', 151), ('responding', 152), ('lost', 153), ('k', 154), ('yes', 155), ('received', 156), ('reference', 157), ('package', 158), ('ordered', 159), ('processed', 160), ('even', 161), ('regular', 162), ('follow', 163), ('product', 164), ('robotic', 165), ('system', 166), ('soon', 167), ('live', 168), ('automated', 169), ('holiday', 170), ('offer', 171), ('chat', 172), ('person', 173), ('justification', 174), ('inappropriate', 175), ('looking', 176), ('connect', 177), ('quality', 178), ('doesnt', 179), ('meet', 180), ('expectations', 181), ('greetings', 182), ('already', 183), ('filled', 184), ('form', 185), ('reply', 186), ('also', 187), ('pls', 188), ('cal', 189), ('solve', 190), ('matter', 191), ('high', 192), ('cheap', 193), ('trying', 194), ('deliver', 195), ('perhaps', 196), ('driver', 197), ('might', 198), ('try', 199), ('normal', 200), ('hour', 201), ('chatbot', 202), ('thank', 203), ('goodbye', 204), ('tracking', 205), ('million', 206), ('comes', 207), ('complaints', 208), ('app', 209), ('confirmation', 210), ('advise', 211), ('guess', 212), ('shipment', 213), ('progress', 214), ('think', 215), ('must', 216), ('stop', 217), ('making', 218), ('fool', 219), ('giving', 220), ('cash', 221), ('back', 222), ('delay', 223), ('another', 224), ('stock', 225), ('choose', 226), ('told', 227), ('care', 228), ('nothing', 229), ('rd', 230), ('months', 231), ('handle', 232), ('stupid', 233), ('early', 234), ('morning', 235), ('third', 236), ('call', 237), ('would', 238), ('mail', 239), ('november', 240), ('th', 241), ('everyone', 242), ('going', 243), ('sure', 244), ('tomorrow', 245), ('online', 246), ('says', 247), ('arriving', 248), ('wednesday', 249), ('date', 250), ('though', 251), ('guy', 252), ('called', 253), ('threaten', 254), ('feedback', 255), ('bye', 256), ('loyal', 257), ('bunch', 258), ('hi', 259), ('chatted', 260), ('representatives', 261), ('many', 262), ('times', 263), ('past', 264), ('years', 265), ('issues', 266), ('deliveries', 267), ('resolved', 268), ('reassured', 269), ('never', 270), ('happen', 271), ('ill', 272), ('contacting', 273), ('guys', 274), ('robot', 275), ('missed', 276), ('executive', 277), ('request', 278), ('ment', 279), ('whtr', 280), ('catch', 281), ('later', 282), ('ai', 283), ('appreciate', 284), ('replies', 285), ('investigation', 286), ('appear', 287), ('orders', 288), ('actually', 289), ('fulfilled', 290), ('work', 291), ('one', 292), ('month', 293), ('struggling', 294), ('feels', 295), ('futile', 296), ('happy', 297), ('filling', 298), ('concert', 299), ('sold', 300), ('injured', 301), ('home', 302), ('way', 303), ('refuse', 304), ('plz', 305), ('new', 306), ('year', 307), ('get', 308), ('ive', 309), ('finished', 310), ('tampered', 311), ('supplied', 312), ('fake', 313), ('faileddetectonline', 314), ('help', 315), ('identified', 316), ('cartridges', 317), ('weekend', 318), ('updates', 319), ('hours', 320), ('cancel', 321), ('proceed', 322), ('cancellation', 323), ('action', 324), ('website', 325), ('human', 326), ('promotion', 327), ('drop', 328), ('showing', 329), ('content', 330), ('need', 331), ('n', 332), ('able', 333), ('return', 334), ('bad', 335), ('press', 336), ('button', 337), ('take', 338), ('week', 339), ('point', 340), ('membership', 341), ('svc', 342), ('rep', 343), ('joke', 344), ('bpl', 345), ('tv', 346), ('listed', 347), ('informed', 348), ('pricing', 349), ('error', 350), ('id', 351), ('happens', 352), ('written', 353), ('waste', 354), ('rural', 355), ('others', 356), ('exchange', 357), ('better', 358), ('next', 359), ('condition', 360), ('helping', 361), ('wonderful', 362), ('evening', 363), ('login', 364), ('anymore', 365), ('reason', 366), ('file', 367), ('case', 368), ('consumer', 369), ('court', 370), ('resolution', 371), ('local', 372), ('team', 373), ('denied', 374), ('could', 375), ('place', 376), ('however', 377), ('intimation', 378), ('retailer', 379), ('using', 380), ('responded', 381), ('communication', 382), ('forwarded', 383), ('emails', 384), ('thats', 385), ('locate', 386), ('consider', 387), ('leave', 388), ('lightly', 389), ('share', 390), ('legal', 391), ('address', 392), ('advocate', 393), ('got', 394), ('attached', 395), ('proof', 396), ('completion', 397), ('writing', 398), ('least', 399), ('refunding', 400), ('use', 401), ('falty', 402), ('usually', 403), ('complain', 404), ('things', 405), ('😌', 406), ('shipped', 407), ('ish', 408), ('little', 409), ('faith', 410), ('add', 411), ('insult', 412), ('injury', 413), ('telling', 414), ('basically', 415), ('fault', 416), ('providing', 417), ('instructions', 418), ('frankly', 419), ('angered', 420), ('insulted', 421), ('works', 422), ('boys', 423), ('drones', 424), ('decision', 425), ('paygrade', 426), ('escalate', 427), ('internally', 428), ('hold', 429), ('marketing', 430), ('gimmick', 431), ('carried', 432), ('misguide', 433), ('increased', 434), ('festival', 435), ('escalates', 436), ('thatpathetic', 437), ('services', 438), ('ups', 439), ('unavailable', 440), ('interaction', 441), ('artificial', 442), ('intelligence', 443), ('restock', 444), ('attempted', 445), ('supposed', 446), ('disappointed', 447), ('provide', 448), ('drivers', 449), ('fail', 450), ('attention', 451), ('detail', 452), ('dm', 453), ('ongoing', 454), ('couple', 455), ('weeks', 456), ('checkout', 457), ('apart', 458), ('assistance', 459), ('required', 460), ('kindly', 461), ('range', 462), ('last', 463), ('benefit', 464), ('pdt', 465), ('nw', 466), ('replcmnt', 467), ('ads', 468), ('invalid', 469), ('asked', 470), ('selling', 471), ('toys', 472), ('expose', 473), ('children', 474), ('toxic', 475), ('materials', 476), ('unfair', 477), ('transfer', 478), ('payment', 479), ('purpose', 480), ('appalling', 481), ('againweb', 482), ('lack', 483), ('poor', 484), ('anyone', 485), ('sensible', 486), ('sort', 487), ('deliberately', 488), ('free', 489), ('trace', 490), ('listing', 491), ('mrp', 492), ('crime', 493), ('india', 494), ('member', 495), ('easy', 496), ('digital', 497), ('good evening', 498), ('voucher', 499), ('helpful', 500), ('technical', 501), ('busy', 502), ('mobile', 503), ('bothered', 504), ('social', 505), ('media', 506), ('escalation', 507), ('improve', 508), ('experience', 509), ('responsibility', 510), ('real', 511), ('flash', 512), ('made', 513), ('twice', 514), ('shopping', 515), ('recommend', 516), ('every', 517), ('suggest', 518), ('bot', 519), ('carrier', 520), ('assigned', 521), ('said', 522), ('amzl', 523), ('hello there', 524), ('products', 525), ('continue', 526), ('damaged', 527), ('amd', 528), ('fc', 529), ('ask', 530), ('relevant', 531), ('touch', 532), ('possible', 533), ('enough', 534), ('everything', 535), ('exclusive', 536), ('well', 537), ('simplier', 538), ('job', 539), ('anyway', 540), ('less', 541), ('worse', 542), ('option', 543), ('expected', 544), ('redelivery', 545), ('guarantee', 546), ('waiting', 547), ('clearly', 548), ('working', 549), ('box', 550), ('good', 551), ('review', 552), ('purchased', 553), ('reviews', 554), ('keeps', 555), ('sending', 556), ('notifications', 557), ('elses', 558), ('contacted', 559), ('man', 560), ('start', 561), ('thinking', 562), ('hiding', 563), ('cheating', 564), ('fluctuations', 565), ('assist', 566), ('approaching', 567), ('sad', 568), ('special', 569), ('period', 570), ('promisedbut', 571), ('orderraised', 572), ('complaint', 573), ('frustrating', 574), ('items', 575), ('idea', 576), ('started', 577), ('ago', 578), ('hear', 579), ('counted', 580), ('transactional', 581), ('books', 582), ('rather', 583), ('argue', 584), ('pi', 585), ('give', 586), ('report', 587), ('missing', 588), ('avoid', 589), ('experiencing', 590), ('autumn', 591), ('fire', 592), ('sales', 593), ('schedule', 594), ('pickup', 595), ('long', 596), ('amex', 597), ('trouble', 598), ('getting', 599), ('near', 600), ('worst', 601), ('ever', 602), ('lied', 603), ('requested', 604), ('oct', 605), ('reach', 606), ('capable', 607), ('regarding', 608), ('senior', 609), ('manager', 610), ('prob', 611), ('mentioning', 612), ('spoke', 613), ('mind', 614), ('paying', 615), ('shared', 616), ('quoted', 617), ('guaranteed', 618), ('dispatched', 619), ('monday', 620), ('glitch', 621), ('warning', 622), ('lot', 623), ('cod', 624), ('placing', 625), ('returnrefund', 626), ('wrong', 627), ('packed', 628), ('amazonseller', 629), ('returns', 630), ('talking', 631), ('look', 632), ('history', 633), ('useless', 634), ('different', 635), ('comment', 636), ('excuse', 637), ('cashback', 638), ('ref', 639), ('considering', 640), ('effort', 641), ('makes', 642), ('click', 643), ('amp', 644), ('fill', 645), ('recd', 646), ('sue', 647), ('weekly', 648), ('impressed', 649), ('speaking', 650), ('assistants', 651), ('regards', 652), ('packaging', 653), ('hv', 654), ('phd', 655), ('d', 656), ('r', 657), ('totally', 658), ('unrelated', 659), ('common', 660), ('sense', 661), ('fed', 662), ('mails', 663), ('widout', 664), ('results', 665), ('scream', 666), ('twitter', 667), ('void', 668), ('delete', 669), ('credit', 670), ('card', 671), ('charge', 672), ('stuff', 673), ('go', 674), ('erin', 675), ('brokovich', 676), ('answer', 677), ('question', 678), ('pending', 679), ('consistent', 680), ('wlisting', 681), ('policy', 682), ('enquiry', 683), ('mohankrishna', 684), ('replied', 685), ('transit', 686), ('pity', 687), ('organisation', 688), ('set', 689), ('locked', 690), ('cancelled', 691), ('debited', 692), ('replace', 693), ('charged', 694), ('double', 695), ('overall', 696), ('buy', 697), ('damnedest', 698), ('hp', 699), ('laptop', 700), ('firefox', 701), ('pictures', 702), ('project', 703), ('drive', 704), ('needed', 705), ('nope', 706), ('correct', 707), ('infact', 708), ('great', 709), ('delivers', 710), ('fraud', 711), ('suggesting', 712), ('mobiles', 713), ('name', 714), ('offers', 715), ('letter', 716), ('markdown', 717), ('brilliant', 718), ('noone', 719), ('phone', 720), ('salutations', 721), ('taken', 722), ('checked', 723), ('remove', 724), ('incorrect', 725), ('eu', 726), ('chargers', 727), ('complete', 728), ('completed', 729), ('fingers', 730), ('crossed', 731), ('assurance', 732), ('wait', 733), ('stooped', 734), ('low', 735), ('provided', 736), ('central', 737), ('deposit', 738), ('confirm', 739), ('beware', 740), ('bluffs', 741), ('sc', 742), ('harassment', 743), ('accepting', 744), ('buyers', 745), ('harassed', 746), ('market', 747), ('performance', 748), ('buying', 749), ('manned', 750), ('office', 751), ('yet', 752), ('claim', 753), ('ok', 754), ('video', 755), ('games', 756), ('m', 757), ('damned', 758), ('ds', 759), ('tym', 760), ('act', 761), ('filthy', 762), ('mailed', 763), ('prority', 764), ('looks', 765), ('zef', 766), ('forward', 767), ('billed', 768), ('previously', 769), ('left', 770), ('finally', 771), ('pckg', 772), ('declared', 773), ('undeliverable', 774), ('reasons', 775), ('seeking', 776), ('specialist', 777), ('buddy', 778), ('fixed', 779), ('hit', 780), ('birthday', 781), ('changed', 782), ('password', 783), ('calling', 784), ('minutes', 785), ('endless', 786), ('script', 787), ('completely', 788), ('grotesque', 789), ('accent', 790), ('unbearable', 791), ('haha', 792), ('nowadays', 793), ('gf', 794), ('rate', 795), ('cheats', 796), ('paid', 797), ('balance', 798), ('deducted', 799), ('simple', 800), ('purchase', 801), ('needful', 802), ('promo', 803), ('code', 804), ('correspondence', 805), ('almost', 806), ('glad', 807), ('giant', 808), ('company', 809), ('taking', 810), ('companies', 811), ('packages', 812), ('chk', 813), ('properly', 814), ('waited', 815), ('till', 816), ('become', 817), ('richman', 818), ('query', 819), ('unanswered', 820), ('truly', 821), ('violated', 822), ('location', 823), ('played', 824), ('reimbursement', 825), ('normally', 826), ('thru', 827), ('grandsons', 828), ('payments', 829), ('declined', 830), ('cloud', 831), ('reader', 832), ('book', 833), ('bought', 834), ('linux', 835), ('installing', 836), ('wine', 837), ('expired', 838), ('update', 839), ('feedbacks', 840), ('acted', 841), ('gone', 842), ('trash', 843), ('certain', 844), ('may', 845), ('recorded', 846), ('handed', 847), ('apple', 848), ('iphone', 849), ('advance', 850), ('traking', 851), ('updated', 852), ('since', 853), ('sunday', 854), ('portal', 855), ('setting', 856), ('echo', 857), ('language', 858), ('₹', 859), ('innovative', 860), ('attract', 861), ('cart', 862), ('sometimes', 863), ('notification', 864), ('pincode', 865), ('gets', 866), ('delivrd', 867), ('waygo', 868), ('digitalisationuuttrly', 869), ('unprofessnl', 870), ('toll', 871), ('disconnects', 872), ('closed', 873), ('allow', 874), ('problems', 875), ('unacceptable', 876), ('except', 877), ('standard', 878), ('utterly', 879), ('cstmr', 880), ('emi', 881), ('power', 882), ('campaign', 883), ('asking', 884), ('send', 885), ('longer', 886), ('access', 887), ('cc', 888), ('old', 889), ('logistics', 890), ('end', 891), ('eligible', 892), ('oneplus', 893), ('launch', 894), ('oriented', 895), ('orelse', 896), ('tenure', 897), ('seen', 898), ('battery', 899), ('computerized', 900), ('clue', 901), ('meif', 902), ('actual', 903), ('canceled', 904), ('happened', 905), ('buyer', 906), ('responsible', 907), ('requesting', 908), ('strange', 909), ('cust', 910), ('saying', 911), ('failed', 912), ('choosing', 913), ('insecure', 914), ('money', 915), ('returning', 916), ('picked', 917), ('notice', 918), ('names', 919), ('escaped', 920), ('became', 921), ('irresponsible', 922), ('dispatching', 923), ('state', 924), ('half', 925), ('conversation', 926), ('via', 927), ('unmoved', 928), ('public', 929), ('appreciated', 930), ('second', 931), ('house', 932), ('resident', 933), ('needs', 934), ('nonsense', 935), ('seems', 936), ('placed', 937), ('arrive', 938), ('pm', 939), ('far', 940), ('dispatch', 941), ('courier', 942), ('atoz', 943), ('undelivered', 944), ('compensates', 945), ('interest', 946), ('associates', 947), ('respond', 948), ('impossible', 949), ('open', 950), ('andbrheybdenied', 951), ('tried', 952), ('instruction', 953), ('posted', 954), ('hell', 955), ('turning', 956), ('explain', 957), ('pathetic', 958), ('cs', 959), ('kind', 960), ('horrible', 961), ('shows', 962), ('super', 963), ('signed', 964), ('netherlands', 965), ('abut', 966), ('watch', 967), ('asks', 968), ('method', 969), ('two', 970), ('temper', 971), ('glass', 972), ('vide', 973), ('tempered', 974), ('timelines', 975), ('invoice', 976), ('january', 977), ('irritated', 978), ('harassing', 979), ('screwed', 980), ('misleading', 981), ('ready', 982), ('cut', 983), ('concerns', 984), ('kidding', 985), ('proper', 986), ('expensive', 987), ('electronic', 988), ('smh', 989), ('logged', 990), ('cheated', 991), ('translation', 992), ('availability', 993), ('protection', 994), ('plan', 995), ('originally', 996), ('stated', 997), ('nine', 998), ('claims', 999), ('soc', 1000), ('mmbr', 1001), ('gowthamhe', 1002), ('thought', 1003), ('speakers', 1004), ('intermittently', 1005), ('coz', 1006), ('st', 1007), ('reset', 1008), ('say', 1009), ('shalu', 1010), ('agarwal', 1011), ('mob', 1012), ('cross', 1013), ('belt', 1014), ('small', 1015), ('size', 1016), ('incentive', 1017), ('incent', 1018), ('leads', 1019), ('behavior', 1020), ('overwhelmed', 1021), ('revert', 1022), ('side', 1023), ('big', 1024), ('wedding', 1025), ('honeymoon', 1026), ('tactics', 1027), ('task', 1028), ('packet', 1029), ('delivering', 1030), ('mentioned', 1031), ('hired', 1032), ('robots', 1033), ('seemed', 1034), ('gibberish', 1035), ('hardly', 1036), ('understanderwhen', 1037), ('satisfaction', 1038), ('survey', 1039), ('coming', 1040), ('friend', 1041), ('machine', 1042), ('handling', 1043), ('camera', 1044), ('lens', 1045), ('empty', 1046), ('incompetent', 1047), ('employee', 1048), ('deveshsali', 1049), ('treating', 1050), ('changes', 1051), ('guides', 1052), ('farewell', 1053), ('initiated', 1054), ('cancels', 1055), ('faced', 1056), ('creditcard', 1057), ('terrible', 1058), ('false', 1059), ('including', 1060), ('pick', 1061), ('eco', 1062), ('spot', 1063), ('includes', 1064), ('black', 1065), ('multiple', 1066), ('supporting', 1067), ('warehouse', 1068), ('city', 1069), ('nov', 1070), ('hopeless', 1071), ('inefficient', 1072), ('ecr', 1073), ('askng', 1074), ('height', 1075), ('testing', 1076), ('patience', 1077), ('extended', 1078), ('reps', 1079), ('placate', 1080), ('shop', 1081), ('convenience', 1082), ('timely', 1083), ('exp', 1084), ('famp', 1085), ('iv', 1086), ('tagging', 1087), ('mystery', 1088), ('seeing', 1089), ('show', 1090), ('surprised', 1091), ('receive', 1092), ('basis', 1093), ('beacuse', 1094), ('demanding', 1095), ('sound', 1096), ('knew', 1097), ('shocked', 1098), ('dream', 1099), ('came', 1100), ('true', 1101), ('😍', 1102), ('reschedule', 1103), ('without', 1104), ('reject', 1105), ('limited', 1106), ('user', 1107), ('attach', 1108), ('pic', 1109), ('truth', 1110), ('lure', 1111), ('ahead', 1112), ('concerned', 1113), ('previous', 1114), ('wards', 1115), ('luck', 1116), ('inspection', 1117), ('procedure', 1118), ('seasonal', 1119), ('compromised', 1120), ('gr', 1121), ('amazons', 1122), ('careful', 1123), ('sme', 1124), ('careless', 1125), ('fact', 1126), ('contrary', 1127), ('mean', 1128), ('fools', 1129), ('meaning', 1130), ('estimates', 1131), ('tricky', 1132), ('unnecessary', 1133), ('questions', 1134), ('thoroughly', 1135), ('displeased', 1136), ('beyond', 1137), ('late', 1138), ('becoming', 1139), ('justify', 1140), ('lately', 1141), ('extra', 1142), ('savings', 1143), ('carriers', 1144), ('biggest', 1145), ('shut', 1146), ('morons', 1147), ('hrs', 1148), ('blocked', 1149), ('change', 1150), ('coupon', 1151), ('log', 1152), ('affiliate', 1153), ('none', 1154), ('collect', 1155), ('cheque', 1156), ('hate', 1157), ('canceling', 1158), ('good morning', 1159), ('suddenly', 1160), ('ignoring', 1161), ('safe', 1162), ('yrs', 1163), ('😡', 1164), ('ultimately', 1165), ('earlier', 1166), ('estimate', 1167), ('nevertheless', 1168), ('crm', 1169), ('faq', 1170), ('based', 1171), ('answers', 1172), ('disconnected', 1173), ('referred', 1174), ('kudos', 1175), ('nailing', 1176), ('release', 1177), ('complaining', 1178), ('pattern', 1179), ('official', 1180), ('members', 1181), ('cloudtail', 1182), ('tata', 1183), ('coffee', 1184), ('grand', 1185), ('purchasing', 1186), ('particularly', 1187), ('gold', 1188), ('seam', 1189), ('stumble', 1190), ('upon', 1191), ('characters', 1192), ('vacation', 1193), ('passed', 1194), ('advised', 1195), ('telephone', 1196), ('numerous', 1197), ('occasions', 1198), ('dumb', 1199), ('refunded', 1200), ('found', 1201), ('forum', 1202), ('bon', 1203), ('jovi', 1204), ('timeline', 1205), ('hanging', 1206), ('listen', 1207), ('replacing', 1208), ('explanation', 1209), ('page', 1210), ('submitted', 1211), ('looting', 1212), ('ireland', 1213), ('avail', 1214), ('saving', 1215), ('kindle', 1216), ('oasis', 1217), ('audible', 1218), ('download', 1219), ('audibles', 1220), ('kindles', 1221), ('wrote', 1222), ('bounced', 1223), ('unhappy', 1224), ('delayed', 1225), ('single', 1226), ('sim', 1227), ('instead', 1228), ('dual', 1229), ('combo', 1230), ('vague', 1231), ('hoping', 1232), ('amz', 1233), ('reduction', 1234), ('around', 1235), ('sir', 1236), ('spite', 1237), ('repeated', 1238), ('reminder', 1239), ('falls', 1240), ('deaf', 1241), ('ears', 1242), ('talked', 1243), ('rude', 1244), ('hang', 1245), ('listening', 1246), ('renewed', 1247), ('subscription', 1248), ('colleagues', 1249), ('investigated', 1250), ('stolen', 1251), ('creating', 1252), ('permission', 1253), ('confirmed', 1254), ('bill', 1255), ('pretty', 1256), ('complicated', 1257), ('continuation', 1258), ('ordeal', 1259), ('difficulty', 1260), ('desk', 1261), ('mention', 1262), ('furtheris', 1263), ('fair', 1264), ('ebay', 1265), ('sell', 1266), ('platform', 1267), ('student', 1268), ('afford', 1269), ('renewal', 1270), ('replaced', 1271), ('genuine', 1272), ('maybe', 1273), ('breath', 1274), ('returned', 1275), ('awb', 1276), ('girl', 1277), ('amount', 1278), ('recent', 1279), ('absolutely', 1280), ('encounter', 1281), ('unethical', 1282), ('illogical', 1283), ('ruin', 1284), ('image', 1285), ('couriers', 1286), ('aramax', 1287), ('engaged', 1288), ('credited', 1289), ('despite', 1290), ('expecting', 1291), ('type', 1292), ('stopped', 1293), ('replying', 1294), ('ship', 1295), ('upto', 1296), ('rebate', 1297), ('unpredictable', 1298), ('processes', 1299), ('killing', 1300), ('dissatisfied', 1301), ('losing', 1302), ('plot', 1303), ('ensure', 1304), ('commitment', 1305), ('honoured', 1306), ('urgently', 1307), ('operator', 1308), ('notmy', 1309), ('choice', 1310), ('helpless', 1311), ('neither', 1312), ('piece', 1313), ('good afternoon', 1314), ('calls', 1315), ('subscribing', 1316), ('sept', 1317), ('betray', 1318), ('assured', 1319), ('leadership', 1320), ('dianne', 1321), ('auto', 1322), ('nobody', 1323), ('answering', 1324), ('subscriber', 1325), ('robbed', 1326), ('pre', 1327), ('xbox', 1328), ('x', 1329), ('damage', 1330), ('cleaner', 1331), ('three', 1332), ('fourth', 1333), ('disgusting', 1334), ('reverse', 1335), ('metro', 1336), ('kolkata', 1337), ('brought', 1338), ('disappointing', 1339), ('patheticno', 1340), ('loyalty', 1341), ('situation', 1342), ('design', 1343), ('pages', 1344), ('annoyingly', 1345), ('huge', 1346), ('difficult', 1347), ('read', 1348), ('bolded', 1349), ('intentional', 1350), ('drops', 1351), ('ordering', 1352), ('difference', 1353), ('door', 1354), ('dogs', 1355), ('mess', 1356), ('strangers', 1357), ('predictably', 1358), ('sign', 1359), ('word', 1360), ('fifth', 1361), ('van', 1362), ('fled', 1363), ('scene', 1364), ('accident', 1365), ('insurance', 1366), ('reluctant', 1367), ('partners', 1368), ('relationship', 1369), ('disappointment', 1370), ('unused', 1371), ('portion', 1372), ('prefer', 1373), ('versus', 1374), ('confident', 1375), ('statement', 1376), ('interactions', 1377), ('department', 1378), ('solutions', 1379), ('records', 1380), ('went', 1381), ('dates', 1382), ('spend', 1383), ('jst', 1384), ('nowits', 1385), ('protective', 1386), ('padding', 1387), ('met', 1388), ('sat', 1389), ('tue', 1390), ('dec', 1391), ('alerted', 1392), ('discrimination', 1393), ('certainly', 1394), ('used', 1395), ('pop', 1396), ('vinyls', 1397), ('todaydamaged', 1398), ('painful', 1399), ('malfunctioning', 1400), ('arduous', 1401), ('arrogance', 1402), ('downfall', 1403), ('dys', 1404), ('foramp', 1405), ('wiped', 1406), ('shops', 1407), ('devices', 1408), ('count', 1409), ('amazing', 1410), ('walnut', 1411), ('full', 1412), ('bugs', 1413), ('although', 1414), ('expiry', 1415), ('away', 1416), ('manufacturer', 1417), ('world', 1418), ('exist', 1419), ('wht', 1420), ('knw', 1421), ('beenn', 1422), ('looted', 1423), ('lyf', 1424), ('hence', 1425), ('wrongly', 1426), ('rebook', 1427), ('increase', 1428), ('cost', 1429), ('tips', 1430), ('flaw', 1431), ('cords', 1432), ('ipad', 1433), ('straight', 1434), ('flatmate', 1435), ('subtitles', 1436), ('behalf', 1437), ('convoluted', 1438), ('cre', 1439), ('related', 1440), ('acreg', 1441), ('shirt', 1442), ('advertised', 1443), ('software', 1444), ('loot', 1445), ('peoplebig', 1446), ('diwali', 1447), ('hats', 1448), ('assure', 1449), ('employees', 1450), ('gift', 1451), ('philips', 1452), ('trimmer', 1453), ('niva', 1454), ('restrict', 1455), ('specific', 1456), ('household', 1457), ('inside', 1458), ('label', 1459), ('rs', 1460), ('boy', 1461), ('peoples', 1462), ('knowing', 1463), ('ignored', 1464), ('list', 1465), ('redmi', 1466), ('mic', 1467), ('smartcheck', 1468), ('pin', 1469), ('aggravated', 1470), ('amazonuk', 1471), ('bank', 1472), ('rejected', 1473), ('leaving', 1474), ('sour', 1475), ('taste', 1476), ('mouth', 1477), ('increasing', 1478), ('prices', 1479), ('season', 1480), ('appropriated', 1481), ('party', 1482), ('published', 1483), ('switched', 1484), ('matters', 1485), ('hope', 1486), ('re', 1487), ('messages', 1488), ('notify', 1489), ('applied', 1490), ('requires', 1491), ('ideas', 1492), ('amazonsuch', 1493), ('pathatic', 1494), ('knowledge', 1495), ('stsfc', 1496), ('good day', 1497), ('waterproof', 1498), ('sweat', 1499), ('wanted', 1500), ('brands', 1501), ('solved', 1502), ('separators', 1503), ('unusable', 1504), ('wow', 1505), ('boxes', 1506), ('plastic', 1507), ('foam', 1508), ('rubber', 1509), ('business', 1510), ('efficient', 1511), ('chose', 1512), ('ampm', 1513), ('shit', 1514), ('together', 1515), ('reputation', 1516), ('cd', 1517), ('magazines', 1518), ('exact', 1519), ('similar', 1520), ('festive', 1521), ('gifts', 1522), ('surprisingly', 1523), ('agents', 1524), ('exactly', 1525), ('line', 1526), ('signature', 1527), ('bots', 1528), ('fee', 1529), ('chance', 1530), ('nocost', 1531), ('applicable', 1532), ('flipkart', 1533), ('whining', 1534), ('expect', 1535), ('unprofessional', 1536), ('uneducated', 1537), ('accepted', 1538), ('👍', 1539), ('🏻', 1540), ('opened', 1541), ('mistake', 1542), ('🙏', 1543), ('landmark', 1544), ('global', 1545), ('per', 1546), ('terms', 1547), ('rest', 1548), ('phonewhy', 1549), ('copy', 1550), ('bw', 1551), ('sms', 1552), ('earliest', 1553), ('fuck', 1554), ('dnt', 1555), ('cl', 1556), ('prise', 1557), ('checking', 1558), ('four', 1559), ('pkg', 1560), ('occurred', 1561), ('flase', 1562), ('tag', 1563), ('buks', 1564), ('charges', 1565), ('salute', 1566), ('clues', 1567), ('facility', 1568), ('thrown', 1569), ('ink', 1570), ('bottle', 1571), ('wish', 1572), ('gap', 1573), ('upset', 1574), ('competitors', 1575), ('vendor', 1576), ('express', 1577), ('holding', 1578), ('msgs', 1579), ('accounts', 1580), ('se', 1581), ('omg', 1582), ('fighting', 1583), ('august', 1584), ('blame', 1585), ('bargain', 1586), ('cx', 1587), ('serv', 1588), ('richmond', 1589), ('toronto', 1590), ('ground', 1591), ('👇', 1592), ('recieved', 1593), ('corner', 1594), ('dissapoint', 1595), ('moves', 1596), ('distance', 1597), ('opposite', 1598), ('direction', 1599), ('destination', 1600), ('surprise', 1601), ('flop', 1602), ('movie', 1603), ('upload', 1604), ('irritatng', 1605), ('displayed', 1606), ('inr', 1607), ('news', 1608), ('make', 1609), ('transaction', 1610), ('micromax', 1611), ('ke', 1612), ('wale', 1613), ('dabbe', 1614), ('kewal', 1615), ('chargerdabba', 1616), ('khali', 1617), ('bina', 1618), ('ka', 1619), ('plzz', 1620), ('preorder', 1621), ('top', 1622), ('reduced', 1623), ('stalling', 1624), ('lives', 1625), ('upi', 1626), ('december', 1627), ('relied', 1628), ('cards', 1629), ('hacked', 1630), ('duping', 1631), ('scheme', 1632), ('dlvrd', 1633), ('reorder', 1634), ('hung', 1635), ('generic', 1636), ('contents', 1637), ('adding', 1638), ('intend', 1639), ('polling', 1640), ('kannada', 1641), ('easier', 1642), ('signing', 1643), ('toy', 1644), ('dries', 1645), ('october', 1646), ('sbi', 1647), ('debit', 1648), ('electronically', 1649), ('generated', 1650), ('sole', 1651), ('considered', 1652), ('secretprivate', 1653), ('manage', 1654), ('recordings', 1655), ('intelcom', 1656), ('playing', 1657), ('air', 1658), ('lst', 1659), ('indian', 1660), ('explained', 1661), ('expedited', 1662), ('put', 1663), ('firm', 1664), ('refunds', 1665), ('questionare', 1666), ('legally', 1667), ('allowed', 1668), ('printed', 1669), ('yesno', 1670), ('original', 1671), ('todays', 1672), ('deals', 1673), ('category', 1674), ('fax', 1675), ('secured', 1676), ('documents', 1677), ('supervisor', 1678), ('mysteriously', 1679), ('owner', 1680), ('extremely', 1681), ('frustrated', 1682), ('otherwise', 1683), ('compelled', 1684), ('attends', 1685), ('helpline', 1686), ('imei', 1687), ('yea', 1688), ('often', 1689), ('dear', 1690), ('headphone', 1691), ('lower', 1692), ('laterin', 1693), ('large', 1694), ('shock', 1695), ('horror', 1696), ('assurances', 1697), ('starving', 1698), ('staff', 1699), ('run', 1700), ('dark', 1701), ('rob', 1702), ('vunerable', 1703), ('messing', 1704), ('driving', 1705), ('crazy', 1706), ('bother', 1707), ('french', 1708), ('custommer', 1709), ('respect', 1710), ('law', 1711), ('bd', 1712), ('approval', 1713), ('parceli', 1714), ('reclaim', 1715), ('nearing', 1716), ('´', 1717), ('within', 1718), ('mine', 1719), ('intention', 1720), ('disturb', 1721), ('costed', 1722), ('personal', 1723), ('faster', 1724), ('switchboard', 1725), ('extension', 1726), ('shoes', 1727), ('self', 1728), ('wil', 1729), ('reimburse', 1730), ('denying', 1731), ('delaying', 1732), ('network', 1733), ('discussion', 1734), ('logging', 1735), ('landlord', 1736), ('outcome', 1737), ('suffer', 1738), ('cuz', 1739), ('feeling', 1740), ('refer', 1741), ('deliverdmailed', 1742), ('pantry', 1743), ('accept', 1744), ('topped', 1745), ('servicesadvise', 1746), ('subscribe', 1747), ('save', 1748), ('cancelling', 1749), ('future', 1750), ('preorders', 1751), ('arrival', 1752), ('fluke', 1753), ('az', 1754), ('anywhere', 1755), ('specified', 1756), ('seem', 1757), ('intervention', 1758), ('issued', 1759), ('loss', 1760), ('ypu', 1761), ('honest', 1762), ('promptly', 1763), ('ivr', 1764), ('record', 1765), ('reached', 1766), ('canned', 1767), ('queue', 1768), ('learn', 1769), ('hands', 1770), ('best', 1771), ('believe', 1772), ('sufficient', 1773), ('colleges', 1774), ('lying', 1775), ('sumithra', 1776), ('thomas', 1777), ('surely', 1778), ('pass', 1779), ('coach', 1780), ('axis', 1781), ('showed', 1782), ('wtf', 1783), ('marketplace', 1784), ('lie', 1785), ('escalations', 1786), ('training', 1787), ('mall', 1788), ('sit', 1789), ('behind', 1790), ('noreply', 1791), ('addresses', 1792), ('😠', 1793), ('mi', 1794), ('memebership', 1795), ('result', 1796), ('friendly', 1797), ('approach', 1798), ('satisfactory', 1799), ('ll', 1800), ('sep', 1801), ('preaching', 1802), ('switch', 1803), ('china', 1804), ('post', 1805), ('avaliable', 1806), ('pampers', 1807), ('premium', 1808), ('taille', 1809), ('diapers', 1810), ('explanations', 1811), ('apologize', 1812), ('additional', 1813), ('subscribeampsave', 1814), ('create', 1815), ('close', 1816), ('incoming', 1817), ('valuable', 1818), ('parcels', 1819), ('indiapost', 1820), ('shoutout', 1821), ('rashmi', 1822), ('calming', 1823), ('school', 1824), ('managers', 1825), ('promising', 1826), ('failing', 1827), ('presume', 1828), ('understanding', 1829), ('policies', 1830), ('mental', 1831), ('ampecinomical', 1832), ('events', 1833), ('doubt', 1834), ('level', 1835), ('competency', 1836), ('material', 1837), ('turned', 1838), ('navigational', 1839), ('restarted', 1840), ('talks', 1841), ('remote', 1842), ('aware', 1843), ('urs', 1844), ('web', 1845), ('rectify', 1846), ('hot', 1847), ('zone', 1848), ('rules', 1849), ('guidelines', 1850), ('chennai', 1851), ('resume', 1852), ('usual', 1853), ('elaborate', 1854), ('preordered', 1855), ('released', 1856), ('overkilled', 1857), ('pet', 1858), ('chicken', 1859), ('acceptable', 1860), ('landover', 1861), ('locations', 1862), ('tired', 1863), ('reporting', 1864), ('inaccurate', 1865), ('searching', 1866), ('yahoo', 1867), ('uses', 1868), ('decided', 1869), ('permanently', 1870), ('shitty', 1871), ('unfortunately', 1872), ('cancle', 1873), ('deliverd', 1874), ('row', 1875), ('somethingso', 1876), ('identify', 1877), ('bits', 1878), ('blocks', 1879), ('shame', 1880), ('partially', 1881), ('whatsoever', 1882), ('wife', 1883), ('necessary', 1884), ('tweet', 1885), ('harsheen', 1886), ('ab', 1887), ('extreme', 1888), ('gym', 1889), ('equipmenti', 1890), ('broken', 1891), ('saturday', 1892), ('unhelpful', 1893), ('behaviour', 1894), ('wardrobe', 1895), ('slow', 1896), ('sounds', 1897), ('drunk', 1898), ('looong', 1899), ('bck', 1900), ('registration', 1901), ('scheduled', 1902), ('furniture', 1903), ('assembly', 1904), ('move', 1905), ('current', 1906), ('states', 1907), ('norwegian', 1908), ('audio', 1909), ('english', 1910), ('cast', 1911), ('ah', 1912), ('restrictive', 1913), ('apology', 1914), ('eye', 1915), ('sp', 1916), ('vivo', 1917), ('waits', 1918), ('arrives', 1919), ('agree', 1920), ('decides', 1921), ('resend', 1922), ('der', 1923), ('wit', 1924), ('comp', 1925), ('nd', 1926), ('acknowledge', 1927), ('nt', 1928), ('carry', 1929), ('assume', 1930), ('sstore', 1931), ('xx', 1932), ('dd', 1933), ('helped', 1934), ('fulfilling', 1935), ('commitments', 1936), ('compny', 1937), ('allowing', 1938), ('dis', 1939), ('saturdaynd', 1940), ('approx', 1941), ('rough', 1942), ('wants', 1943), ('verification', 1944), ('store', 1945), ('course', 1946), ('hassles', 1947), ('publish', 1948), ('explaining', 1949), ('farming', 1950), ('tiny', 1951), ('miles', 1952), ('suggestionbugcorrection', 1953), ('lightning', 1954), ('becomes', 1955), ('del', 1956), ('staffs', 1957), ('worth', 1958), ('goes', 1959), ('frauded', 1960), ('fyi', 1961), ('reauthorise', 1962), ('hunt', 1963), ('disregard', 1964), ('appliances', 1965), ('shakes', 1966), ('consumers', 1967), ('confidence', 1968), ('covered', 1969), ('fooled', 1970), ('complained', 1971), ('irony', 1972), ('body', 1973), ('cared', 1974), ('suffering', 1975), ('🤔', 1976), ('step', 1977), ('sucks', 1978), ('neverending', 1979), ('moreover', 1980), ('rescinded', 1981), ('attempt', 1982), ('basics', 1983), ('cable', 1984), ('red', 1985), ('sticker', 1986), ('serial', 1987), ('directly', 1988), ('reaching', 1989), ('wary', 1990), ('annoyed', 1991), ('movies', 1992), ('loops', 1993), ('screen', 1994), ('bet', 1995), ('attempts', 1996), ('starting', 1997), ('lose', 1998), ('described', 1999), ('chatting', 2000), ('execs', 2001), ('disillusioned', 2002), ('refused', 2003), ('fast', 2004), ('marble', 2005), ('fantastic', 2006), ('total', 2007), ('failure', 2008), ('apni', 2009), ('dukaan', 2010), ('forms', 2011), ('area', 2012), ('ac', 2013), ('fad', 2014), ('ruthless', 2015), ('cheaters', 2016), ('dumbest', 2017), ('good night', 2018), ('excellent', 2019), ('letters', 2020), ('tc', 2021), ('write', 2022), ('operators', 2023), ('color', 2024), ('smaller', 2025), ('whole', 2026), ('faulty', 2027), ('contacts', 2028), ('advisors', 2029), ('supervisors', 2030), ('latest', 2031), ('h', 2032), ('honour', 2033), ('tampc', 2034), ('crap', 2035), ('security', 2036), ('dosent', 2037), ('tracks', 2038), ('excessive', 2039), ('rescheduling', 2040), ('hand', 2041), ('heart', 2042), ('right', 2043), ('standards', 2044), ('yo', 2045), ('allows', 2046), ('friday', 2047), ('discounts', 2048), ('oh', 2049), ('yeah', 2050), ('fianc', 2051), ('medical', 2052), ('equipment', 2053), ('breathe', 2054), ('night', 2055), ('text', 2056), ('links', 2057), ('projection', 2058), ('unchanged', 2059), ('truthfully', 2060), ('worked', 2061), ('indians', 2062), ('worried', 2063), ('value', 2064), ('charging', 2065), ('takes', 2066), ('mother', 2067), ('quicker', 2068), ('model', 2069), ('temporarily', 2070), ('suspended', 2071), ('reactivate', 2072), ('exit', 2073), ('quantities', 2074), ('coordination', 2075), ('requests', 2076), ('authorisation', 2077), ('overcharge', 2078), ('bajaj', 2079), ('finserv', 2080), ('oneplust', 2081), ('didcheck', 2082), ('fucking', 2083), ('bandwidth', 2084), ('else', 2085), ('funky', 2086), ('tvs', 2087), ('hardik', 2088), ('patel', 2089), ('saller', 2090), ('paymant', 2091), ('whoa', 2092), ('c', 2093), ('ended', 2094), ('sooner', 2095), ('wd', 2096), ('hr', 2097), ('thrs', 2098), ('waitlist', 2099), ('smthings', 2100), ('several', 2101), ('tweets', 2102), ('chats', 2103), ('accountable', 2104), ('fulfillment', 2105), ('centre', 2106), ('blue', 2107), ('dart', 2108), ('preferences', 2109), ('restrictions', 2110), ('noted', 2111), ('five', 2112), ('registered', 2113), ('scenario', 2114), ('goods', 2115), ('whch', 2116), ('kms', 2117), ('luks', 2118), ('walking', 2119), ('appreciating', 2120), ('quick', 2121), ('unusual', 2122), ('cite', 2123), ('noticed', 2124), ('ad', 2125), ('intentionally', 2126), ('evidenceswaited', 2127), ('prescribed', 2128), ('instructed', 2129), ('numbers', 2130), ('rupay', 2131), ('impatient', 2132), ('gst', 2133), ('mandatory', 2134), ('stuck', 2135), ('organization', 2136), ('following', 2137), ('tomm', 2138), ('headed', 2139), ('fulfilment', 2140), ('urgency', 2141), ('shoe', 2142), ('priced', 2143), ('offering', 2144), ('queen', 2145), ('bed', 2146), ('note', 2147), ('commercial', 2148), ('bag', 2149), ('seperately', 2150), ('ko', 2151), ('karne', 2152), ('bhi', 2153), ('vaale', 2154), ('ho', 2155), ('ya', 2156), ('ye', 2157), ('bnaya', 2158), ('hua', 2159), ('hai', 2160), ('ki', 2161), ('baar', 2162), ('yehi', 2163), ('dena', 2164), ('offered', 2165), ('bundled', 2166), ('ahows', 2167), ('saudi', 2168), ('arabia', 2169), ('view', 2170), ('google', 2171), ('alexa', 2172), ('calendars', 2173), ('honor', 2174), ('otp', 2175), ('ashamed', 2176), ('separate', 2177), ('nth', 2178), ('apologetic', 2179), ('utter', 2180), ('followed', 2181), ('⭐', 2182), ('️', 2183), ('❗', 2184), ('tracked', 2185), ('postal', 2186), ('dublin', 2187), ('depot', 2188), ('cheater', 2189), ('convince', 2190), ('husband', 2191), ('associated', 2192), ('emailid', 2193), ('bloody', 2194), ('creditcardif', 2195), ('knows', 2196), ('err', 2197), ('tsup', 2198), ('apparent', 2199), ('wallet', 2200), ('various', 2201), ('delviering', 2202), ('authorization', 2203), ('mailing', 2204), ('acknowledgement', 2205), ('pack', 2206), ('kids', 2207), ('broke', 2208), ('tab', 2209), ('persons', 2210), ('diyar', 2211), ('texts', 2212), ('tel', 2213), ('consistently', 2214), ('part', 2215), ('managed', 2216), ('whoever', 2217), ('ticket', 2218), ('quite', 2219), ('annoying', 2220), ('insist', 2221), ('sprite', 2222), ('ps', 2223), ('amit', 2224), ('angry', 2225), ('inability', 2226), ('packing', 2227), ('torch', 2228), ('unpacking', 2229), ('lies', 2230), ('valid', 2231), ('brand', 2232), ('esteemed', 2233), ('proudly', 2234), ('select', 2235), ('locker', 2236), ('luxembourg', 2237), ('exists', 2238), ('zip', 2239), ('marked', 2240), ('lehigh', 2241), ('valley', 2242), ('thrice', 2243), ('tenor', 2244), ('denial', 2245), ('abuse', 2246), ('shipments', 2247), ('jeeze', 2248), ('chill', 2249), ('😂', 2250), ('added', 2251), ('rply', 2252), ('serious', 2253), ('strict', 2254), ('mood', 2255), ('orderno', 2256), ('sellerwithout', 2257), ('display', 2258), ('defect', 2259), ('addressed', 2260), ('propose', 2261), ('sony', 2262), ('rescheduled', 2263), ('regulation', 2264), ('lenovo', 2265), ('😒', 2266), ('scenarios', 2267), ('dispute', 2268), ('shipoment', 2269), ('lacking', 2270), ('windows', 2271), ('eyeliner', 2272), ('ar', 2273), ('blackberry', 2274), ('keyone', 2275), ('galaxy', 2276), ('contests', 2277), ('winners', 2278), ('shall', 2279), ('sick', 2280), ('partner', 2281), ('consent', 2282), ('weather', 2283), ('personally', 2284), ('morrow', 2285), ('nuisance', 2286), ('earth', 2287), ('beggars', 2288), ('suspend', 2289), ('willing', 2290), ('colours', 2291), ('legible', 2292), ('untimely', 2293), ('pain', 2294), ('yday', 2295), ('diasater', 2296), ('patna', 2297), ('exhibition', 2298), ('road', 2299), ('disable', 2300), ('cabin', 2301), ('initiation', 2302), ('era', 2303), ('promoted', 2304), ('stands', 2305), ('involved', 2306), ('outside', 2307), ('looked', 2308), ('vehicles', 2309), ('front', 2310), ('vans', 2311), ('unlike', 2312), ('vehicle', 2313), ('yep', 2314), ('thth', 2315), ('odd', 2316), ('constantly', 2317), ('liquid', 2318), ('zs', 2319), ('crashing', 2320), ('personnel', 2321), ('closing', 2322), ('downright', 2323), ('silly', 2324), ('installation', 2325), ('pasted', 2326), ('ridiculous', 2327), ('water', 2328), ('non', 2329), ('functioning', 2330), ('scrap', 2331), ('known', 2332), ('psp', 2333), ('pro', 2334), ('chasing', 2335), ('admit', 2336), ('daysand', 2337), ('arrogant', 2338), ('tells', 2339), ('elsewhere', 2340), ('picking', 2341), ('vikas', 2342), ('thakur', 2343), ('shocking', 2344), ('cheat', 2345), ('hardcore', 2346), ('lover', 2347), ('paperwhite', 2348), ('parts', 2349), ('providedall', 2350), ('exucative', 2351), ('alsi', 2352), ('hike', 2353), ('rupees', 2354), ('honestly', 2355), ('solely', 2356), ('guyz', 2357), ('fo', 2358), ('buyprice', 2359), ('fraudulent', 2360), ('kick', 2361), ('cars', 2362), ('reordered', 2363), ('themseems', 2364), ('dog', 2365), ('food', 2366), ('pair', 2367), ('socks', 2368), ('rang', 2369), ('💯', 2370), ('yesi', 2371), ('pswd', 2372), ('forgot', 2373), ('nightmare', 2374), ('kdp', 2375), ('shitshow', 2376), ('somewhere', 2377), ('randomly', 2378), ('probably', 2379), ('saved', 2380), ('consequences', 2381), ('deborah', 2382), ('dicrespectful', 2383), ('perk', 2384), ('photos', 2385), ('firestick', 2386), ('creepy', 2387), ('thricebut', 2388), ('incorrectly', 2389), ('pretend', 2390), ('either', 2391), ('hire', 2392), ('decent', 2393), ('hes', 2394), ('manning', 2395), ('active', 2396), ('begining', 2397), ('respected', 2398), ('possitively', 2399), ('gb', 2400), ('symbol', 2401), ('spin', 2402), ('brother', 2403), ('halifax', 2404), ('interesting', 2405), ('loop', 2406), ('disconnect', 2407), ('disappoint', 2408), ('continuous', 2409), ('biz', 2410), ('unpleasant', 2411), ('exprerience', 2412), ('instance', 2413), ('goofed', 2414), ('mediocre', 2415), ('legitimate', 2416), ('reimbursed', 2417), ('secure', 2418), ('planning', 2419), ('issuer', 2420), ('duplicate', 2421), ('suspicious', 2422), ('activity', 2423), ('print', 2424), ('tweeted', 2425), ('whatever', 2426), ('mistakes', 2427), ('unsuccessful', 2428), ('round', 2429), ('circles', 2430), ('tedious', 2431), ('potential', 2432), ('thinks', 2433), ('ugh', 2434), ('quit', 2435), ('🤷', 2436), ('♀', 2437), ('pics', 2438), ('stood', 2439), ('pulled', 2440), ('took', 2441), ('picture', 2442), ('sadly', 2443), ('buck', 2444), ('disgusted', 2445), ('reqst', 2446), ('rinse', 2447), ('lather', 2448), ('repeat', 2449), ('ltd', 2450), ('jere', 2451), ('amzon', 2452), ('intimating', 2453), ('miss', 2454), ('lasership', 2455), ('lazy', 2456), ('witnessed', 2457), ('throwing', 2458), ('sharing', 2459), ('disaster', 2460), ('estimated', 2461), ('traceable', 2462), ('sorting', 2463), ('everybodys', 2464), ('short', 2465), ('trending', 2466), ('across', 2467), ('psn', 2468), ('😣', 2469), ('directions', 2470), ('dussapointing', 2471), ('visa', 2472), ('rewards', 2473), ('deliveredst', 2474), ('shockedeven', 2475), ('sureso', 2476), ('availablecan', 2477), ('guysbecause', 2478), ('patheticcan', 2479), ('quickly', 2480), ('contactbecause', 2481), ('sitting', 2482), ('quiet', 2483), ('worlds', 2484), ('centric', 2485), ('valued', 2486), ('demand', 2487), ('striked', 2488), ('fooling', 2489), ('hilarious', 2490), ('cb', 2491), ('gud', 2492), ('co', 2493), ('acknowledging', 2494), ('✌', 2495), ('private', 2496), ('comfortable', 2497), ('publicly', 2498), ('heard', 2499), ('locally', 2500), ('entertain', 2501), ('tba', 2502), ('fruitful', 2503), ('letting', 2504), ('serviceu', 2505), ('words', 2506), ('issuecant', 2507), ('satisfy', 2508), ('shortage', 2509), ('trees', 2510), ('overkill', 2511), ('refusal', 2512), ('idiots', 2513), ('stocknis', 2514), ('repeatedly', 2515), ('advertising', 2516), ('processing', 2517), ('thn', 2518), ('occasionyou', 2519), ('ruined', 2520), ('gave', 2521), ('untrained', 2522), ('educational', 2523), ('avoided', 2524), ('oid', 2525), ('station', 2526), ('currently', 2527), ('recieving', 2528), ('favorable', 2529), ('reship', 2530), ('reviewers', 2531), ('swiping', 2532), ('canada', 2533), ('callback', 2534), ('virus', 2535), ('downloading', 2536), ('responds', 2537), ('yodel', 2538), ('warehouses', 2539), ('ohio', 2540), ('pitiful', 2541), ('commandeered', 2542), ('receipts', 2543), ('linked', 2544), ('printer', 2545), ('register', 2546), ('awful', 2547), ('stamped', 2548), ('practices', 2549), ('escalated', 2550), ('unresolved', 2551), ('advantage', 2552), ('dissapointed', 2553), ('sorted', 2554), ('decisive', 2555), ('improvement', 2556), ('nouse', 2557), ('retweeted', 2558), ('play', 2559), ('retail', 2560), ('inconvenient', 2561), ('defeats', 2562), ('trusted', 2563), ('octi', 2564), ('funny', 2565), ('oneclick', 2566), ('discuss', 2567), ('control', 2568), ('suosneion', 2569), ('madden', 2570), ('imp', 2571), ('desperately', 2572), ('kills', 2573), ('owning', 2574), ('planned', 2575), ('reachable', 2576), ('deliverable', 2577), ('snapshot', 2578), ('lg', 2579), ('perfect', 2580), ('game', 2581), ('vain', 2582), ('highly', 2583), ('intact', 2584), ('avid', 2585), ('users', 2586), ('warn', 2587), ('flaws', 2588), ('revise', 2589), ('priority', 2590), ('tickets', 2591), ('activate', 2592), ('search', 2593), ('mov', 2594), ('subscriptions', 2595), ('ruining', 2596), ('declineing', 2597), ('along', 2598), ('channels', 2599), ('paper', 2600), ('laws', 2601), ('greiverances', 2602), ('kept', 2603), ('deadline', 2604), ('automatic', 2605), ('obvious', 2606), ('min', 2607), ('ss', 2608), ('countless', 2609), ('prepaid', 2610), ('present', 2611), ('nearly', 2612), ('midnight', 2613), ('faxing', 2614), ('competent', 2615), ('head', 2616), ('thi', 2617), ('implement', 2618), ('muting', 2619), ('muffle', 2620), ('twitterrific', 2621), ('hd', 2622), ('handsfree', 2623), ('practice', 2624), ('doorsteps', 2625), ('sheer', 2626), ('whichever', 2627), ('distribution', 2628), ('united', 2629), ('statescanada', 2630), ('awesome', 2631), ('wondering', 2632), ('adopted', 2633), ('hostingcloud', 2634), ('dint', 2635), ('classic', 2636), ('refusing', 2637), ('👋', 2638), ('🏼', 2639), ('screenshot', 2640), ('samsung', 2641), ('stereotype', 2642), ('professionalism', 2643), ('cat', 2644), ('poop', 2645), ('means', 2646), ('tuesday', 2647), ('prepare', 2648), ('spare', 2649), ('room', 2650), ('moved', 2651), ('templates', 2652), ('responses', 2653), ('submit', 2654), ('companyyou', 2655), ('spain', 2656), ('flagged', 2657), ('description', 2658), ('matching', 2659), ('practical', 2660), ('terminating', 2661), ('inform', 2662), ('aquaguard', 2663), ('purifier', 2664), ('mar', 2665), ('property', 2666), ('accidentally', 2667), ('appears', 2668), ('moto', 2669), ('g', 2670), ('plus', 2671), ('mobilebut', 2672), ('everyday', 2673), ('worthless', 2674), ('cares', 2675), ('definitely', 2676), ('financial', 2677), ('hesitant', 2678), ('thief', 2679), ('whose', 2680), ('invoices', 2681), ('purchases', 2682), ('dealsand', 2683), ('primeif', 2684), ('lip', 2685), ('suffered', 2686), ('harrassment', 2687), ('disrespectful', 2688), ('ball', 2689), ('paly', 2690), ('emotion', 2691), ('child', 2692), ('responsegood', 2693), ('fb', 2694), ('mnc', 2695), ('pl', 2696), ('indepth', 2697), ('conflicting', 2698), ('loading', 2699), ('jump', 2700), ('likely', 2701), ('scam', 2702), ('running', 2703), ('continuously', 2704), ('cooler', 2705), ('master', 2706), ('ndth', 2707), ('nextday', 2708), ('anybody', 2709), ('flex', 2710), ('paste', 2711), ('bether', 2712), ('hundreds', 2713), ('⊙', 2714), ('☉', 2715), ('eventually', 2716), ('neighborhood', 2717), ('steel', 2718), ('latter', 2719), ('sums', 2720), ('techie', 2721), ('damaging', 2722), ('pleased', 2723), ('woeful', 2724), ('section', 2725), ('hole', 2726), ('nothingness', 2727), ('challenging', 2728), ('creditcardi', 2729), ('receiving', 2730), ('attempting', 2731), ('fresh', 2732), ('coordinate', 2733), ('ducking', 2734), ('selected', 2735), ('asin', 2736), ('images', 2737), ('higher', 2738), ('somethingmy', 2739), ('mom', 2740), ('automatically', 2741), ('scared', 2742), ('fortune', 2743), ('router', 2744), ('mt', 2745), ('neighbors', 2746), ('directs', 2747), ('settings', 2748), ('um', 2749), ('tryst', 2750), ('debitcredit', 2751), ('untile', 2752), ('fedex', 2753), ('deling', 2754), ('secent', 2755), ('wear', 2756), ('class', 2757), ('music', 2758), ('dot', 2759), ('government', 2760), ('agency', 2761), ('indicated', 2762), ('harrassing', 2763), ('window', 2764), ('awaiting', 2765), ('preparing', 2766), ('default', 2767), ('involving', 2768), ('clueless', 2769), ('thnx', 2770), ('shoppng', 2771), ('opener', 2772), ('y', 2773), ('promote', 2774), ('somebody', 2775), ('😕', 2776), ('panic', 2777), ('☺', 2778), ('benefits', 2779), ('delvry', 2780), ('attmpt', 2781), ('den', 2782), ('txt', 2783), ('sayn', 2784), ('removing', 2785), ('quantity', 2786), ('limit', 2787), ('dedicated', 2788), ('charger', 2789), ('elgato', 2790), ('capture', 2791), ('usps', 2792), ('notes', 2793), ('complicate', 2794), ('prompted', 2795), ('enter', 2796), ('spoken', 2797), ('nikon', 2798), ('canon', 2799), ('eos', 2800), ('delays', 2801), ('recevd', 2802), ('mai', 2803), ('conclude', 2804), ('hiring', 2805), ('executives', 2806), ('cutomer', 2807), ('git', 2808), ('correspondent', 2809), ('embarassed', 2810), ('amazonamp', 2811), ('experinece', 2812), ('incident', 2813), ('inflicted', 2814), ('everythingnothing', 2815), ('serviceyour', 2816), ('queries', 2817), ('photographs', 2818), ('memory', 2819), ('replacements', 2820), ('raise', 2821), ('teams', 2822), ('presence', 2823), ('oneday', 2824), ('crucial', 2825), ('wig', 2826), ('helps', 2827), ('shown', 2828), ('consignment', 2829), ('afternoon', 2830), ('digitalisation', 2831), ('uttrly', 2832), ('highlight', 2833), ('bags', 2834), ('strictly', 2835), ('reviewed', 2836), ('ruark', 2837), ('feb', 2838), ('reversed', 2839), ('proposed', 2840), ('fired', 2841), ('sends', 2842), ('csr', 2843), ('agreement', 2844), ('shipper', 2845), ('redelivered', 2846), ('besides', 2847), ('ie', 2848), ('anyhow', 2849), ('halloween', 2850), ('inspite', 2851), ('nm', 2852), ('metropolitan', 2853), ('prouduct', 2854), ('upheld', 2855), ('standing', 2856), ('headzup', 2857), ('😀', 2858), ('citing', 2859), ('justice', 2860), ('earpod', 2861), ('invent', 2862), ('understood', 2863), ('loosers', 2864), ('africa', 2865), ('trial', 2866), ('edd', 2867), ('bt', 2868), ('rqst', 2869), ('cncld', 2870), ('stating', 2871), ('stth', 2872), ('tie', 2873), ('existing', 2874), ('spent', 2875), ('apologizing', 2876), ('positive', 2877), ('resp', 2878), ('☹', 2879), ('plenty', 2880), ('squashing', 2881), ('route', 2882), ('forgotten', 2883), ('suggestion', 2884), ('dropped', 2885), ('useful', 2886), ('authority', 2887), ('supportive', 2888), ('victim', 2889), ('hospitalized', 2890), ('surgery', 2891), ('redirected', 2892), ('putting', 2893), ('clauses', 2894), ('changing', 2895), ('customergood', 2896), ('selection', 2897), ('adde', 2898), ('defected', 2899), ('beginning', 2900), ('monthly', 2901), ('nxt', 2902), ('agn', 2903), ('difrnt', 2904), ('template', 2905), ('created', 2906), ('wake', 2907), ('steal', 2908), ('infoampdrain', 2909), ('seemingly', 2910), ('errors', 2911), ('basket', 2912), ('liar', 2913), ('perishables', 2914), ('inconvenience', 2915), ('caused', 2916), ('bug', 2917), ('adds', 2918), ('uhhhgg', 2919), ('updateaction', 2920), ('ther', 2921), ('♂', 2922), ('🤦', 2923), ('depending', 2924), ('batteries', 2925), ('degraded', 2926), ('ethics', 2927), ('confused', 2928), ('tiring', 2929), ('uncertain', 2930), ('remark', 2931), ('ownership', 2932), ('professional', 2933), ('pune', 2934), ('chakan', 2935), ('expectedthe', 2936), ('transport', 2937), ('clarified', 2938), ('stillfeeling', 2939), ('rslost', 2940), ('hectic', 2941), ('unsurprisingly', 2942), ('major', 2943), ('octthen', 2944), ('thamp', 2945), ('dated', 2946), ('soonu', 2947), ('forever', 2948), ('closes', 2949), ('idle', 2950), ('despatched', 2951), ('fails', 2952), ('orderfinally', 2953), ('sub', 2954), ('par', 2955), ('rejectedso', 2956), ('thisthere', 2957), ('squeeze', 2958), ('efficency', 2959), ('madam', 2960), ('hedwig', 2961), ('dealing', 2962), ('theft', 2963), ('employer', 2964), ('insight', 2965), ('smooth', 2966), ('dollar', 2967), ('inch', 2968), ('thick', 2969), ('mattress', 2970), ('sleeping', 2971), ('negative', 2972), ('raised', 2973), ('barred', 2974), ('supportwhats', 2975), ('fone', 2976), ('falling', 2977), ('transferred', 2978), ('supervisormanager', 2979), ('avialability', 2980), ('👏', 2981), ('wt', 2982), ('reverted', 2983), ('vary', 2984), ('untrue', 2985), ('directional', 2986), ('thrujust', 2987), ('james', 2988), ('miracle', 2989), ('worker', 2990), ('stick', 2991), ('pc', 2992), ('sells', 2993), ('july', 2994), ('hld', 2995), ('pound', 2996), ('bolke', 2997), ('bhejoge', 2998), ('kya', 2999), ('unblock', 3000), ('family', 3001), ('regions', 3002), ('spam', 3003), ('vehement', 3004), ('threats', 3005), ('unreal', 3006), ('yeahh', 3007), ('aampe', 3008), ('picnic', 3009), ('travel', 3010), ('accessories', 3011), ('decorative', 3012), ('cane', 3013), ('multipurpose', 3014), ('fruits', 3015), ('vegetables', 3016), ('britches', 3017), ('malpractice', 3018), ('exploitation', 3019), ('king', 3020), ('fine', 3021), ('towers', 3022), ('verifying', 3023), ('phishing', 3024), ('lengthy', 3025), ('turn', 3026), ('space', 3027), ('bookings', 3028), ('decade', 3029), ('shameful', 3030), ('nokia', 3031), ('atleast', 3032), ('massage', 3033), ('deliveryhow', 3034), ('subtracting', 3035), ('simply', 3036), ('lessons', 3037), ('learnt', 3038), ('actions', 3039), ('crying', 3040), ('loud', 3041), ('bear', 3042), ('claptrap', 3043), ('won', 3044), ('jobs', 3045), ('completing', 3046), ('android', 3047), ('chrome', 3048), ('maintaining', 3049), ('integrity', 3050), ('visible', 3051), ('doorstep', 3052), ('knock', 3053), ('slip', 3054), ('trucks', 3055), ('evade', 3056), ('tolls', 3057), ('ages', 3058), ('patch', 3059), ('sellerorderid', 3060), ('shiptrack', 3061), ('posting', 3062), ('mostly', 3063), ('annoy', 3064), ('cleared', 3065), ('break', 3066), ('hilariously', 3067), ('initially', 3068), ('holy', 3069), ('trap', 3070), ('habit', 3071), ('pointless', 3072), ('postpone', 3073), ('saw', 3074), ('bamph', 3075), ('weirder', 3076), ('wks', 3077), ('friends', 3078), ('recipient', 3079), ('hack', 3080), ('remains', 3081), ('hub', 3082), ('forwarding', 3083), ('dealer', 3084), ('regret', 3085), ('entered', 3086), ('billing', 3087), ('meant', 3088), ('according', 3089), ('elapsed', 3090), ('ats', 3091), ('nah', 3092), ('rain', 3093), ('bribe', 3094), ('wishlist', 3095), ('confusing', 3096), ('lessor', 3097), ('peterboroughuk', 3098), ('minimum', 3099), ('visited', 3100), ('househow', 3101), ('readyseems', 3102), ('intelligencepl', 3103), ('forwards', 3104), ('sites', 3105), ('worrying', 3106), ('vulnerable', 3107), ('phonehelp', 3108), ('cannotwill', 3109), ('advisor', 3110), ('data', 3111), ('cache', 3112), ('kill', 3113), ('restart', 3114), ('outragous', 3115), ('undwrstanding', 3116), ('onyo', 3117), ('police', 3118), ('fully', 3119), ('violating', 3120), ('courage', 3121), ('okay', 3122), ('methods', 3123), ('wishlists', 3124), ('🤫', 3125), ('hallway', 3126), ('w', 3127), ('surprising', 3128), ('obviously', 3129), ('claimed', 3130), ('firstly', 3131), ('signin', 3132), ('ignore', 3133), ('rubbish', 3134), ('occasion', 3135), ('lodge', 3136), ('formal', 3137), ('limbo', 3138), ('hermes', 3139), ('childs', 3140), ('melted', 3141), ('browsers', 3142), ('genios', 3143), ('cracks', 3144), ('maquinas', 3145), ('idolos', 3146), ('putos', 3147), ('amos', 3148), ('nicely', 3149), ('exec', 3150), ('legit', 3151), ('en', 3152), ('suspect', 3153), ('hopes', 3154), ('fees', 3155), ('rarely', 3156), ('recordwhos', 3157), ('drinking', 3158), ('comments', 3159), ('rmail', 3160), ('test', 3161), ('pigeons', 3162), ('prefect', 3163), ('drone', 3164), ('favor', 3165), ('key', 3166), ('delayno', 3167), ('ky', 3168), ('ue', 3169), ('yr', 3170), ('kapil', 3171), ('eyes', 3172), ('ser', 3173), ('weekends', 3174), ('ringing', 3175), ('bell', 3176), ('b', 3177), ('verry', 3178), ('incorrectu', 3179), ('shippingbilling', 3180), ('pr', 3181), ('kr', 3182), ('rhe', 3183), ('kuch', 3184), ('koi', 3185), ('aurwhat', 3186), ('patronised', 3187), ('deleted', 3188), ('september', 3189), ('shortly', 3190), ('understatement', 3191), ('term', 3192), ('deduction', 3193), ('accountability', 3194), ('qualify', 3195), ('headphones', 3196), ('cm', 3197), ('loads', 3198), ('filler', 3199), ('ludicrous', 3200), ('contract', 3201), ('block', 3202), ('god', 3203), ('supplemented', 3204), ('surepost', 3205), ('match', 3206), ('mile', 3207), ('developers', 3208), ('teaching', 3209), ('fba', 3210), ('snatch', 3211), ('assumption', 3212), ('messed', 3213), ('publisher', 3214), ('clearer', 3215), ('liereceived', 3216), ('revision', 3217), ('manner', 3218), ('bcoz', 3219), ('love', 3220), ('timewhy', 3221), ('troubling', 3222), ('thatno', 3223), ('mark', 3224), ('suspected', 3225), ('datesa', 3226), ('tonight', 3227), ('upsusps', 3228), ('dead', 3229), ('voice', 3230), ('escaping', 3231), ('problematic', 3232), ('specify', 3233), ('diary', 3234), ('misplaced', 3235), ('quibble', 3236), ('discrepancy', 3237), ('reward', 3238), ('weak', 3239), ('gives', 3240), ('bird', 3241), ('improved', 3242), ('issueauto', 3243), ('equally', 3244), ('literally', 3245), ('restaurant', 3246), ('shouting', 3247), ('rspc', 3248), ('bit', 3249), ('fuming', 3250), ('pendrive', 3251), ('bhopal', 3252), ('mever', 3253), ('tweeting', 3254), ('filed', 3255), ('forth', 3256), ('idiotacy', 3257), ('residence', 3258), ('satisfied', 3259), ('soaking', 3260), ('wet', 3261), ('electrical', 3262), ('😩', 3263), ('hsn', 3264), ('moltul', 3265), ('mobil', 3266), ('oil', 3267), ('cotact', 3268), ('pointing', 3269), ('ugly', 3270), ('clicking', 3271), ('confirmationwarning', 3272), ('kiya', 3273), ('lekin', 3274), ('nahin', 3275), ('shots', 3276), ('confuse', 3277), ('neighbor', 3278), ('immediately', 3279), ('weird', 3280), ('addition', 3281), ('detailed', 3282), ('wtech', 3283), ('streaming', 3284), ('craps', 3285), ('phne', 3286), ('cud', 3287), ('cow', 3288), ('instant', 3289), ('gratification', 3290), ('dangerzone', 3291), ('removed', 3292), ('humans', 3293), ('aaked', 3294), ('updatecontact', 3295), ('relations', 3296), ('unsecured', 3297), ('advising', 3298), ('yethope', 3299), ('desactivated', 3300), ('blocking', 3301), ('tolerated', 3302), ('slowly', 3303), ('degrading', 3304), ('ny', 3305), ('fulfil', 3306), ('aap', 3307), ('kar', 3308), ('rahe', 3309), ('iski', 3310), ('topi', 3311), ('uske', 3312), ('apneaap', 3313), ('thak', 3314), ('chod', 3315), ('dega', 3316), ('karna', 3317), ('cheapest', 3318), ('deactivated', 3319), ('entering', 3320), ('twoday', 3321), ('prevent', 3322), ('leading', 3323), ('inputs', 3324), ('servers', 3325), ('projector', 3326), ('worry', 3327), ('radio', 3328), ('postage', 3329), ('pans', 3330), ('supplier', 3331), ('position', 3332), ('yearly', 3333), ('operated', 3334), ('powers', 3335), ('places', 3336), ('fracking', 3337), ('baby', 3338), ('seals', 3339), ('ecofriendly', 3340), ('dt', 3341), ('provides', 3342), ('titles', 3343), ('exploit', 3344), ('estimation', 3345), ('dimension', 3346), ('weight', 3347), ('limits', 3348), ('unboxed', 3349), ('mad', 3350), ('ricoh', 3351), ('sn', 3352), ('innovation', 3353), ('competitive', 3354), ('sand', 3355), ('ohhh', 3356), ('ondont', 3357), ('freakwho', 3358), ('committing', 3359), ('whether', 3360), ('😳', 3361), ('✅', 3362), ('troubleshooting', 3363), ('massive', 3364), ('pocket', 3365), ('redressal', 3366), ('unattended', 3367), ('earths', 3368), ('customercentric', 3369), ('multiorder', 3370), ('receipt', 3371), ('agreed', 3372), ('somewhat', 3373), ('disingenuous', 3374), ('located', 3375), ('especially', 3376), ('refrigerator', 3377), ('destroyed', 3378), ('explains', 3379), ('eggs', 3380), ('underneath', 3381), ('green', 3382), ('teathis', 3383), ('unlock', 3384), ('jeff', 3385), ('favourite', 3386), ('atm', 3387), ('load', 3388), ('laughing', 3389), ('swearing', 3390), ('background', 3391), ('foolish', 3392), ('frame', 3393), ('ring', 3394), ('hint', 3395), ('exchanging', 3396), ('cover', 3397), ('perfection', 3398), ('pethetic', 3399), ('listened', 3400), ('systems', 3401), ('tight', 3402), ('copycat', 3403), ('moron', 3404), ('handy', 3405), ('thumbs', 3406), ('jus', 3407), ('proceeded', 3408), ('chapter', 3409), ('writen', 3410), ('typing', 3411), ('hopefully', 3412), ('saga', 3413), ('continues', 3414), ('listens', 3415), ('documented', 3416), ('isolation', 3417), ('steps', 3418), ('fit', 3419), ('amazed', 3420), ('largest', 3421), ('blu', 3422), ('ray', 3423), ('excited', 3424), ('anxiously', 3425), ('taught', 3426), ('ow', 3427), ('nr', 3428), ('reinstall', 3429), ('style', 3430), ('samples', 3431), ('unsatisfactory', 3432), ('ios', 3433), ('username', 3434), ('dreadfully', 3435), ('necessarily', 3436), ('adjust', 3437), ('trained', 3438), ('lakhs', 3439), ('cf', 3440), ('unless', 3441), ('mr', 3442), ('nonprime', 3443), ('versed', 3444), ('strongly', 3445), ('raising', 3446), ('redirect', 3447), ('vishal', 3448), ('yadav', 3449), ('filtering', 3450), ('yup', 3451), ('choked', 3452), ('denies', 3453), ('chase', 3454), ('dad', 3455), ('psa', 3456), ('overcharging', 3457), ('wiid', 3458), ('shud', 3459), ('z', 3460), ('dtdc', 3461), ('stole', 3462), ('cleverly', 3463), ('complains', 3464), ('chromalux', 3465), ('💡', 3466), ('frostedw', 3467), ('misdelivery', 3468), ('ummno', 3469), ('regardless', 3470), ('moot', 3471), ('ukcan', 3472), ('hook', 3473), ('middle', 3474), ('acc', 3475), ('labels', 3476), ('competed', 3477), ('tht', 3478), ('despatch', 3479), ('thursday', 3480), ('reviewing', 3481), ('blessing', 3482), ('roadblock', 3483), ('assembled', 3484), ('frm', 3485), ('julie', 3486), ('band', 3487), ('primeim', 3488), ('warned', 3489), ('yiu', 3490), ('acknnowledge', 3491), ('stucked', 3492), ('shippeddelivered', 3493), ('hopeful', 3494), ('powerbank', 3495), ('neighbour', 3496), ('investigate', 3497), ('dishonest', 3498), ('alert', 3499), ('finding', 3500), ('sth', 3501), ('kyon', 3502), ('karate', 3503), ('yaar', 3504), ('subject', 3505), ('resolving', 3506), ('individual', 3507), ('subsciptions', 3508), ('subscibe', 3509), ('alright', 3510), ('lifesucking', 3511), ('banned', 3512), ('geneva', 3513), ('convention', 3514), ('experiences', 3515), ('exemplary', 3516), ('probs', 3517), ('tools', 3518), ('antiquated', 3519), ('silver', 3520), ('spoon', 3521), ('anime', 3522), ('benn', 3523), ('wid', 3524), ('compmsd', 3525), ('mebrshp', 3526), ('usd', 3527), ('apartment', 3528), ('😤', 3529), ('uninstall', 3530), ('beta', 3531), ('subtitle', 3532), ('tech', 3533), ('unaware', 3534), ('soln', 3535), ('delever', 3536), ('announce', 3537), ('outdated', 3538), ('todaybut', 3539), ('webcam', 3540), ('fulfill', 3541), ('stuffs', 3542), ('pf', 3543), ('tool', 3544), ('usa', 3545), ('hidden', 3546), ('dealt', 3547), ('broadband', 3548), ('provider', 3549), ('oops', 3550), ('sry', 3551), ('happier', 3552), ('cond', 3553), ('teachrequire', 3554), ('groceries', 3555), ('initiate', 3556), ('pushing', 3557), ('jc', 3558), ('dirty', 3559), ('sheet', 3560), ('blanket', 3561), ('iddlisbk', 3562), ('frequently', 3563), ('ajay', 3564), ('corporation', 3565), ('loved', 3566), ('trackedyou', 3567), ('url', 3568), ('correspond', 3569), ('singing', 3570), ('song', 3571), ('refundedwht', 3572), ('stone', 3573), ('cold', 3574), ('foul', 3575), ('bezos', 3576), ('messaging', 3577), ('bills', 3578), ('brings', 3579), ('management', 3580), ('plural', 3581), ('twisting', 3582), ('unauthorised', 3583), ('tryna', 3584), ('visit', 3585), ('locks', 3586), ('deregistered', 3587), ('uninstalled', 3588), ('reinstalled', 3589), ('downloads', 3590), ('aftr', 3591), ('kendrick', 3592), ('whereas', 3593), ('everytime', 3594), ('speed', 3595), ('kim', 3596), ('doa', 3597), ('bouncing', 3598), ('crushed', 3599), ('padded', 3600), ('envelope', 3601), ('sigh', 3602), ('gm', 3603), ('coin', 3604), ('flight', 3605), ('paris', 3606), ('sake', 3607), ('unsuitable', 3608), ('informing', 3609), ('luckily', 3610), ('vat', 3611), ('counter', 3612), ('showroom', 3613), ('apologised', 3614), ('fridaycyber', 3615), ('distressing', 3616), ('reflects', 3617), ('badly', 3618), ('effective', 3619), ('stacks', 3620), ('confidential', 3621), ('psvr', 3622), ('headsets', 3623), ('bundle', 3624), ('urgent', 3625), ('build', 3626), ('harder', 3627), ('keeping', 3628), ('reminds', 3629), ('airport', 3630), ('towards', 3631), ('internet', 3632), ('connection', 3633), ('assuring', 3634), ('supported', 3635), ('blunders', 3636), ('answerable', 3637), ('hide', 3638), ('map', 3639), ('bookmyshow', 3640), ('egv', 3641), ('emailing', 3642), ('nowhere', 3643), ('maintain', 3644), ('blah', 3645), ('unauthorized', 3646), ('interests', 3647), ('amzn', 3648), ('inferior', 3649), ('brief', 3650), ('infuriating', 3651), ('hearing', 3652), ('feature', 3653), ('confiscates', 3654), ('passes', 3655), ('ordr', 3656), ('v', 3657), ('wonder', 3658), ('relief', 3659), ('pen', 3660), ('refundtheir', 3661), ('possibly', 3662), ('employed', 3663), ('expnot', 3664), ('suit', 3665), ('comming', 3666), ('uncover', 3667), ('undervalued', 3668), ('penalty', 3669), ('decling', 3670), ('couldbut', 3671), ('verificationwhich', 3672), ('unreliable', 3673), ('sealed', 3674), ('experienced', 3675), ('renew', 3676), ('somehow', 3677), ('sloppy', 3678), ('mix', 3679), ('sister', 3680), ('comedy', 3681), ('ebt', 3682), ('ssi', 3683), ('jan', 3684), ('age', 3685), ('mecrap', 3686), ('alabama', 3687), ('intiated', 3688), ('hiim', 3689), ('conclusion', 3690), ('afyer', 3691), ('tax', 3692), ('hmmm', 3693), ('inhelpful', 3694), ('loose', 3695), ('fluctuation', 3696), ('customerfriendly', 3697), ('activating', 3698), ('nonetheless', 3699), ('yous', 3700), ('illegal', 3701), ('handles', 3702), ('lockers', 3703), ('walked', 3704), ('marshall', 3705), ('canclling', 3706), ('safety', 3707), ('argued', 3708), ('opcancelling', 3709), ('membershipfeel', 3710), ('credibility', 3711), ('grass', 3712), ('reality', 3713), ('competing', 3714), ('lifetime', 3715), ('earphones', 3716), ('recvd', 3717), ('answered', 3718), ('e', 3719), ('facebook', 3720), ('chain', 3721), ('initial', 3722), ('leaves', 3723), ('impression', 3724), ('bottomline', 3725), ('mercy', 3726), ('seconds', 3727), ('merit', 3728), ('rectifying', 3729), ('grateful', 3730), ('restore', 3731), ('holder', 3732), ('approximately', 3733), ('giftcard', 3734), ('gbp', 3735), ('tlkd', 3736), ('smone', 3737), ('rishabh', 3738), ('ripping', 3739), ('sandisk', 3740), ('behaves', 3741), ('rudely', 3742), ('inquiry', 3743), ('shooping', 3744), ('preferred', 3745), ('mins', 3746), ('mega', 3747), ('branding', 3748), ('reminded', 3749), ('unheard', 3750), ('vice', 3751), ('versa', 3752), ('ecom', 3753), ('unique', 3754), ('among', 3755), ('thousand', 3756), ('grabbers', 3757), ('🤬', 3758), ('attitude', 3759), ('trend', 3760), ('accessible', 3761), ('thankeew', 3762), ('timestamp', 3763), ('glitched', 3764), ('win', 3765), ('decrease', 3766), ('dirt', 3767), ('marks', 3768), ('profit', 3769), ('invite', 3770), ('bluff', 3771), ('associate', 3772), ('fragile', 3773), ('envelopes', 3774), ('packaged', 3775), ('amazonis', 3776), ('amounting', 3777), ('disclaimer', 3778), ('bureaucratic', 3779), ('rung', 3780), ('deteriorating', 3781), ('unclear', 3782), ('loaded', 3783), ('defaulted', 3784), ('net', 3785), ('banking', 3786), ('bar', 3787), ('pst', 3788), ('verify', 3789), ('shits', 3790), ('hurts', 3791), ('expectation', 3792), ('repurchase', 3793), ('elderly', 3794), ('peoplecompanies', 3795), ('stopping', 3796), ('success', 3797), ('mastercard', 3798), ('repeating', 3799), ('moving', 3800), ('😢', 3801), ('washing', 3802), ('conditions', 3803), ('incredible', 3804), ('circus', 3805), ('addressing', 3806), ('lists', 3807), ('therefore', 3808), ('living', 3809), ('street', 3810), ('edison', 3811), ('reflecting', 3812), ('pigeon', 3813), ('ccare', 3814), ('wana', 3815), ('finger', 3816), ('btwn', 3817), ('hav', 3818), ('notified', 3819), ('blissfully', 3820), ('ignorant', 3821), ('indication', 3822), ('pushed', 3823), ('screwing', 3824), ('grievance', 3825), ('funded', 3826), ('cia', 3827), ('nodal', 3828), ('misunderstood', 3829), ('mid', 3830), ('interview', 3831), ('lancaster', 3832), ('york', 3833), ('inn', 3834), ('😟', 3835), ('intervene', 3836), ('advice', 3837), ('recently', 3838), ('servicehighly', 3839), ('preference', 3840), ('dumped', 3841), ('bins', 3842), ('doors', 3843), ('arrange', 3844), ('🙄', 3845), ('car', 3846), ('manufacturing', 3847), ('dsnt', 3848), ('ceo', 3849), ('writes', 3850), ('sympathetic', 3851), ('verging', 3852), ('sarcastic', 3853), ('jurisdiction', 3854), ('presumably', 3855), ('com', 3856), ('ca', 3857), ('dropping', 3858), ('discnt', 3859), ('backed', 3860), ('froud', 3861), ('servicd', 3862), ('train', 3863), ('polices', 3864), ('orderd', 3865), ('junk', 3866), ('sollution', 3867), ('cheaper', 3868), ('lucky', 3869), ('aws', 3870), ('maintenance', 3871), ('typically', 3872), ('intentions', 3873), ('correcting', 3874), ('absolute', 3875), ('dumbness', 3876), ('startling', 3877), ('rigid', 3878), ('workingpoor', 3879), ('engineer', 3880), ('botched', 3881), ('po', 3882), ('opinion', 3883), ('currency', 3884), ('selecting', 3885), ('sheesh', 3886), ('fuzzy', 3887), ('honored', 3888), ('flat', 3889), ('moms', 3890), ('womb', 3891), ('electric', 3892), ('kettle', 3893), ('committed', 3894), ('pants', 3895), ('amanda', 3896), ('oregon', 3897), ('hotline', 3898), ('fridayonline', 3899), ('messaged', 3900), ('rating', 3901), ('locality', 3902), ('searched', 3903), ('misdelivered', 3904), ('ph', 3905), ('announced', 3906), ('contest', 3907), ('spamming', 3908), ('random', 3909), ('weightloss', 3910), ('stream', 3911), ('standup', 3912), ('apv', 3913), ('australia', 3914), ('laakhon', 3915), ('mein', 3916), ('ek', 3917), ('pleaseee', 3918), ('competence', 3919), ('trade', 3920), ('swing', 3921), ('hq', 3922), ('prints', 3923), ('leap', 3924), ('six', 3925), ('thirdforth', 3926), ('garaunteed', 3927), ('light', 3928), ('stairs', 3929), ('banged', 3930), ('christmas', 3931), ('sensitivities', 3932), ('pincodes', 3933), ('neglect', 3934), ('chargeback', 3935), ('merchant', 3936), ('tons', 3937), ('mockery', 3938), ('updating', 3939), ('prod', 3940), ('struggle', 3941), ('platforms', 3942), ('breach', 3943), ('cheatvery', 3944), ('amt', 3945), ('laughable', 3946), ('reposed', 3947), ('ecommerce', 3948), ('stored', 3949), ('hackers', 3950), ('internal', 3951), ('lethargic', 3952), ('codes', 3953), ('notifying', 3954), ('pw', 3955), ('lisiting', 3956), ('inactive', 3957), ('clarity', 3958), ('advertisements', 3959), ('focused', 3960), ('inevitable', 3961), ('profits', 3962), ('focusing', 3963), ('requirements', 3964), ('escalating', 3965), ('clarification', 3966), ('lemon', 3967), ('caring', 3968), ('counting', 3969), ('providers', 3970), ('resolutions', 3971), ('quiz', 3972), ('winner', 3973), ('ratio', 3974), ('supp', 3975), ('factor', 3976), ('auth', 3977), ('resulting', 3978), ('outright', 3979), ('gotten', 3980), ('larger', 3981), ('prevented', 3982), ('potentially', 3983), ('breaches', 3984), ('realise', 3985), ('amprequested', 3986), ('inexcusable', 3987), ('remerber', 3988), ('hacker', 3989), ('sleep', 3990), ('mechanism', 3991), ('😫', 3992), ('closure', 3993), ('predict', 3994), ('blog', 3995), ('wpast', 3996), ('pillows', 3997), ('desired', 3998), ('investigating', 3999), ('export', 4000), ('llc', 4001), ('rated', 4002), ('independent', 4003), ('chor', 4004), ('desc', 4005), ('buysays', 4006), ('plyng', 4007), ('cn', 4008), ('cr', 4009), ('scan', 4010), ('edition', 4011), ('twitch', 4012), ('kuddos', 4013), ('japan', 4014), ('issuepure', 4015), ('ignorance', 4016), ('flickering', 4017), ('sometime', 4018), ('unresponsive', 4019), ('disgrace', 4020), ('stinks', 4021), ('selll', 4022), ('usedits', 4023), ('irritate', 4024), ('screw', 4025), ('chatampwas', 4026), ('congratulate', 4027), ('jacob', 4028), ('postit', 4029), ('fridge', 4030), ('sowing', 4031), ('num', 4032), ('ewaste', 4033), ('disposal', 4034), ('town', 4035), ('shift', 4036), ('crappy', 4037), ('lowered', 4038), ('lame', 4039), ('galore', 4040), ('zero', 4041), ('verif', 4042), ('addres', 4043), ('identity', 4044), ('films', 4045), ('produced', 4046), ('nfdc', 4047), ('engage', 4048), ('buffs', 4049), ('lousy', 4050), ('dude', 4051), ('incompetence', 4052), ('scheduling', 4053), ('rectified', 4054), ('wasting', 4055), ('hm', 4056), ('intriguing', 4057), ('playlists', 4058), ('opacity', 4059), ('shortened', 4060), ('protect', 4061), ('misuse', 4062), ('suits', 4063), ('seeds', 4064), ('direct', 4065), ('huts', 4066), ('dignity', 4067), ('independence', 4068), ('subsistence', 4069), ('farmers', 4070), ('permanent', 4071), ('homeless', 4072), ('boston', 4073), ('pdf', 4074), ('dearest', 4075), ('exploded', 4076), ('shower', 4077), ('gel', 4078), ('overnighted', 4079), ('blast', 4080), ('stranger', 4081), ('oneliner', 4082), ('dialog', 4083), ('khatam', 4084), ('hone', 4085), ('baad', 4086), ('meri', 4087), ('baat', 4088), ('sun', 4089), ('lena', 4090), ('entity', 4091), ('figure', 4092), ('sameday', 4093), ('calendar', 4094), ('bangles', 4095), ('kaisy', 4096), ('reexplaining', 4097), ('transperent', 4098), ('predominently', 4099), ('voiceview', 4100), ('menu', 4101), ('frauding', 4102), ('increases', 4103), ('wen', 4104), ('thinga', 4105), ('cameras', 4106), ('friendss', 4107), ('soap', 4108), ('speaks', 4109), ('heck', 4110), ('hve', 4111), ('receivd', 4112), ('politely', 4113), ('tf', 4114), ('cctv', 4115), ('footage', 4116), ('landscape', 4117), ('mode', 4118), ('bucks', 4119), ('combined', 4120), ('nothingbut', 4121), ('tolerance', 4122), ('logitech', 4123), ('speaker', 4124), ('beacause', 4125), ('drain', 4126), ('tablet', 4127), ('reminders', 4128), ('subscribed', 4129), ('grey', 4130), ('purchasingcancel', 4131), ('planet', 4132), ('trip', 4133), ('froze', 4134), ('resigned', 4135), ('undo', 4136), ('insisting', 4137), ('prior', 4138), ('clients', 4139), ('specifically', 4140), ('resent', 4141), ('suggested', 4142), ('peopleitz', 4143), ('severe', 4144), ('indicate', 4145), ('thattheir', 4146), ('enjoying', 4147), ('widespread', 4148), ('reports', 4149), ('quickest', 4150), ('falsely', 4151), ('jacking', 4152), ('discounting', 4153), ('inspire', 4154), ('server', 4155), ('creates', 4156), ('bluedart', 4157), ('photo', 4158), ('fucked', 4159), ('anyi', 4160), ('fakely', 4161), ('donewithout', 4162), ('research', 4163), ('puerto', 4164), ('rico', 4165), ('costume', 4166), ('misleadcheat', 4167), ('indialack', 4168), ('chased', 4169), ('verified', 4170), ('webpage', 4171), ('recng', 4172), ('pkgs', 4173), ('apt', 4174), ('building', 4175), ('deadlock', 4176), ('ones', 4177), ('seperate', 4178), ('praise', 4179), ('culture', 4180), ('welcoming', 4181), ('communicating', 4182), ('confirming', 4183), ('consecutively', 4184), ('field', 4185), ('convenient', 4186), ('frozen', 4187), ('resetting', 4188), ('decide', 4189), ('plain', 4190), ('easily', 4191), ('browse', 4192), ('audiobooks', 4193), ('credits', 4194), ('fun', 4195), ('😊', 4196), ('smartphone', 4197), ('nasty', 4198), ('curious', 4199), ('wallets', 4200), ('sounded', 4201), ('coupdnt', 4202), ('pace', 4203), ('await', 4204), ('digits', 4205), ('neighbours', 4206), ('behaving', 4207), ('nondelivery', 4208), ('introducing', 4209), ('tracker', 4210), ('bf', 4211), ('force', 4212), ('fishy', 4213), ('obtain', 4214), ('etc', 4215), ('informatiom', 4216), ('tendered', 4217), ('fr', 4218), ('hs', 4219), ('dollars', 4220), ('norefund', 4221), ('numb', 4222), ('daughter', 4223), ('pos', 4224), ('inventory', 4225), ('dropship', 4226), ('arghhh', 4227), ('congratulations', 4228), ('joining', 4229), ('fraudulently', 4230), ('compassion', 4231), ('attentiveness', 4232), ('greatly', 4233), ('rights', 4234), ('deceive', 4235), ('hungup', 4236), ('brag', 4237), ('paytm', 4238), ('thks', 4239), ('boat', 4240), ('screws', 4241), ('exchanges', 4242), ('openly', 4243), ('isolated', 4244), ('dysbit', 4245), ('seal', 4246), ('knife', 4247), ('toothpaste', 4248), ('thermometer', 4249), ('bluetooth', 4250), ('beanies', 4251), ('mens', 4252), ('antenna', 4253), ('laundry', 4254), ('employing', 4255), ('routes', 4256), ('ha', 4257), ('😎', 4258), ('worries', 4259), ('timeframe', 4260), ('customerstoday', 4261), ('dadagiri', 4262), ('okpls', 4263), ('donewent', 4264), ('idscustomer', 4265), ('profile', 4266), ('f', 4267), ('grocery', 4268), ('bluffing', 4269), ('popsicle', 4270), ('skulduggery', 4271), ('q', 4272), ('toldme', 4273), ('royally', 4274), ('stuffed', 4275), ('laziness', 4276), ('pleasure', 4277), ('target', 4278), ('achievement', 4279), ('died', 4280), ('centers', 4281), ('wide', 4282), ('tens', 4283), ('thousands', 4284), ('prompts', 4285), ('proceeding', 4286), ('thus', 4287), ('guysvery', 4288), ('harrasment', 4289), ('pun', 4290), ('jkasdfjkjkjkljklsdfnmdfmngdsnfgmdsgnsdjklgnkljdfgkldfjgsdfkl', 4291), ('jewellery', 4292), ('hallmark', 4293), ('defects', 4294), ('widin', 4295), ('dlvry', 4296), ('shoppers', 4297), ('submitting', 4298), ('attachment', 4299), ('est', 4300), ('tough', 4301), ('fri', 4302), ('heavy', 4303), ('timing', 4304), ('holidays', 4305), ('fly', 4306), ('mechanic', 4307), ('imitations', 4308), ('prealpha', 4309), ('tester', 4310), ('redeem', 4311), ('curiosity', 4312), ('guide', 4313), ('hdd', 4314), ('applying', 4315), ('adjustment', 4316), ('president', 4317), ('ripped', 4318), ('towels', 4319), ('straws', 4320), ('gravy', 4321), ('strainer', 4322), ('sig', 4323), ('damages', 4324), ('exausting', 4325), ('manufacture', 4326), ('nonreturnable', 4327), ('substandard', 4328), ('hefty', 4329), ('named', 4330), ('cardboards', 4331), ('othets', 4332), ('clicks', 4333), ('pdo', 4334), ('path', 4335), ('tia', 4336), ('wnt', 4337), ('factory', 4338), ('unplugged', 4339), ('aid', 4340), ('rave', 4341), ('minutesreceiving', 4342), ('tat', 4343), ('con', 4344), ('yestrday', 4345), ('deliverywill', 4346), ('compensate', 4347), ('costumer', 4348), ('wanda', 4349), ('wo', 4350), ('replacementrefund', 4351), ('docafe', 4352), ('cheers', 4353), ('pals', 4354), ('emotional', 4355), ('supportrequest', 4356), ('deptmnt', 4357), ('ths', 4358), ('whrevr', 4359), ('kg', 4360), ('gms', 4361), ('wash', 4362), ('officers', 4363), ('audacity', 4364), ('accused', 4365), ('circumventing', 4366), ('approved', 4367), ('classrooms', 4368), ('shld', 4369), ('rounds', 4370), ('ddr', 4371), ('ram', 4372), ('maryland', 4373), ('decor', 4374), ('contractors', 4375), ('rethinking', 4376), ('vanished', 4377), ('deliveryresched', 4378), ('torn', 4379), ('nither', 4380), ('abbyfern', 4381), ('jiaoyunshi', 4382), ('bs', 4383), ('cafely', 4384), ('pleasantly', 4385), ('swift', 4386), ('scott', 4387), ('reservoir', 4388), ('ahh', 4389), ('story', 4390), ('fullfilled', 4391), ('soundbot', 4392), ('proactive', 4393), ('floor', 4394), ('tb', 4395), ('firecuda', 4396), ('nas', 4397), ('santa', 4398), ('clause', 4399), ('highlighted', 4400), ('directing', 4401), ('pursuing', 4402), ('specialists', 4403), ('dozen', 4404), ('tovreply', 4405), ('fucks', 4406), ('gs', 4407), ('contains', 4408), ('scratch', 4409), ('facts', 4410), ('incidence', 4411), ('duped', 4412), ('appeal', 4413), ('evn', 4414), ('bothrd', 4415), ('wat', 4416), ('ws', 4417), ('rplying', 4418), ('alwys', 4419), ('humane', 4420), ('rcvd', 4421), ('irritating', 4422), ('defining', 4423), ('redeeming', 4424), ('lodging', 4425), ('inclusive', 4426), ('dey', 4427), ('producti', 4428), ('board', 4429), ('blank', 4430), ('recipients', 4431), ('disappears', 4432), ('todaysays', 4433), ('deliveryreturned', 4434), ('forcefully', 4435), ('hookup', 4436), ('awaited', 4437), ('booked', 4438), ('followups', 4439), ('tip', 4440), ('exploiting', 4441), ('pause', 4442), ('pad', 4443), ('tfw', 4444), ('mechanical', 4445), ('turk', 4446), ('configure', 4447), ('ww', 4448), ('😪', 4449), ('belong', 4450), ('filing', 4451), ('reordering', 4452), ('xiaomi', 4453), ('fans', 4454), ('reinvestigating', 4455), ('ice', 4456), ('lecture', 4457), ('rapidly', 4458), ('professionally', 4459), ('deceiving', 4460), ('fashion', 4461), ('problemplease', 4462), ('csrs', 4463), ('interacted', 4464), ('cudnt', 4465), ('trapped', 4466), ('customercare', 4467), ('uselessnot', 4468), ('scripts', 4469), ('resorted', 4470), ('stopreturn', 4471), ('reluctantly', 4472), ('crossbeats', 4473), ('raga', 4474), ('earphone', 4475), ('earpiece', 4476), ('jokers', 4477), ('joined', 4478), ('concierge', 4479), ('entry', 4480), ('youth', 4481), ('romantic', 4482), ('vol', 4483), ('novel', 4484), ('bhupendra', 4485), ('popup', 4486), ('humour', 4487), ('supposedly', 4488), ('plagiarized', 4489), ('vouchers', 4490), ('deffective', 4491), ('recover', 4492), ('gettin', 4493), ('exception', 4494), ('relying', 4495), ('consistency', 4496), ('totals', 4497), ('trail', 4498), ('mondays', 4499), ('lashonta', 4500), ('acces', 4501), ('bringing', 4502), ('flights', 4503), ('secondly', 4504), ('lodged', 4505), ('myhome', 4506), ('nite', 4507), ('appearing', 4508), ('horrifying', 4509), ('detaily', 4510), ('hemalatha', 4511), ('purely', 4512), ('datetwice', 4513), ('watching', 4514), ('subpar', 4515), ('😞', 4516), ('blatant', 4517), ('heavily', 4518), ('negligence', 4519), ('hebron', 4520), ('kygteast', 4521), ('pointgagtyoung', 4522), ('harris', 4523), ('gawrong', 4524), ('facilitygtmemphistngtatlantagand', 4525), ('delver', 4526), ('avialable', 4527), ('decency', 4528), ('amazin', 4529), ('nfl', 4530), ('upgrade', 4531), ('recognise', 4532), ('transactions', 4533), ('treatment', 4534), ('videos', 4535), ('col', 4536), ('hipe', 4537), ('pe', 4538), ('nai', 4539), ('de', 4540), ('sakte', 4541), ('mere', 4542), ('regional', 4543), ('loosing', 4544), ('teamsad', 4545), ('advertisement', 4546), ('scorpio', 4547), ('categories', 4548), ('brazil', 4549), ('teething', 4550), ('frustratingunhelpful', 4551), ('conv', 4552), ('concerning', 4553), ('unsure', 4554), ('wherever', 4555), ('withdrawn', 4556), ('soo', 4557), ('dumbampcheck', 4558), ('officials', 4559), ('suppose', 4560), ('intensionally', 4561), ('hosting', 4562), ('buyback', 4563), ('worste', 4564), ('porbandar', 4565), ('goons', 4566), ('setup', 4567), ('hk', 4568), ('belgium', 4569), ('overnight', 4570), ('doubting', 4571), ('logs', 4572), ('guilty', 4573), ('unimpressive', 4574), ('solutioni', 4575), ('doubled', 4576), ('srvice', 4577), ('attaching', 4578), ('snap', 4579), ('realis', 4580), ('tic', 4581), ('taxes', 4582), ('overpackage', 4583), ('unbreakable', 4584), ('muppets', 4585), ('moneyshould', 4586), ('daysstill', 4587), ('folder', 4588), ('hitting', 4589), ('🙃', 4590), ('comm', 4591), ('drains', 4592), ('stressed', 4593), ('compatible', 4594), ('jio', 4595), ('exclude', 4596), ('transportation', 4597), ('install', 4598), ('loader', 4599), ('overlapping', 4600), ('believed', 4601), ('scraped', 4602), ('correctlyno', 4603), ('dads', 4604), ('refundi', 4605), ('onwards', 4606), ('backwards', 4607), ('operate', 4608), ('mannered', 4609), ('recommendation', 4610), ('avenel', 4611), ('printable', 4612), ('version', 4613), ('xxx', 4614), ('atrocious', 4615), ('contracted', 4616), ('traditional', 4617), ('cashload', 4618), ('banco', 4619), ('llibres', 4620), ('digested', 4621), ('p', 4622), ('players', 4623), ('gd', 4624), ('assoc', 4625), ('wacct', 4626), ('transferring', 4627), ('stickers', 4628), ('deliveryperson', 4629), ('onus', 4630), ('ten', 4631), ('conniving', 4632), ('compassionate', 4633), ('gesture', 4634), ('slips', 4635), ('weybridge', 4636), ('genpact', 4637), ('hyundai', 4638), ('incomplete', 4639), ('setpower', 4640), ('base', 4641), ('fences', 4642), ('sidewalk', 4643), ('heaves', 4644), ('unlocked', 4645), ('gate', 4646), ('onto', 4647), ('porch', 4648), ('april', 4649), ('asleep', 4650), ('raises', 4651), ('differently', 4652), ('misbehaving', 4653), ('bravo', 4654), ('fight', 4655), ('echos', 4656), ('stereo', 4657), ('heads', 4658), ('stoped', 4659), ('serviceampinstallation', 4660), ('absorb', 4661), ('retry', 4662), ('glitches', 4663), ('dispersed', 4664), ('malta', 4665), ('shocker', 4666), ('whoes', 4667), ('pissed', 4668), ('birminghams', 4669), ('sudden', 4670), ('spotify', 4671), ('undetermined', 4672), ('reaches', 4673), ('guidebook', 4674), ('unlimited', 4675), ('goodreads', 4676), ('felt', 4677), ('tp', 4678), ('client', 4679), ('unknown', 4680), ('function', 4681), ('singin', 4682), ('cycle', 4683), ('clever', 4684), ('retype', 4685), ('experiencedyou', 4686), ('fa', 4687), ('charcoal', 4688), ('indigo', 4689), ('font', 4690), ('surein', 4691), ('deliveryi', 4692), ('files', 4693), ('replyin', 4694), ('snapdeal', 4695), ('js', 4696), ('aspects', 4697), ('shout', 4698), ('waking', 4699), ('piss', 4700), ('grer', 4701), ('vos', 4702), ('abonnements', 4703), ('<unknown>', 4704)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_dim = 200\n",
    "vocab_size = len(vocabulary) + 1  # +1 for unknown token \n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in vocabulary.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # Assign the chosen unknown vector representation\n",
    "        embedding_matrix[i] = embeddings_index.get(\"<unknown>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encocde sentences as sequences of integers: This is something I have just added\n",
    "def encode_sequences(tokenized_sentences, vocab):\n",
    "    sequences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sequence = [vocab.get(word, \"<unknown>\") for word in sentence]  # 0 for unknown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq = encode_sequences(X, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split dataset into stratified train and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq, y_encoded, test_size=0.2, shuffle=True, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert the target variable to a tensor\n",
    "X_train_tensors = [torch.tensor(sequence, dtype=torch.long) for sequence in X_train]\n",
    "X_val_tensors = [torch.tensor(sequence, dtype=torch.long) for sequence in X_val]\n",
    "\n",
    "# Pad the sequences\n",
    "X_train_tensor = pad_sequence(X_train_tensors, batch_first=True)\n",
    "X_val_tensor = pad_sequence(X_val_tensors, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10721, 31]), torch.Size([2681, 32]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape, X_val_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_pad_sequences(tensor, fixed_length):\n",
    "    # Calculate the difference in length\n",
    "    length_diff = fixed_length - tensor.shape[1]\n",
    "    \n",
    "    if length_diff > 0:\n",
    "        # If the tensor is shorter, pad it\n",
    "        padding = torch.zeros((tensor.shape[0], length_diff), dtype=tensor.dtype)\n",
    "        padded_tensor = torch.cat([tensor, padding], dim=1)\n",
    "    elif length_diff < 0:\n",
    "        # If the tensor is longer, truncate it\n",
    "        padded_tensor = tensor[:, :fixed_length]\n",
    "    else:\n",
    "        # If the tensor is already at the desired length, do nothing\n",
    "        padded_tensor = tensor\n",
    "    \n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = re_pad_sequences(X_train_tensor, 32)\n",
    "X_val_tensor = re_pad_sequences(X_val_tensor, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train)\n",
    "y_val_tensor = torch.tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import LongTensor\n",
    "# train_seq_lengths = LongTensor(list(map(len, X_train_tensors)))\n",
    "# val_seq_lengths = LongTensor(list(map(len, X_val_tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort sequences by lengths in descending order (if not already sorted)\n",
    "# X_train_sorted, train_lengths_sorted = X_train_tensor[train_seq_lengths.sort(descending=True)[1]], train_seq_lengths[train_seq_lengths.sort(descending=True)[1]]\n",
    "# X_val_sorted, val_lengths_sorted = X_val_tensor[val_seq_lengths.sort(descending=True)[1]], val_seq_lengths[val_seq_lengths.sort(descending=True)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_train and y_val are your target tensors\n",
    "\n",
    "# Get sorting indices for train and validation sequences\n",
    "# train_sort_indices = train_seq_lengths.sort(descending=True)[1]\n",
    "# val_sort_indices = val_seq_lengths.sort(descending=True)[1]\n",
    "\n",
    "# # Sort y_train and y_val using the obtained indices\n",
    "# y_train_sorted = y_train_tensor[train_sort_indices]\n",
    "# y_val_sorted = y_val_tensor[val_sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack the padded sequences \n",
    "# X_train_packed = pack_padded_sequence(X_train_sorted, train_lengths_sorted, batch_first=True, enforce_sorted=False)\n",
    "# X_val_packed = pack_padded_sequence(X_val_sorted, val_lengths_sorted, batch_first=True, enforce_sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     sequences, labels = zip(*batch)\n",
    "#     lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "#     padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "#     labels = torch.tensor(labels)\n",
    "#     return padded_sequences, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test datasets\n",
    "# train_dataset = Subset(TensorDataset(X_tensor, y_tensor), train_indices)\n",
    "# test_dataset = Subset(TensorDataset(X_tensor, y_tensor), val_indices)\n",
    "\n",
    "# DataLoader for train and test datasets\n",
    "train_dataloader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train' is a DataFrame containing 'Utterance' and 'Intent' columns\n",
    "\n",
    "# Tokenize the text data using PyTorch's tokenizer\n",
    "# The text already seems to be tokenized \n",
    "\n",
    "# Split the data into train and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, \n",
    "#                                                   shuffle=True, stratify=y, random_state=seed_value)\n",
    "\n",
    "# # Label encode the target variable\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "# y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "\n",
    "# # Convert encoded targets to PyTorch tensors\n",
    "# y_train_encoded = torch.tensor(y_train_encoded, dtype=torch.long) \n",
    "# y_val_encoded = torch.tensor(y_val_encoded, dtype=torch.long)\n",
    "\n",
    "# print(f'\\nShape checks:\\nX_train: {len(X_train)} X_val: {len(X_val)}\\ny_train: {len(y_train_encoded)} y_val: {len(y_val_encoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_X_train = encode_sequences(X_train, vocabulary)\n",
    "# encoded_X_val = encode_sequences(X_val, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length: This is something I have just added\n",
    "\n",
    "# Convert encoded sequences to PyTorch tensors\n",
    "# encoded_X_train_tensors = [torch.tensor(seq) for seq in encoded_X_train]\n",
    "# encoded_X_val_tensors = [torch.tensor(seq) for seq in encoded_X_val]\n",
    "\n",
    "# Pad sequences\n",
    "# Set batch_first=True to have the batch dimension first\n",
    "# padded_X_train = pad_sequence(encoded_X_train_tensors, batch_first=True, padding_value=0)\n",
    "# padded_X_val = pad_sequence(encoded_X_val_tensors, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Custom collate function to pad sequences \n",
    "# def collate_fn(batch):\n",
    "#     sequences, labels = zip(*batch)\n",
    "#     lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "#     padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "#     labels = torch.tensor(labels)\n",
    "#     return padded_sequences, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just an experimental check\n",
    "from torch.nn import Embedding  \n",
    "# embedding_layer = Embedding(num_embeddings=embedding_matrix_tensor.size(0), \n",
    "#                             embedding_dim=embedding_matrix_tensor.size(1), \n",
    "#                             _weight=embedding_matrix_tensor)\n",
    "\n",
    "# # Freeze the embedding layer\n",
    "# embedding_layer.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming padded_X_train and padded_X_val are NumPy arrays\n",
    "# padded_X_train_tensor = torch.LongTensor(padded_X_train)\n",
    "# padded_X_val_tensor = torch.LongTensor(padded_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_X_val_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repad the data\n",
    "# Assuming you've decided on a fixed sequence length, for example, the max length found in the training set\n",
    "# fixed_seq_length = 32\n",
    "\n",
    "# Function to re-pad tensors to a fixed length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len = padded_X_train_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-pad both the training and validation tensors\n",
    "# padded_X_train_tensor = re_pad_sequences(padded_X_train_tensor, fixed_seq_length)\n",
    "# padded_X_val_tensor = re_pad_sequences(padded_X_val_tensor, fixed_seq_length)\n",
    "\n",
    "# Now both tensors should have the same shape in terms of sequence length\n",
    "# print(padded_X_train_tensor.shape)\n",
    "# print(padded_X_val_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embedding layer\n",
    "# embedding_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "# embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "# embedding.weight = nn.Parameter(embedding_matrix_tensor)\n",
    "# embedding.weight.requires_grad = False  # To not train the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sweep config \n",
    "# sweep_config = {\n",
    "#     \"method\": \"random\",\n",
    "#     \"metric\": {\"goal\": \"maximize\", \"name\": \"val_accuracy\"}, \n",
    "#     \"parameters\": {\n",
    "#                     \"learning_rate\": {\"values\": [0.01, 0.001, 0.0001]},\n",
    "#                     \"epochs\": {\"values\": [30]},\n",
    "#                     \"batch_size\": {\"values\": [32, 64, 128]},\n",
    "#                     \"embedding_size\": {\"values\": [100]},\n",
    "#                     \"hidden_size\": {\"values\": [64, 128, 256]},\n",
    "#                     \"output_size\": {\"values\": [9]},\n",
    "#                     \"num_layers\": {\"values\": [1, 2, 3]},\n",
    "#                     \"dropout\": {\"values\": [0.1, 0.2, 0.3]}, \n",
    "#                     \"weight_decay\": {\"values\": [1e-3, 1e-4, 1e-5]},\n",
    "#                     \"scheduler_lambda_epoch_threshold\": {\"values\": [10]},\n",
    "#                     \"scheduler_decay_rate\": {\"values\": [-0.1]},\n",
    "#                 }\n",
    "# }\n",
    "\n",
    "# sweep_defaults = {\n",
    "#     \"learning_rate\": 0.001,\n",
    "#     \"epochs\": 30,\n",
    "#     \"batch_size\": 16, \n",
    "#     \"embedding_size\": 100,\n",
    "#     \"hidden_size\": 128,\n",
    "#     \"output_size\": 9,\n",
    "#     \"num_layers\": 3,\n",
    "#     \"dropout\": 0.05,\n",
    "#     \"eval_metric\": \"accuracy\", \n",
    "#     \"weight_decay\": 1e-3,\n",
    "#     \"scheduler_lambda_epoch_threshold\": 10,\n",
    "#     \"scheduler_decay_rate\": -0.1\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"intent-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MODEL_EVAL_METRIC:\n",
    "#     accuracy = \"accuracy\"\n",
    "#     f1_score = \"f1_score\"\n",
    "    \n",
    "# class Config: \n",
    "#     VOCAB_SIZE = 0\n",
    "#     BATCH_SIZE = 32 \n",
    "#     EMB_SIZE = 300 \n",
    "#     OUT_SIZE = 9 # Corresponds to the number of intents\n",
    "#     NUM_FOLDS = 5 \n",
    "#     NUM_EPOCHS = 5\n",
    "#     NUM_WORKERS = 8\n",
    "    \n",
    "#     # I want to update the pretrainhttps://wandb.ai/sinhasagar507/intent-classification/sweeps/4sd3drrded embedding weights during training process \n",
    "#     # I want to use a pretrained embedding\n",
    "#     OPTIMIZER = \"Adam\"\n",
    "#     EMB_WT_UPDATE = True\n",
    "#     DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "#     FAST_DEV_RUN = False \n",
    "#     PATIENCE = 6 \n",
    "#     IS_BIDIRECTIONAL = True \n",
    "    \n",
    "     \n",
    "#     # Model hyperparameters\n",
    "#     MODEL_PARAMS = {\n",
    "#         \"hidden_size\": 128,\n",
    "#         \"num_layers\": 2,\n",
    "#         \"drop_out\": 0.4258,\n",
    "#         \"lr\": 0.000366,\n",
    "#         \"weight_decay\": 0.00001\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just an experimental check\n",
    "# from torch.nn import Embedding  \n",
    "# embedding_layer = Embedding(num_embeddings=embedding_matrix_tensor.size(0), \n",
    "#                             embedding_dim=embedding_matrix_tensor.size(1), \n",
    "#                             _weight=embedding_matrix_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the architecture later \n",
    "class IntentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, embedding_matrix): \n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding_dim = wandb.config[\"embedding_size\"]\n",
    "        embedding_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "        self.embedding = nn.Embedding(seq_len, self.embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix_tensor)\n",
    "        self.embedding.weight.requires_grad = False  # To not train the embedding layer\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.hidden_dim = wandb.config[\"hidden_size\"]\n",
    "        self.num_layers = wandb.config[\"num_layers\"]\n",
    "        self.dropout = nn.Dropout(wandb.config[\"dropout\"])\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, \n",
    "                            hidden_size=self.hidden_dim, \n",
    "                            num_layers=self.num_layers, \n",
    "                            bidirectional=True, \n",
    "                            dropout=wandb.config[\"dropout\"], \n",
    "                            batch_first=True)\n",
    "        \n",
    "        # The output of this operation should be \n",
    "        \n",
    "        # Dense layers \n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden_dim*2, 1024)  # 2 for bidirectional. Over here, its (128*2) = 256, 1024 is the output dimension of the first dense layer\n",
    "        self.fc2 = nn.Linear(1024, 512) \n",
    "        self.fc3 = nn.Linear(512, 256) \n",
    "        \n",
    "        # Dropout layer\n",
    "       # self.dropout = nn.Dropout(self.dropout)  \n",
    "        \n",
    "        # Output layer\n",
    "        self.output_dim = wandb.config[\"output_size\"]\n",
    "        self.out = nn.Linear(256, self.output_dim) ## Yaar idhr output hoga RNN ya LSTM ka (batch_size output_dim, no_of_classes) aayega kya? \n",
    "        # self.out_2 = nn.Linear(output_dim, 9)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # text = [batch_size, embed_length]\n",
    "        \n",
    "        # embeddings = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        # embedded = [batch_size, sent_length, emb_dim]\n",
    "\n",
    "        # if self.embedding_matrix is not None: \n",
    "        #     assert self.embeddings.shape == (inputs.shape[0], inputs.shape[1], self.embedding_dim)\n",
    "         \n",
    "        # pack_padded_sequence before feeding to the LSTM. This is required so PyTorch knows \n",
    "        # which elements of the sequence are padded and ignores them in the computation \n",
    "        # Accomplished only after the embedding step \n",
    "        # embeds_pack = pack_padded_sequence(embeddings, inputs_lengths, batch_first=True)\n",
    "        \n",
    "        # Get the dimensions of the packed sequence \n",
    "        # dimensions = embeds_pack.data.size()\n",
    "\n",
    "        # Assert the shape of input sequence \n",
    "        # assert inputs.shape == (Config.BATCH_SIZE, 1000)\n",
    "\n",
    "        embeddings = self.embedding(inputs)\n",
    "        # print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        _, (hidden, _) = self.lstm(embeddings)\n",
    "\n",
    "        # hidden shape: [num_layers*num_directions, batch_size, hidden_dim]\n",
    "        # print(f\"Hidden shape: {hidden.shape}\n",
    "        \n",
    "        # Ours task being a classification model, we are only interested in the final hidden state and not the LSTM output \n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        final_hidden_forward = hidden[-2, :, :] # [batch_size, hidden_dim]\n",
    "        final_hidden_backward = hidden[-1, :, :] # [bacth_size, hidden_dim]\n",
    "\n",
    "        # print(f\"Final hidden forward shape: {final_hidden_forward.shape}\") # Iska shape is \n",
    "        # print(f\"Final hidden backward shape: {final_hidden_backward.shape}\")\n",
    "        \n",
    "        # Concat the final forward and hidden backward states \n",
    "        hidden = torch.cat((final_hidden_forward, final_hidden_backward), dim=1)\n",
    "        # print(f\"Hidden shape after concatenation: {hidden.shape}\")\n",
    "                \n",
    "        # Dense Linear Layers \n",
    "        dense_outputs_1 = self.fc1(hidden)\n",
    "        dense_outputs_1 = nn.ReLU()(dense_outputs_1) \n",
    "        # Dropout layer \n",
    "        dense_outputs_1 = self.dropout(dense_outputs_1) \n",
    "        dense_outputs_2 = self.fc2(dense_outputs_1)\n",
    "     #   dense_outputs_2 = self.dropout(dense_outputs_2)\n",
    "        dense_outputs_2 = nn.ReLU()(dense_outputs_2) \n",
    "        dense_outputs_3 = self.fc3(dense_outputs_2)\n",
    "        dense_outputs_3 = nn.ReLU()(dense_outputs_3)\n",
    "     #  dense_outputs_3 = self.dropout(dense_outputs_3)\n",
    "\n",
    "        # Final output classification layer\n",
    "        # Applying the Softmax layer \n",
    "        final_output = (self.out(dense_outputs_3))\n",
    "        # print(f\"Final output shape: {final_output.shape}\")\n",
    "    \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "confusion_matrix_epoch, class_report_epoch = [], []\n",
    "class ModelTrainer:\n",
    "    def __init__(self, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_dim = wandb.config[\"embedding_size\"]\n",
    "        self.embedding_matrix = embedding_matrix   \n",
    "        self.hidden_dim = wandb.config[\"hidden_size\"]\n",
    "        self.output_dim = wandb.config[\"output_size\"]\n",
    "        self.n_layers = wandb.config[\"num_layers\"]\n",
    "        self.batch_size = wandb.config[\"batch_size\"]\n",
    "        self.epochs = wandb.config[\"epochs\"]\n",
    "     #   self.dropout = wandb.config[\"dropout\"]\n",
    "        # Assuming IntentClassifier is defined elsewhere and matches these parameters\n",
    "        # print(self.seq_len, self.embedding_dim, self.hidden_dim, self.output_dim, self.embedding_matrix)\n",
    "        self.model = IntentClassifier(self.seq_len, self.embedding_matrix)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # Assuming Config.OPTIMIZER is a valid PyTorch optimizer class\n",
    "        self.learning_rate = wandb.config[\"learning_rate\"]\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=wandb.config[\"weight_decay\"])\n",
    "        self.scheduler_epoch_threshold = wandb.config[\"scheduler_lambda_epoch_threshold\"]\n",
    "        self.scheduler_decay_rate = wandb.config[\"scheduler_decay_rate\"]\n",
    "        self.epoch_lst = []\n",
    "\n",
    "    def train(self, train_dataloader, val_dataloader):\n",
    "        #TODO: Change the function format afterwards \n",
    "        # X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "        # X_val = torch.tensor(X_val, dtype=torch.float)\n",
    "        # y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "        # y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "        # Assuming X_train, y_train, X_val, y_val are already tensors\n",
    "        # Ensure they have matching first dimensions\n",
    "        # assert X_train.shape[0] == y_train.shape[0], \"Training feature and label count mismatch\"\n",
    "        # assert X_val.shape[0] == y_val.shape[0], \"Validation feature and label count mismatch\"\n",
    "        \n",
    "       \n",
    "        train_accuracies_epoch, val_accuracies_epoch = [], []\n",
    "        self.valid_loss_min = np.Inf\n",
    "\n",
    "        # Assuming `optimizer` is already defined\n",
    "        # Define the lambda function for learning rate adjustment using W&B config\n",
    "        # lambda_lr = lambda epoch: 1 if epoch < self.scheduler_epoch_threshold else torch.exp(torch.tensor(-self.scheduler_decay_rate))\n",
    "\n",
    "        # Initialize the LambdaLR scheduler with the optimizer and lambda function\n",
    "        # scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, valid_loss = 0.0, 0.0\n",
    "            correct, total = 0, 0\n",
    "\n",
    "\n",
    "            self.model.train()\n",
    "            for data, target in train_dataloader:\n",
    "                # Log the shape of the data and target tensors\n",
    "                # assert data.shape == (self.batch_size, self.embedding_dim), f\"Data shape mismatch: {data.shape}\"\n",
    "                # assert target.shape == (self.batch_size,), f\"Target shape mismatch: {target.shape}\"\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # print(output.shape)\n",
    "                pred_labels = torch.argmax(output, 1)\n",
    "                correct += (pred_labels == target).sum().item()\n",
    "                total += target.size(0)\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "\n",
    "\n",
    "            train_accuracy = 100 * correct / total\n",
    "            train_accuracies_epoch.append(train_accuracy)\n",
    "\n",
    "            # Log the training loss and accuracy\n",
    "            # wandb.log({\"Training Accuracy\": train_accuracy, \"Training Loss\": train_loss})\n",
    "\n",
    "            self.model.eval()\n",
    "            correct, total = 0, 0\n",
    "            all_targets, all_preds = [], []\n",
    "            for data, target in val_dataloader:\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                pred_labels = torch.argmax(output, 1)\n",
    "                correct += (pred_labels == target).sum().item()\n",
    "                total += target.size(0)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                all_targets.extend(list(target.numpy()))\n",
    "                all_preds.extend(list(pred_labels.numpy()))\n",
    "\n",
    "            valid_accuracy = 100 * correct / total\n",
    "            val_accuracies_epoch.append(valid_accuracy)\n",
    "\n",
    "            f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "            conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "            class_report = classification_report(all_targets, all_preds, labels=[str(i) for i in range(self.output_dim)])\n",
    "            confusion_matrix_epoch.append(conf_matrix)\n",
    "            class_report_epoch.append(class_report)\n",
    "\n",
    "            # Log the validation loss and accuracy\n",
    "            # print(f\"Epoch: {epoch+1}/{self.epochs}.. Training Accuracy: {train_accuracy:.3f}.. Validation Accuracy: {valid_accuracy:.3f}\")\n",
    "\n",
    "            # Log epoch-wise accuracies\n",
    "            wandb.log({\"epoch\": epoch, \"Training Accuracy\": train_accuracy, \n",
    "                       \"Validation Accuracy\": valid_accuracy, \"Training Loss\": train_loss, \n",
    "                       \"Validation Loss\": valid_loss,  \"Validation F1 Score\": f1,\n",
    "                       })\n",
    "\n",
    "            if valid_loss <= self.valid_loss_min:\n",
    "                print(f\"Validation loss decreased ({self.valid_loss_min:.3f} --> {valid_loss:.3f}). Saving model...\")\n",
    "                \n",
    "                # Log the model and its parameters \n",
    "                # wandb.log_artifact(self.model)\n",
    "                state = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"state_dict\": self.model.state_dict(),\n",
    "                    \"optimizer\": self.optimizer.state_dict(),\n",
    "                    \"loss\": valid_loss, \n",
    "                   \n",
    "                }\n",
    "                torch.save(state, \"../models/intent_classification_model.pt\")\n",
    "                self.valid_loss_min = valid_loss\n",
    "\n",
    "            self.epoch_lst.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things I Need to Add\n",
    "- WandB table\n",
    "- Log artifact (model)\n",
    "- For now, include all the basic elements (then we can improve upon this in the future)\n",
    "- Ability to track across multiple hyperparameters\n",
    "- Set the configuration after the run is complete\n",
    "- Sweeps (...) AND Improvisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# trainer = ModelTrainer(padded_X_train.shape[1])\n",
    "# train_features, val_features = padded_X_train, padded_X_val\n",
    "# trainer.train(train_features, y_train_encoded, val_features, y_val_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset needs extensive cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 1775.704). Saving model...\n",
      "Validation loss decreased (1775.704 --> 1410.457). Saving model...\n",
      "Validation loss decreased (1410.457 --> 1296.015). Saving model...\n",
      "Validation loss decreased (1296.015 --> 1148.233). Saving model...\n",
      "Validation loss decreased (1148.233 --> 1107.683). Saving model...\n",
      "Validation loss decreased (1107.683 --> 1070.194). Saving model...\n",
      "Validation loss decreased (1070.194 --> 1057.250). Saving model...\n",
      "Validation loss decreased (1057.250 --> 1038.462). Saving model...\n",
      "Validation loss decreased (1038.462 --> 1004.998). Saving model...\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = ModelTrainer(seq_len)\n",
    "trainer.train(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.69      0.74      0.71       314\\n           1       0.99      0.97      0.98       279\\n           2       0.89      0.88      0.88       293\\n           3       0.99      0.99      0.99       320\\n           4       0.97      1.00      0.98       279\\n           5       0.76      0.70      0.73       301\\n           6       0.99      1.00      1.00       312\\n           7       0.56      0.63      0.60       284\\n           8       0.91      0.81      0.85       299\\n\\n   micro avg       0.86      0.86      0.86      2681\\n   macro avg       0.86      0.86      0.86      2681\\nweighted avg       0.86      0.86      0.86      2681\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_report_epoch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 5, ..., 3, 6, 4])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['challenge_robot', 'discount', 'quality', ..., 'goodbye',\n",
       "       'speak_representative', 'greeting'], dtype='<U20')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded_inverse = label_encoder.inverse_transform(y_encoded)\n",
    "y_encoded_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 5, 6, 2, 5, 3, 8, 0, 1, 8, 6, 2, 0, 4, 6, 7, 1, 5, 0])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['challenge_robot', 'discount', 'quality', 'speak_representative',\n",
       "       'discount', 'quality', 'goodbye', 'track', 'account',\n",
       "       'challenge_robot', 'track', 'speak_representative', 'discount',\n",
       "       'account', 'greeting', 'speak_representative', 'support',\n",
       "       'challenge_robot', 'quality', 'account'], dtype='<U20')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded_inverse[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a label-code dictionary\n",
    "label_code_dict = dict(zip(y_encoded, y_encoded_inverse))\n",
    "## Create inverse as well \n",
    "label_code_dict_inverse = dict(zip(y_encoded_inverse, y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[231,   1,  11,   0,   0,   4,   1,  50,  16],\n",
       "       [  0, 272,   0,   0,   7,   0,   0,   0,   0],\n",
       "       [  8,   0, 257,   0,   0,  11,   0,  16,   1],\n",
       "       [  0,   0,   0, 318,   0,   2,   0,   0,   0],\n",
       "       [  0,   0,   0,   0, 279,   0,   0,   0,   0],\n",
       "       [ 14,   0,  12,   1,   0, 210,   0,  59,   5],\n",
       "       [  0,   0,   0,   0,   0,   0, 312,   0,   0],\n",
       "       [ 46,   0,   8,   2,   2,  44,   1, 179,   2],\n",
       "       [ 35,   1,   2,   0,   0,   7,   0,  13, 241]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_epoch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the Training Function\n",
    "# def train():\n",
    "#     # Initialize a new wandb run\n",
    "#     wandb.init(config=sweep_defaults)\n",
    "    \n",
    "#     # Modify the trainer initialization and training process to use config parameters\n",
    "#     trainer = ModelTrainer(padded_X_train.shape[1])\n",
    "#     train_features, val_features = padded_X_train, padded_X_val\n",
    "    \n",
    "#     # Assuming the trainer.train method is modified to accept epochs and batch_size\n",
    "#     trainer.train(train_features, y_train_encoded, val_features, y_val_encoded)\n",
    "        \n",
    "# # Step 4: Start the Sweep Agent\n",
    "# wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_checkpoint(model, optimizer, filename='../models/intent_classification_model.pt'):\n",
    "#     # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "#     start_epoch = 15\n",
    "#     if os.path.isfile(filename):\n",
    "#         print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "#         checkpoint = torch.load(filename)\n",
    "#         start_epoch = checkpoint['epoch']\n",
    "#         model.load_state_dict(checkpoint['state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#         losslogger = checkpoint['loss']\n",
    "#         print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "#                   .format(filename, checkpoint['epoch']))\n",
    "#     else:\n",
    "#         print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "#     return model, optimizer, start_epoch, losslogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = IntentClassifier(seq_len, embedding_matrix)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# model, optimizer, start_epoch, losslogger = load_checkpoint(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training the model\n",
    "# model_trainer = ModelTrainer(seq_len)\n",
    "# model_trainer.train(padded_X_train_tensor, y_train_encoded, padded_X_val_tensor, y_val_encoded, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data and related information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TyPe checking\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from ftlangdetect import detect\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Contractions \n",
    "import contractions as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Punctuations I want to remove, including the empty token\n",
    "puncts = ['\\u200d', '?', '....','..','...','','@','#', ',', '.', '\"', ':', ')', '(', '-', '!', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '*', '+', '\\\\', \n",
    "    '•', '~', '£', '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', \n",
    "    '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', \n",
    "    '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', \n",
    "    'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', \n",
    "    '¹', '≤', '‡', '√', '!','🅰','🅱']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What all functions I have applied earlier??? \n",
    "## I have applied \"detect_language\" function to the text data\n",
    "## Apply the \"clean_text\" function to the text data\n",
    "## Apply the \"lemmatize_and_pos\" function to the text data...not required exactly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My preprocessing functions (defining them here so that I could access them from anywhere in the notebook)\n",
    "# Capture the hashtags and/or usertags \n",
    "# Clean comment text \n",
    "\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "# Function to detect language using langdetect\n",
    "## Check if I have to perform sentence level tokenization first \n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect the language of the given text using langdetect library.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text for language detection.\n",
    "\n",
    "    Returns:\n",
    "    - str: The detected language code (e.g., 'en' for English).\n",
    "           If language detection fails, returns 'unknown'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to detect the language using langdetect\n",
    "        return detect(text)[\"lang\"]\n",
    "    \n",
    "    except Exception:\n",
    "        # Return 'unknown' if language detection fails\n",
    "        return \"unknown\"\n",
    "\n",
    "def clean_text(\n",
    "        text, words=True, stops=True, urls=True, tags=True, hashtags = True, punctuations=True,  \n",
    "        newLine=True, ellipsis=True, special_chars=True, condensed=True, non_breaking_space=True, \n",
    "        character_encodings=True, stopwords=True, only_words=True) -> str:\n",
    "    \n",
    "    \"\"\" Clean tweets after extracting all hashtags and username tags\n",
    "    Not comprehensive enough to capture all idiosyncrasies, but works for most of the time\n",
    "    \"\"\"\n",
    "    \n",
    "    # Capture only words and no numbers\n",
    "    if words:\n",
    "        pattern = r\"\\d\"\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "        \n",
    "    # Remove URLs \n",
    "    if urls:\n",
    "        pattern = \"(https\\:)*\\/*\\/*(www\\.)?(\\w+)(\\.\\w+)\\/*\\w*\"\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "        \n",
    "    # Remove tags \n",
    "    if tags:\n",
    "        text = re.sub(\"@\\S+\", \"\", text)\n",
    "        \n",
    "    # Remove hashtags \n",
    "    if hashtags: \n",
    "        text = re.sub(\"#\\w+\", \"\", text)\n",
    "        \n",
    "    # Remove punctuations\n",
    "    if punctuations:\n",
    "        for punct in puncts: \n",
    "            text = text.replace(punct, \"\")\n",
    "        \n",
    "    # Replacing one or more occurrences of '\\n' with ''\n",
    "    # Replacing multiple occurrences, i.e., >=2 occurrences with '.'\n",
    "    if newLine:\n",
    "        text = re.sub(\"\\n+\", \"\", text)\n",
    "        text = re.sub(r'\\.\\s+', '.', text)\n",
    "        \n",
    "    # Fix contractions\n",
    "    if condensed:\n",
    "        try:\n",
    "            text = cm.fix(text)\n",
    "        except: \n",
    "            print(text)\n",
    "        \n",
    "    # Remove non-breaking space \n",
    "    if non_breaking_space: \n",
    "        pattern = r\"(\\xa0|&nbsp)\"\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "        \n",
    "    # Remove stopwords\n",
    "    if stopwords:\n",
    "        text = text.lower()\n",
    "        # print(f\"Original Shape of the Data is {.shape}\")\n",
    "        \n",
    "        # Splitting with NLTK's Tweet tokenizer. This limits repeated characters to \n",
    "        # three with the reduce lens parameter and strips all the \"@'s\". It also splits \n",
    "        # it into 1-gram tokens         \n",
    "        words = tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word not in eng_stopwords]\n",
    "        text = \" \".join(words)\n",
    "        text = text.strip()  # Add further checks for cleaning \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inbound_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>outbound_text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>inbound_lang</th>\n",
       "      <th>inbound_hashtags</th>\n",
       "      <th>outbound_hashtags</th>\n",
       "      <th>clean_inbound_text</th>\n",
       "      <th>clean_outbound_text</th>\n",
       "      <th>outbound_tokens_pos</th>\n",
       "      <th>inbound_tokens_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@AmazonHelp 3 different people have given 3 di...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-10-31 23:28:00+00:00</td>\n",
       "      <td>@115820 We'd like to take a further look into ...</td>\n",
       "      <td>619</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>different people have given different answers ...</td>\n",
       "      <td>wed like to take a further look into this with...</td>\n",
       "      <td>['-PRON-: NOUN', 'd: VERB', 'like: VERB', 'to:...</td>\n",
       "      <td>['different: NOUN', 'people: NOUN', 'have: NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Way to drop the ball on customer service @1158...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-10-31 22:29:00+00:00</td>\n",
       "      <td>@115820 I'm sorry we've let you down! Without ...</td>\n",
       "      <td>616</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>way to drop the ball on customer service so pi...</td>\n",
       "      <td>i am sorry we have let you down without provid...</td>\n",
       "      <td>['i: NOUN', 'be: NOUN', 'sorry: NOUN', '-PRON-...</td>\n",
       "      <td>['way: NOUN', 'to: NOUN', 'drop: VERB', 'the: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@115823 I want my amazon payments account CLOS...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-10-31 22:28:34+00:00</td>\n",
       "      <td>@115822 I am unable to affect your account via...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>i want my amazon payments account closed dm me...</td>\n",
       "      <td>i am unable to affect your account via twitter...</td>\n",
       "      <td>['i: NOUN', 'be: NOUN', 'unable: NOUN', 'to: N...</td>\n",
       "      <td>['i: NOUN', 'want: VERB', '-PRON-: NOUN', 'ama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@AmazonHelp @115826 Yeah this is crazy we’re l...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-11-01 12:53:34+00:00</td>\n",
       "      <td>@115827 Thanks for your patience. ^KM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>yeah this is crazy were less than a week away ...</td>\n",
       "      <td>thanks for your patience km</td>\n",
       "      <td>['thank: NOUN', 'for: NOUN', '-PRON-: NOUN', '...</td>\n",
       "      <td>['yeah: NOUN', 'this: NOUN', 'be: NOUN', 'craz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@115828 How about you guys figure out my Xbox ...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-10-31 22:28:00+00:00</td>\n",
       "      <td>@115826 I'm sorry for the wait. You'll receive...</td>\n",
       "      <td>627</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>how about you guys figure out my xbox one x pr...</td>\n",
       "      <td>i am sorry for the wait you will receive an em...</td>\n",
       "      <td>['i: NOUN', 'be: NOUN', 'sorry: NOUN', 'for: N...</td>\n",
       "      <td>['how: NOUN', 'about: NOUN', '-PRON-: NOUN', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        inbound_text   author_id  \\\n",
       "0  @AmazonHelp 3 different people have given 3 di...  AmazonHelp   \n",
       "1  Way to drop the ball on customer service @1158...  AmazonHelp   \n",
       "2  @115823 I want my amazon payments account CLOS...  AmazonHelp   \n",
       "3  @AmazonHelp @115826 Yeah this is crazy we’re l...  AmazonHelp   \n",
       "4  @115828 How about you guys figure out my Xbox ...  AmazonHelp   \n",
       "\n",
       "                  created_at  \\\n",
       "0  2017-10-31 23:28:00+00:00   \n",
       "1  2017-10-31 22:29:00+00:00   \n",
       "2  2017-10-31 22:28:34+00:00   \n",
       "3  2017-11-01 12:53:34+00:00   \n",
       "4  2017-10-31 22:28:00+00:00   \n",
       "\n",
       "                                       outbound_text response_tweet_id  \\\n",
       "0  @115820 We'd like to take a further look into ...               619   \n",
       "1  @115820 I'm sorry we've let you down! Without ...               616   \n",
       "2  @115822 I am unable to affect your account via...               NaN   \n",
       "3              @115827 Thanks for your patience. ^KM               NaN   \n",
       "4  @115826 I'm sorry for the wait. You'll receive...               627   \n",
       "\n",
       "  inbound_lang inbound_hashtags outbound_hashtags  \\\n",
       "0           en               []                []   \n",
       "1           en               []                []   \n",
       "2           en               []                []   \n",
       "3           en               []                []   \n",
       "4           en               []                []   \n",
       "\n",
       "                                  clean_inbound_text  \\\n",
       "0  different people have given different answers ...   \n",
       "1  way to drop the ball on customer service so pi...   \n",
       "2  i want my amazon payments account closed dm me...   \n",
       "3  yeah this is crazy were less than a week away ...   \n",
       "4  how about you guys figure out my xbox one x pr...   \n",
       "\n",
       "                                 clean_outbound_text  \\\n",
       "0  wed like to take a further look into this with...   \n",
       "1  i am sorry we have let you down without provid...   \n",
       "2  i am unable to affect your account via twitter...   \n",
       "3                        thanks for your patience km   \n",
       "4  i am sorry for the wait you will receive an em...   \n",
       "\n",
       "                                 outbound_tokens_pos  \\\n",
       "0  ['-PRON-: NOUN', 'd: VERB', 'like: VERB', 'to:...   \n",
       "1  ['i: NOUN', 'be: NOUN', 'sorry: NOUN', '-PRON-...   \n",
       "2  ['i: NOUN', 'be: NOUN', 'unable: NOUN', 'to: N...   \n",
       "3  ['thank: NOUN', 'for: NOUN', '-PRON-: NOUN', '...   \n",
       "4  ['i: NOUN', 'be: NOUN', 'sorry: NOUN', 'for: N...   \n",
       "\n",
       "                                  inbound_tokens_pos  \n",
       "0  ['different: NOUN', 'people: NOUN', 'have: NOU...  \n",
       "1  ['way: NOUN', 'to: NOUN', 'drop: VERB', 'the: ...  \n",
       "2  ['i: NOUN', 'want: VERB', '-PRON-: NOUN', 'ama...  \n",
       "3  ['yeah: NOUN', 'this: NOUN', 'be: NOUN', 'craz...  \n",
       "4  ['how: NOUN', 'about: NOUN', '-PRON-: NOUN', '...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import processed data \n",
    "processed_data = pd.read_csv(\"../data/processed/processed_v2.csv\")\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@115828 How about you guys figure out my Xbox One X project Scorpio edition first. No expected delivery or shipping date and it’s only a week away'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[\"inbound_text\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of common stopwords\n",
    "manual_stopwords = {\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', \n",
    "    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', \n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', \n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def basic_preprocess_tokens(tokens):\n",
    "    \n",
    "    # Convert string representation of list to actual list\n",
    "    # tokens = ast.literal_eval(tokens)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in manual_stopwords]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sorry',\n",
       " 'wait',\n",
       " 'receive',\n",
       " 'email',\n",
       " 'soon',\n",
       " 'estimated',\n",
       " 'delivery',\n",
       " 'date',\n",
       " 'fj']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text_eg = clean_text(processed_data[\"outbound_text\"][4])\n",
    "cleaned_tokens_eg = basic_preprocess_tokens(cleaned_text_eg.split())\n",
    "cleaned_tokens_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_eg = shuffled_df[shuffled_df[\"intent\"] == \"support\"]\n",
    "account_eg = shuffled_df[shuffled_df[\"intent\"] == \"account\"]\n",
    "greet_eg = shuffled_df[shuffled_df[\"intent\"] == \"greeting\"]\n",
    "goodbye_eg = shuffled_df[shuffled_df[\"intent\"] == \"goodbye\"]\n",
    "representative_eg = shuffled_df[shuffled_df[\"intent\"] == \"speak_representative\"]\n",
    "robot_eg = shuffled_df[shuffled_df[\"intent\"] == \"challenge_robot\"]\n",
    "quality_eg = shuffled_df[shuffled_df[\"intent\"] == \"quality\"]\n",
    "track_eg = shuffled_df[shuffled_df[\"intent\"] == \"track\"]\n",
    "discount_eg = shuffled_df[shuffled_df[\"intent\"] == \"discount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[231,   1,  11,   0,   0,   4,   1,  50,  16],\n",
       "       [  0, 272,   0,   0,   7,   0,   0,   0,   0],\n",
       "       [  8,   0, 257,   0,   0,  11,   0,  16,   1],\n",
       "       [  0,   0,   0, 318,   0,   2,   0,   0,   0],\n",
       "       [  0,   0,   0,   0, 279,   0,   0,   0,   0],\n",
       "       [ 14,   0,  12,   1,   0, 210,   0,  59,   5],\n",
       "       [  0,   0,   0,   0,   0,   0, 312,   0,   0],\n",
       "       [ 46,   0,   8,   2,   2,  44,   1, 179,   2],\n",
       "       [ 35,   1,   2,   0,   0,   7,   0,  13, 241]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_epoch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'challenge_robot',\n",
       " 2: 'discount',\n",
       " 5: 'quality',\n",
       " 6: 'speak_representative',\n",
       " 3: 'goodbye',\n",
       " 8: 'track',\n",
       " 0: 'account',\n",
       " 4: 'greeting',\n",
       " 7: 'support'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_code_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>account</td>\n",
       "      <td>[have, you, the, contact, number, thanks, sorr...</td>\n",
       "      <td>[contact, number, thanks, sorry, hassle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>account</td>\n",
       "      <td>[does, the, same, information, apply, to, uk, ...</td>\n",
       "      <td>[information, apply, uk, residents]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>account</td>\n",
       "      <td>[i, know, its, a, minor, thing, but, why, can,...</td>\n",
       "      <td>[know, minor, thing, opt, general, account, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>account</td>\n",
       "      <td>[hey, please, pay, my, seller, funds, which, h...</td>\n",
       "      <td>[hey, please, pay, seller, funds, held, days, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>account</td>\n",
       "      <td>[hey, and, what, is, the, deal, with, you, try...</td>\n",
       "      <td>[hey, deal, trying, deliver, something, perhap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13480</th>\n",
       "      <td>account</td>\n",
       "      <td>[i, have, sent, emails, and, am, locked, out, ...</td>\n",
       "      <td>[sent, emails, locked, account, response, fix]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13483</th>\n",
       "      <td>account</td>\n",
       "      <td>[what, is, up, with, your, deliveries, amazon,...</td>\n",
       "      <td>[deliveries, amazon, one, days, purchase, upda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13484</th>\n",
       "      <td>account</td>\n",
       "      <td>[if, you, need, anymore, info, please, let, me...</td>\n",
       "      <td>[need, anymore, info, please, let, know]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13488</th>\n",
       "      <td>account</td>\n",
       "      <td>[hey, thanks, for, the, link, couple, of, agen...</td>\n",
       "      <td>[hey, thanks, link, couple, agents, able, fix,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13494</th>\n",
       "      <td>account</td>\n",
       "      <td>[shout, out, to, for, not, verifying, customer...</td>\n",
       "      <td>[shout, verifying, customers, credit, card, in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1495 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        intent                                             tokens  \\\n",
       "8      account  [have, you, the, contact, number, thanks, sorr...   \n",
       "13     account  [does, the, same, information, apply, to, uk, ...   \n",
       "19     account  [i, know, its, a, minor, thing, but, why, can,...   \n",
       "34     account  [hey, please, pay, my, seller, funds, which, h...   \n",
       "57     account  [hey, and, what, is, the, deal, with, you, try...   \n",
       "...        ...                                                ...   \n",
       "13480  account  [i, have, sent, emails, and, am, locked, out, ...   \n",
       "13483  account  [what, is, up, with, your, deliveries, amazon,...   \n",
       "13484  account  [if, you, need, anymore, info, please, let, me...   \n",
       "13488  account  [hey, thanks, for, the, link, couple, of, agen...   \n",
       "13494  account  [shout, out, to, for, not, verifying, customer...   \n",
       "\n",
       "                                          cleaned_tokens  \n",
       "8               [contact, number, thanks, sorry, hassle]  \n",
       "13                   [information, apply, uk, residents]  \n",
       "19     [know, minor, thing, opt, general, account, su...  \n",
       "34     [hey, please, pay, seller, funds, held, days, ...  \n",
       "57     [hey, deal, trying, deliver, something, perhap...  \n",
       "...                                                  ...  \n",
       "13480     [sent, emails, locked, account, response, fix]  \n",
       "13483  [deliveries, amazon, one, days, purchase, upda...  \n",
       "13484           [need, anymore, info, please, let, know]  \n",
       "13488  [hey, thanks, link, couple, agents, able, fix,...  \n",
       "13494  [shout, verifying, customers, credit, card, in...  \n",
       "\n",
       "[1495 rows x 3 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out different examples\n",
    "account_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deliveries',\n",
       " 'amazon',\n",
       " 'one',\n",
       " 'days',\n",
       " 'purchase',\n",
       " 'update',\n",
       " 'another',\n",
       " 'delayed',\n",
       " 'info']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_eg[\"cleaned_tokens\"][13483]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_robot = robot_eg[\"cleaned_tokens\"][0]\n",
    "eg_account = account_eg[\"cleaned_tokens\"][13483]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = IntentClassifier(seq_len, embedding_matrix)\n",
    "checkpoint = torch.load(\"../models/intent_classification_model.pt\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6)\n",
    "\n",
    "# Load the state dictionary\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "epoch = checkpoint[\"epoch\"]\n",
    "valid_loss = checkpoint[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4704"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[\"<unknown>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "733\n",
      "1092\n",
      "133\n",
      "167\n",
      "2461\n",
      "67\n",
      "250\n",
      "4704\n"
     ]
    }
   ],
   "source": [
    "for token in cleaned_tokens_eg:\n",
    "    token_repr = vocabulary.get(token, vocabulary[\"<unknown>\"])\n",
    "    print(token_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "def inference(tokens):\n",
    "    \"\"\" \n",
    "    Perform preprocessing and inference on the input text using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model for intent classification.\n",
    "    - text: The input text string.\n",
    "    - vocabulary: A dictionary mapping tokens to indices.\n",
    "    - seq_len: The fixed sequence length expected by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - pred_label: The predicted label index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess the text\n",
    "    # tokens = text.split()\n",
    "    indices = [vocabulary.get(token, vocabulary[\"<unknown>\"]) for token in tokens]  # Use 0 for unknown words\n",
    "    padded_indices = indices[:seq_len] + [0] * max(0, seq_len - len(indices))  # Pad with zeros\n",
    "    print(padded_indices)\n",
    "    input_tensor = torch.tensor(padded_indices).unsqueeze(0)  # Add batch dimension\n",
    "    print(input_tensor)\n",
    "    # print(input_tensor.shape)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        label = torch.argmax(output, 1)\n",
    "        label_text = label_encoder.inverse_transform(label)\n",
    "    \n",
    "    return label_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 733, 1092, 133, 167, 2461, 67, 250, 4704, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tensor([[  35,  733, 1092,  133,  167, 2461,   67,  250, 4704,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['account'], dtype='<U20')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(cleaned_tokens_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['greeting'], dtype='<U20')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_label = label_encoder.inverse_transform([4])\n",
    "original_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon_support",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
