{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Classification With PyTorch\n",
    "Previously, my focus in the notebooks was on obtaining labeled data for my chatbot. However, this current notebook is centered around utilizing PyTorch for the classification of intents within fresh, unseen user-generated data. The model has transitioned to a supervised learning approach, leveraging the labels derived from the unsupervised learning conducted in the preceding notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RASA Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasa trains this intent classification step with SVM and GridsearchCV because they can try different configurations ([source](https://medium.com/bhavaniravi/intent-classification-demystifying-rasanlu-part-4-685fc02f5c1d)). When deploying preprocessing pipeline should remain same between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.17.3-py3-none-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/saggysimmba/Library/Python/3.12/lib/python/site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/saggysimmba/Library/Python/3.12/lib/python/site-packages (from wandb) (4.2.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/saggysimmba/Library/Python/3.12/lib/python/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (2.31.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.7.1-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp312-cp312-macosx_10_9_universal2.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/saggysimmba/Library/Python/3.12/lib/python/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.17.3-py3-none-macosx_11_0_arm64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.7.1-py2.py3-none-any.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.2/300.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp312-cp312-macosx_10_9_universal2.whl (16 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.7.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import wandb\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.2.2\n",
      "Numpy: 1.26.4\n",
      "Sklearn: 1.4.2\n",
      "Training data:                                                track  \\\n",
      "0                                  [no, information]   \n",
      "1  [issue, is, resolved, and, item, is, being, re...   \n",
      "2  [expected, delivery, date, is, th, october, tr...   \n",
      "3  [expected, delivery, date, is, th, october, tr...   \n",
      "4               [no, emails, no, reason, for, delay]   \n",
      "\n",
      "                                             support  \\\n",
      "0  [very, poor, feedback, very, disappointing, se...   \n",
      "1  [already, done, i, am, frankly, fed, up, with,...   \n",
      "2  [very, poor, feedback, very, disappointing, se...   \n",
      "3  [can, see, you, have, replied, to, others, who...   \n",
      "4  [my, issue, is, not, resolved, really, should,...   \n",
      "\n",
      "                                             quality  \\\n",
      "0   [done, attached, is, the, proof, of, completion]   \n",
      "1                 [return, pick, up, not, happening]   \n",
      "2  [your, target, completed, return, policy, expi...   \n",
      "3  [order, is, lost, no, one, taking, responsibil...   \n",
      "4  [tampered, supplied, defective, fake, failedde...   \n",
      "\n",
      "                                            discount  \\\n",
      "0                 [amazon, pay, transaction, amt, ₹]   \n",
      "1                                 [next, sale, date]   \n",
      "2          [mrp, on, product, rs, mrp, on, site, rs]   \n",
      "3  [but, flipkart, and, shop, clues, are, giving,...   \n",
      "4  [but, flipkart, and, shop, clues, are, giving,...   \n",
      "\n",
      "                                         account  \\\n",
      "0                      [email, account, details]   \n",
      "1                      [email, account, details]   \n",
      "2                      [email, account, details]   \n",
      "3  [the, credit, card, information, is, correct]   \n",
      "4                        [account, email, email]   \n",
      "\n",
      "                  speak_representative           greeting   goodbye  \\\n",
      "0                [talk, human, please]               [hi]   [goodb]   \n",
      "1  [let, me, talk, to, apple, support]            [hello]     [bye]   \n",
      "2       [can, i, speak, agent, person]        [whats, up]   [thank]   \n",
      "3                [talk, human, please]    [good, morning]  [thanks]   \n",
      "4  [let, me, talk, to, apple, support]  [good, afternoon]    [done]   \n",
      "\n",
      "     challenge_robot  \n",
      "0     [robot, human]  \n",
      "1  [are, you, robot]  \n",
      "2    [who, are, you]  \n",
      "3     [robot, human]  \n",
      "4  [are, you, robot]  \n"
     ]
    }
   ],
   "source": [
    "# Standard \n",
    "import collections\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "import numpy as np\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "\n",
    "# Machine Learning\n",
    "import sklearn\n",
    "print(f\"Sklearn: {sklearn.__version__}\")\n",
    "\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Preprocessing and Torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# from torchtext.data import get_tokenizer\n",
    "\n",
    "# Reading in training data\n",
    "train = pd.read_pickle('../objects/train.pkl')\n",
    "print(f'Training data: {train.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msinhasagar507\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cd47e10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration for training\n",
    "# Change all of the following configurations as per the specifications in the original repo \n",
    "# Set a seed value \n",
    "seed_value = 12321 \n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.melt(train)\n",
    "train.columns = [\"intent\", \"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>challenge_robot</td>\n",
       "      <td>[who, are, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>challenge_robot</td>\n",
       "      <td>[are, you, robot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>support</td>\n",
       "      <td>[i, tried, to, find, a, customer, services, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speak_representative</td>\n",
       "      <td>[let, me, talk, to, apple, support]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>challenge_robot</td>\n",
       "      <td>[who, are, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>support</td>\n",
       "      <td>[really, disappointed, with, your, service, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>account</td>\n",
       "      <td>[does, amazon, no, longer, provide, refund, if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>account</td>\n",
       "      <td>[i, should, not, have, to, speak, to, now, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>account</td>\n",
       "      <td>[i, have, found, out, some, discrepancy, in, y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    intent                                             tokens\n",
       "0          challenge_robot                                    [who, are, you]\n",
       "1          challenge_robot                                  [are, you, robot]\n",
       "2                  support  [i, tried, to, find, a, customer, services, em...\n",
       "3     speak_representative                [let, me, talk, to, apple, support]\n",
       "4          challenge_robot                                    [who, are, you]\n",
       "...                    ...                                                ...\n",
       "8995               support  [really, disappointed, with, your, service, co...\n",
       "8996               account  [does, amazon, no, longer, provide, refund, if...\n",
       "8997               account  [i, should, not, have, to, speak, to, now, a, ...\n",
       "8998               goodbye                                            [thank]\n",
       "8999               account  [i, have, found, out, some, discrepancy, in, y...\n",
       "\n",
       "[9000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df = train.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent    object\n",
      "tokens    object\n",
      "dtype: object\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Print the data types of the columns\n",
    "print(shuffled_df.dtypes)\n",
    "\n",
    "# Check the data types of each row in the \"tokens\" column and if its not a list, highlight the the error \n",
    "# Don't print it, log it \n",
    "print(\" \")\n",
    "for index, row in shuffled_df.iterrows():\n",
    "    if not isinstance(row[\"tokens\"], list):\n",
    "        print(f\"Error: {row['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [token_lst for token_lst in shuffled_df['tokens']]\n",
    "X = [*X]\n",
    "y = [*shuffled_df['intent'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saggysimmba/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saggysimmba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchtext tokenizer \n",
    "- Add description later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of Action\n",
    "- Prepare the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Applications/saggydev/projects_learning/amazon_support/notebooks'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps taken\n",
    "    -   the words would involve creating a vocabulary dictionary to map words to indices \n",
    "    -   For each sequence, the words are converted into their corresponding indices based on the word dictionary \n",
    "    - When feeding sentences into the model, ensure a consistent sequence length is crucial \n",
    "    - To achieve this, sequences are padded with zeros until they reach the length of the longest sequence \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape checks:\n",
      "X_train: 6300 X_val: 2700\n",
      "y_train: 6300 y_val: 2700\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'train' is a DataFrame containing 'Utterance' and 'Intent' columns\n",
    "\n",
    "# Tokenize the text data using PyTorch's tokenizer\n",
    "# The text already seems to be tokenized \n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, \n",
    "                                                  shuffle=True, stratify=y, random_state=7)\n",
    "\n",
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "\n",
    "# Convert encoded targets to PyTorch tensors\n",
    "y_train_encoded = torch.tensor(y_train_encoded, dtype=torch.long) \n",
    "y_val_encoded = torch.tensor(y_val_encoded, dtype=torch.long)\n",
    "\n",
    "print(f'\\nShape checks:\\nX_train: {len(X_train)} X_val: {len(X_val)}\\ny_train: {len(y_train_encoded)} y_val: {len(y_val_encoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build a vocabulary: This is something I hadve just added \n",
    "from collections import Counter\n",
    "word_counts = Counter(token for sentence in X for token in sentence)\n",
    "vocabulary = {word: i+1 for i, (word, _) in enumerate(word_counts.items())}  # +1 for zero padding\n",
    "vocab_size = len(vocabulary) + 1  # +1 for unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4630"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encocde sentences as sequences of integers: This is something I have just added\n",
    "def encode_sequences(tokenized_sentences, vocab):\n",
    "    sequences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sequence = [vocab.get(word, 0) for word in sentence]  # 0 for unknown words\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "encoded_X_train = encode_sequences(X_train, vocabulary)\n",
    "encoded_X_val = encode_sequences(X_val, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length: This is something I have just added\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Convert encoded sequences to PyTorch tensors\n",
    "encoded_X_train_tensors = [torch.tensor(seq) for seq in encoded_X_train]\n",
    "encoded_X_val_tensors = [torch.tensor(seq) for seq in encoded_X_val]\n",
    "\n",
    "# Pad sequences\n",
    "# Set batch_first=True to have the batch dimension first\n",
    "padded_X_train = pad_sequence(encoded_X_train_tensors, batch_first=True, padding_value=0)\n",
    "padded_X_val = pad_sequence(encoded_X_val_tensors, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6300, 61])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Use glove word embeddings \n",
    "embeddings_index = {}\n",
    "f = open(\"../models/glove.twitter.27B/glove.twitter.27B.100d.txt\", \"r\", encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in vocabulary.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just an experimental check\n",
    "from torch.nn import Embedding  \n",
    "# embedding_layer = Embedding(num_embeddings=embedding_matrix_tensor.size(0), \n",
    "#                             embedding_dim=embedding_matrix_tensor.size(1), \n",
    "#                             _weight=embedding_matrix_tensor)\n",
    "\n",
    "# # Freeze the embedding layer\n",
    "# embedding_layer.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming padded_X_train and padded_X_val are NumPy arrays\n",
    "padded_X_train_tensor = torch.LongTensor(padded_X_train)\n",
    "padded_X_val_tensor = torch.LongTensor(padded_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = padded_X_train_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "embedding_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "embedding.weight = nn.Parameter(embedding_matrix_tensor)\n",
    "embedding.weight.requires_grad = False  # To not train the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"my-awesome-project\",\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.01,\n",
    "#     \"architecture\": \"LSTM-RNN\",\n",
    "#     \"dataset\": \"custom-intent-data\",\n",
    "#     \"optimizer\": \"Adam\",\n",
    "#     \"epochs\": 20,\n",
    "#     \"batch_size\": 32, \n",
    "#     \"embedding_size\": 100,\n",
    "#     \"hidden_size\": 128,\n",
    "#     \"output_size\": 9,\n",
    "#     \"num_layers\": 2,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"eval_metric\": \"accuracy\"\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: xo10spg7\n",
      "Sweep URL: https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7\n"
     ]
    }
   ],
   "source": [
    "# Define the sweep config \n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"val_accuracy\"}, \n",
    "    \"parameters\": {\n",
    "                    \"learning_rate\": {\"values\": [0.01, 0.001, 0.0001]},\n",
    "                    \"epochs\": {\"values\": [30]},\n",
    "                    \"batch_size\": {\"values\": [32, 64, 128]},\n",
    "                    \"embedding_size\": {\"values\": [100]},\n",
    "                    \"hidden_size\": {\"values\": [64, 128, 256]},\n",
    "                    \"output_size\": {\"values\": [9]},\n",
    "                    \"num_layers\": {\"values\": [1, 2, 3]},\n",
    "                    \"dropout\": {\"values\": [0.1, 0.2, 0.3]}, \n",
    "                    \"scheduler_lambda_epoch_threshold\": {\"values\": [10]},\n",
    "                    \"scheduler_decay_rate\": {\"values\": [-0.1]}\n",
    "                }\n",
    "}\n",
    "\n",
    "sweep_defaults = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 32, \n",
    "    \"embedding_size\": 100,\n",
    "    \"hidden_size\": 128,\n",
    "    \"output_size\": 9,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"eval_metric\": \"accuracy\", \n",
    "    \"scheduler_lambda_epoch_threshold\": 10,\n",
    "    \"scheduler_decay_rate\": -0.1\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"intent-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "    \n",
    "class Config: \n",
    "    VOCAB_SIZE = 0\n",
    "    BATCH_SIZE = 32 \n",
    "    EMB_SIZE = 300 \n",
    "    OUT_SIZE = 9 # Corresponds to the number of intents\n",
    "    NUM_FOLDS = 5 \n",
    "    NUM_EPOCHS = 5\n",
    "    NUM_WORKERS = 8\n",
    "    \n",
    "    # I want to update the pretrained embedding weights during training process \n",
    "    # I want to use a pretrained embedding\n",
    "    OPTIMIZER = \"Adam\"\n",
    "    EMB_WT_UPDATE = True\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False \n",
    "    PATIENCE = 6 \n",
    "    IS_BIDIRECTIONAL = True \n",
    "    \n",
    "     \n",
    "    # Model hyperparameters\n",
    "    MODEL_PARAMS = {\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"drop_out\": 0.4258,\n",
    "        \"lr\": 0.000366,\n",
    "        \"weight_decay\": 0.00001\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just an experimental check\n",
    "# from torch.nn import Embedding  \n",
    "# embedding_layer = Embedding(num_embeddings=embedding_matrix_tensor.size(0), \n",
    "#                             embedding_dim=embedding_matrix_tensor.size(1), \n",
    "#                             _weight=embedding_matrix_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the architecture later \n",
    "class IntentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, embedding_matrix): \n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding_dim = wandb.config[\"embedding_size\"]\n",
    "        embedding_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "        self.embedding = nn.Embedding(seq_len, self.embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix_tensor)\n",
    "        self.embedding.weight.requires_grad = False  # To not train the embedding layer\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.hidden_dim = wandb.config[\"hidden_size\"]\n",
    "        self.num_layers = wandb.config[\"num_layers\"]\n",
    "        self.dropout = wandb.config[\"dropout\"]\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, # Embedding dim = 50\n",
    "                            hidden_size=self.hidden_dim, # Hidden dim = 128\n",
    "                            num_layers=self.num_layers, # n_layers is 2 \n",
    "                            bidirectional=True, # Its a bidirectional LSTM\n",
    "                            dropout=self.dropout, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        # The output of this operation should be \n",
    "        \n",
    "        # Dense layers \n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden_dim*2, 600)  # 2 for bidirectional. Over here, its (128*2) = 256, 600 is the output dimension of the first dense layer\n",
    "        self.fc2 = nn.Linear(600, 600) # When passed through this layer, the output would be (600, 600)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(self.dropout)  \n",
    "        \n",
    "        # Output layer\n",
    "        self.output_dim = wandb.config[\"output_size\"]\n",
    "        self.out = nn.Linear(600, self.output_dim) ## Yaar idhr output hoga RNN ya LSTM ka (batch_size output_dim, no_of_classes) aayega kya? \n",
    "        # self.out_2 = nn.Linear(output_dim, 9)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # text = [batch_size, embed_length]\n",
    "        \n",
    "        # embeddings = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        # embedded = [batch_size, sent_length, emb_dim]\n",
    "\n",
    "        # if self.embedding_matrix is not None: \n",
    "        #     assert self.embeddings.shape == (inputs.shape[0], inputs.shape[1], self.embedding_dim)\n",
    "         \n",
    "        # pack_padded_sequence before feeding to the LSTM. This is required so PyTorch knows \n",
    "        # which elements of the sequence are padded and ignores them in the computation \n",
    "        # Accomplished only after the embedding step \n",
    "        # embeds_pack = pack_padded_sequence(embeddings, inputs_lengths, batch_first=True)\n",
    "        \n",
    "        # Get the dimensions of the packed sequence \n",
    "        # dimensions = embeds_pack.data.size()\n",
    "\n",
    "        # Assert the shape of input sequence \n",
    "        # assert inputs.shape == (Config.BATCH_SIZE, 1000)\n",
    "\n",
    "        embeddings = self.embedding(inputs)\n",
    "        # print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        _, (hidden, _) = self.lstm(embeddings)\n",
    "\n",
    "        # hidden shape: [num_layers*num_directions, batch_size, hidden_dim]\n",
    "        # print(f\"Hidden shape: {hidden.shape}\")\n",
    "        \n",
    "\n",
    "        \n",
    "        # Ours task being a classification model, we are only interested in the final hidden state and not the LSTM output \n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        final_hidden_forward = hidden[-2, :, :] # [batch_size, hidden_dim]\n",
    "        final_hidden_backward = hidden[-1, :, :] # [bacth_size, hidden_dim]\n",
    "\n",
    "        # print(f\"Final hidden forward shape: {final_hidden_forward.shape}\") # Iska shape is \n",
    "        # print(f\"Final hidden backward shape: {final_hidden_backward.shape}\")\n",
    "        \n",
    "        # Concat the final forward and hidden backward states \n",
    "        hidden = torch.cat((final_hidden_forward, final_hidden_backward), dim=1)\n",
    "        # print(f\"Hidden shape after concatenation: {hidden.shape}\")\n",
    "                \n",
    "        # Dense Linear Layers \n",
    "        dense_outputs_1 = self.fc1(hidden)\n",
    "        dense_outputs_1 = nn.ReLU()(dense_outputs_1)  \n",
    "        dense_outputs_2 = self.fc2(dense_outputs_1)\n",
    "        dense_outputs_2 = self.dropout(dense_outputs_2)\n",
    "        dense_outputs_2 = nn.ReLU()(dense_outputs_2) \n",
    "\n",
    "        # Final output classification layer\n",
    "        # Applying the Softmax layer \n",
    "        final_output = (self.out(dense_outputs_2))\n",
    "        # print(f\"Final output shape: {final_output.shape}\")\n",
    "    \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_dim = wandb.config[\"embedding_size\"]\n",
    "        self.embedding_matrix = embedding_matrix   \n",
    "        self.hidden_dim = wandb.config[\"hidden_size\"]\n",
    "        self.output_dim = wandb.config[\"output_size\"]\n",
    "        self.n_layers = wandb.config[\"num_layers\"]\n",
    "        self.batch_size = wandb.config[\"batch_size\"]\n",
    "        self.epochs = wandb.config[\"epochs\"]\n",
    "        self.dropout = wandb.config[\"dropout\"]\n",
    "        # Assuming IntentClassifier is defined elsewhere and matches these parameters\n",
    "        # print(self.seq_len, self.embedding_dim, self.hidden_dim, self.output_dim, self.embedding_matrix)\n",
    "        self.model = IntentClassifier(self.seq_len, self.embedding_matrix)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # Assuming Config.OPTIMIZER is a valid PyTorch optimizer class\n",
    "        self.learning_rate = wandb.config[\"learning_rate\"]\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler_epoch_threshold = wandb.config[\"scheduler_lambda_epoch_threshold\"]\n",
    "        self.scheduler_decay_rate = wandb.config[\"scheduler_decay_rate\"]\n",
    "        self.epoch_lst = []\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        # X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "        # X_val = torch.tensor(X_val, dtype=torch.float)\n",
    "        # y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "        # y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "        # Assuming X_train, y_train, X_val, y_val are already tensors\n",
    "        # Ensure they have matching first dimensions\n",
    "        assert X_train.shape[0] == y_train.shape[0], \"Training feature and label count mismatch\"\n",
    "        assert X_val.shape[0] == y_val.shape[0], \"Validation feature and label count mismatch\"\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=self.batch_size)\n",
    "\n",
    "        train_accuracies_epoch, val_accuracies_epoch = [], []\n",
    "        self.valid_loss_min = np.Inf\n",
    "\n",
    "        # Assuming `optimizer` is already defined\n",
    "        # Define the lambda function for learning rate adjustment using W&B config\n",
    "        lambda_lr = lambda epoch: 1 if epoch < self.scheduler_epoch_threshold else torch.exp(torch.tensor(-self.scheduler_decay_rate))\n",
    "\n",
    "        # Initialize the LambdaLR scheduler with the optimizer and lambda function\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, valid_loss = 0.0, 0.0\n",
    "            correct, total = 0, 0\n",
    "\n",
    "            self.model.train()\n",
    "            for data, target in train_loader:\n",
    "                # Log the shape of the data and target tensors\n",
    "                # assert data.shape == (self.batch_size, self.embedding_dim), f\"Data shape mismatch: {data.shape}\"\n",
    "                # assert target.shape == (self.batch_size,), f\"Target shape mismatch: {target.shape}\"\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # print(output.shape)\n",
    "                pred_labels = torch.argmax(output, 1)\n",
    "                correct += (pred_labels == target).sum().item()\n",
    "                total += target.size(0)\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "\n",
    "\n",
    "            train_accuracy = 100 * correct / total\n",
    "            train_accuracies_epoch.append(train_accuracy)\n",
    "\n",
    "            # Log the training loss and accuracy\n",
    "            # wandb.log({\"Training Accuracy\": train_accuracy, \"Training Loss\": train_loss})\n",
    "\n",
    "            self.model.eval()\n",
    "            correct, total = 0, 0\n",
    "            for data, target in val_loader:\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                pred_labels = torch.argmax(output, 1)\n",
    "                correct += (pred_labels == target).sum().item()\n",
    "                total += target.size(0)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "            valid_accuracy = 100 * correct / total\n",
    "            val_accuracies_epoch.append(valid_accuracy)\n",
    "\n",
    "            # Log the validation loss and accuracy\n",
    "            # print(f\"Epoch: {epoch+1}/{self.epochs}.. Training Accuracy: {train_accuracy:.3f}.. Validation Accuracy: {valid_accuracy:.3f}\")\n",
    "\n",
    "            # Log epoch-wise accuracies\n",
    "            wandb.log({\"epoch\": epoch, \"Training Accuracy\": train_accuracy, \"Validation Accuracy\": valid_accuracy, \"Training Loss\": train_loss, \"Validation Loss\": valid_loss})\n",
    "\n",
    "            if valid_loss <= self.valid_loss_min:\n",
    "                print(f\"Validation loss decreased ({self.valid_loss_min:.3f} --> {valid_loss:.3f}). Saving model...\")\n",
    "                \n",
    "                # Log the model and its parameters \n",
    "                # wandb.log_artifact(self.model)\n",
    "\n",
    "                torch.save(self.model.state_dict(), \"../models/intent_classification_model.pt\")\n",
    "                self.valid_loss_min = valid_loss\n",
    "\n",
    "            self.epoch_lst.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things I Need to Add\n",
    "- WandB table\n",
    "- Log artifact (model)\n",
    "- For now, include all the basic elements (then we can improve upon this in the future)\n",
    "- Ability to track across multiple hyperparameters\n",
    "- Set the configuration after the run is complete\n",
    "- Sweeps (...) AND Improvisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# trainer = ModelTrainer(padded_X_train.shape[1])\n",
    "# train_features, val_features = padded_X_train, padded_X_val\n",
    "# trainer.train(train_features, y_train_encoded, val_features, y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iy106ix3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240705_225805-iy106ix3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/iy106ix3' target=\"_blank\">solar-sweep-2</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/iy106ix3' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/iy106ix3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 3141.475). Saving model...\n",
      "Validation loss decreased (3141.475 --> 2567.874). Saving model...\n",
      "Validation loss decreased (2567.874 --> 2428.816). Saving model...\n",
      "Validation loss decreased (2428.816 --> 2401.441). Saving model...\n",
      "Validation loss decreased (2401.441 --> 2366.507). Saving model...\n",
      "Validation loss decreased (2366.507 --> 2340.427). Saving model...\n",
      "Validation loss decreased (2340.427 --> 2318.419). Saving model...\n",
      "Validation loss decreased (2318.419 --> 2256.103). Saving model...\n",
      "Validation loss decreased (2256.103 --> 2195.308). Saving model...\n",
      "Validation loss decreased (2195.308 --> 2148.312). Saving model...\n",
      "Validation loss decreased (2148.312 --> 2065.886). Saving model...\n",
      "Validation loss decreased (2065.886 --> 2037.632). Saving model...\n",
      "Validation loss decreased (2037.632 --> 2029.827). Saving model...\n",
      "Validation loss decreased (2029.827 --> 2009.972). Saving model...\n",
      "Validation loss decreased (2009.972 --> 1943.518). Saving model...\n",
      "Validation loss decreased (1943.518 --> 1935.604). Saving model...\n",
      "Validation loss decreased (1935.604 --> 1877.720). Saving model...\n",
      "Validation loss decreased (1877.720 --> 1877.098). Saving model...\n",
      "Validation loss decreased (1877.098 --> 1823.509). Saving model...\n",
      "Validation loss decreased (1823.509 --> 1783.558). Saving model...\n",
      "Validation loss decreased (1783.558 --> 1767.763). Saving model...\n",
      "Validation loss decreased (1767.763 --> 1765.666). Saving model...\n",
      "Validation loss decreased (1765.666 --> 1748.656). Saving model...\n",
      "Validation loss decreased (1748.656 --> 1725.600). Saving model...\n",
      "Validation loss decreased (1725.600 --> 1653.671). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e96c6bd926445a2b9fbfb2bc7094cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>Training Loss</td><td>█▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██▇█████</td></tr><tr><td>Validation Loss</td><td>█▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>78.52381</td></tr><tr><td>Training Loss</td><td>3352.76792</td></tr><tr><td>Validation Accuracy</td><td>74.88889</td></tr><tr><td>Validation Loss</td><td>1653.67141</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-sweep-2</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/iy106ix3' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/iy106ix3</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_225805-iy106ix3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ejd55f41 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240705_231231-ejd55f41</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/ejd55f41' target=\"_blank\">breezy-sweep-3</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/ejd55f41' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/ejd55f41</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 5462.890). Saving model...\n",
      "Validation loss decreased (5462.890 --> 3606.357). Saving model...\n",
      "Validation loss decreased (3606.357 --> 2822.762). Saving model...\n",
      "Validation loss decreased (2822.762 --> 2510.454). Saving model...\n",
      "Validation loss decreased (2510.454 --> 2344.238). Saving model...\n",
      "Validation loss decreased (2344.238 --> 2249.312). Saving model...\n",
      "Validation loss decreased (2249.312 --> 2120.601). Saving model...\n",
      "Validation loss decreased (2120.601 --> 2028.312). Saving model...\n",
      "Validation loss decreased (2028.312 --> 2000.398). Saving model...\n",
      "Validation loss decreased (2000.398 --> 1845.586). Saving model...\n",
      "Validation loss decreased (1845.586 --> 1788.166). Saving model...\n",
      "Validation loss decreased (1788.166 --> 1769.559). Saving model...\n",
      "Validation loss decreased (1769.559 --> 1733.283). Saving model...\n",
      "Validation loss decreased (1733.283 --> 1723.113). Saving model...\n",
      "Validation loss decreased (1723.113 --> 1707.824). Saving model...\n",
      "Validation loss decreased (1707.824 --> 1647.787). Saving model...\n",
      "Validation loss decreased (1647.787 --> 1647.320). Saving model...\n",
      "Validation loss decreased (1647.320 --> 1610.706). Saving model...\n",
      "Validation loss decreased (1610.706 --> 1605.818). Saving model...\n",
      "Validation loss decreased (1605.818 --> 1572.701). Saving model...\n",
      "Validation loss decreased (1572.701 --> 1559.599). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fc6d28b1be40539a904adafb859da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▃▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>Training Loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▃▁▄▄▅▅▆▆▆▇▇▇▇▇▇▇████▇█████████</td></tr><tr><td>Validation Loss</td><td>█▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>81.19048</td></tr><tr><td>Training Loss</td><td>2888.11346</td></tr><tr><td>Validation Accuracy</td><td>76.62963</td></tr><tr><td>Validation Loss</td><td>1571.52066</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-sweep-3</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/ejd55f41' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/ejd55f41</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_231231-ejd55f41/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0lt45i7p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240705_231545-0lt45i7p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/0lt45i7p' target=\"_blank\">unique-sweep-4</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/0lt45i7p' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/0lt45i7p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2268.871). Saving model...\n",
      "Validation loss decreased (2268.871 --> 1758.952). Saving model...\n",
      "Validation loss decreased (1758.952 --> 1572.210). Saving model...\n",
      "Validation loss decreased (1572.210 --> 1502.867). Saving model...\n",
      "Validation loss decreased (1502.867 --> 1495.706). Saving model...\n",
      "Validation loss decreased (1495.706 --> 1477.524). Saving model...\n",
      "Validation loss decreased (1477.524 --> 1475.703). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b15486087cb4efa937512509a58e1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▅▅▆▆▆▆▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Training Loss</td><td>█▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▅▇▇▇█▇███▇▇███▇█▇▇▇▇██▇█▇████</td></tr><tr><td>Validation Loss</td><td>▅▃▂▁▁▁▁▁▂▂▂▂▂▂▃▄▄▅▆▄▅▆▅▇▇█▇▆██</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>93.26984</td></tr><tr><td>Training Loss</td><td>740.70761</td></tr><tr><td>Validation Accuracy</td><td>77.7037</td></tr><tr><td>Validation Loss</td><td>2744.82757</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-4</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/0lt45i7p' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/0lt45i7p</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_231545-0lt45i7p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 22jm6tci with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240705_231809-22jm6tci</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/22jm6tci' target=\"_blank\">frosty-sweep-5</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/22jm6tci' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/22jm6tci</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2408.844). Saving model...\n",
      "Validation loss decreased (2408.844 --> 2003.061). Saving model...\n",
      "Validation loss decreased (2003.061 --> 1826.533). Saving model...\n",
      "Validation loss decreased (1826.533 --> 1733.941). Saving model...\n",
      "Validation loss decreased (1733.941 --> 1576.946). Saving model...\n",
      "Validation loss decreased (1576.946 --> 1561.977). Saving model...\n",
      "Validation loss decreased (1561.977 --> 1498.791). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b9a93572cf43ef833cba8344ab71df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇███████████████</td></tr><tr><td>Training Loss</td><td>█▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▆▇▇▇▇██████████████████████</td></tr><tr><td>Validation Loss</td><td>▅▃▂▂▁▂▁▁▁▂▁▂▂▃▃▄▃▄▄▃▅▅▅▆▆▆▆▆▇█</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>92.92063</td></tr><tr><td>Training Loss</td><td>823.59919</td></tr><tr><td>Validation Accuracy</td><td>78.88889</td></tr><tr><td>Validation Loss</td><td>3122.60718</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-sweep-5</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/22jm6tci' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/22jm6tci</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_231809-22jm6tci/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u2ff35na with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240705_232807-u2ff35na</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/u2ff35na' target=\"_blank\">rosy-sweep-6</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/u2ff35na' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/u2ff35na</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2093.264). Saving model...\n",
      "Validation loss decreased (2093.264 --> 1738.576). Saving model...\n",
      "Validation loss decreased (1738.576 --> 1610.715). Saving model...\n",
      "Validation loss decreased (1610.715 --> 1570.484). Saving model...\n",
      "Validation loss decreased (1570.484 --> 1485.470). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1931f7e66542a9a73e3047d08efd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>Training Loss</td><td>█▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▅▄▆▆▇▇▇██████▇██████▇████▇█▇▇</td></tr><tr><td>Validation Loss</td><td>▅▂▃▂▁▁▂▁▂▁▁▂▂▃▃▄▃▄▄▅▅▅▆▆▇▇▇▇█▇</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>93.22222</td></tr><tr><td>Training Loss</td><td>791.30516</td></tr><tr><td>Validation Accuracy</td><td>77.74074</td></tr><tr><td>Validation Loss</td><td>2578.9766</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-sweep-6</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/u2ff35na' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/u2ff35na</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_232807-u2ff35na/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bqaesbfp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240705_233207-bqaesbfp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/bqaesbfp' target=\"_blank\">smooth-sweep-7</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/bqaesbfp' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/bqaesbfp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2521.099). Saving model...\n",
      "Validation loss decreased (2521.099 --> 2389.947). Saving model...\n",
      "Validation loss decreased (2389.947 --> 2362.591). Saving model...\n",
      "Validation loss decreased (2362.591 --> 2340.537). Saving model...\n",
      "Validation loss decreased (2340.537 --> 2232.635). Saving model...\n",
      "Validation loss decreased (2232.635 --> 2184.911). Saving model...\n",
      "Validation loss decreased (2184.911 --> 2157.744). Saving model...\n",
      "Validation loss decreased (2157.744 --> 1988.137). Saving model...\n",
      "Validation loss decreased (1988.137 --> 1974.370). Saving model...\n",
      "Validation loss decreased (1974.370 --> 1931.094). Saving model...\n",
      "Validation loss decreased (1931.094 --> 1874.561). Saving model...\n",
      "Validation loss decreased (1874.561 --> 1774.692). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8059b07a79d4eeb89ebb51256f795c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.057 MB uploaded\\r'), FloatProgress(value=0.3304032745084882, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▄▄▄▄▅▄▅▅▅▆▆▆▆▇▇▇████▅▅▆▇▇▇██</td></tr><tr><td>Training Loss</td><td>█▄▄▃▃▃▃▄▃▃▃▃▂▂▂▂▂▁▁▁▁▂▅▃▂▂▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▂▁▂▃▃▄▄▄▄▄▅▆▆▆▆▆▇▇▇▇█▇▄▆▆▇▇█▇█</td></tr><tr><td>Validation Loss</td><td>▅█▄▄▄▄▃▄▃▃▃▂▂▂▂▁▁▁▂▂▄▂▄▂▂▁▁▁▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>76.04762</td></tr><tr><td>Training Loss</td><td>3501.4406</td></tr><tr><td>Validation Accuracy</td><td>72.77778</td></tr><tr><td>Validation Loss</td><td>1806.45162</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-7</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/bqaesbfp' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/bqaesbfp</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_233207-bqaesbfp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wayfssol with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240705_234455-wayfssol</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/wayfssol' target=\"_blank\">lively-sweep-8</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/wayfssol' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/wayfssol</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2789.680). Saving model...\n",
      "Validation loss decreased (2789.680 --> 2417.872). Saving model...\n",
      "Validation loss decreased (2417.872 --> 2393.793). Saving model...\n",
      "Validation loss decreased (2393.793 --> 2244.073). Saving model...\n",
      "Validation loss decreased (2244.073 --> 2176.287). Saving model...\n",
      "Validation loss decreased (2176.287 --> 2121.142). Saving model...\n",
      "Validation loss decreased (2121.142 --> 2068.463). Saving model...\n",
      "Validation loss decreased (2068.463 --> 2009.873). Saving model...\n",
      "Validation loss decreased (2009.873 --> 2005.708). Saving model...\n",
      "Validation loss decreased (2005.708 --> 1949.864). Saving model...\n",
      "Validation loss decreased (1949.864 --> 1818.043). Saving model...\n",
      "Validation loss decreased (1818.043 --> 1776.522). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e9d304893143c7bbd5bccda598972c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.057 MB uploaded\\r'), FloatProgress(value=0.33043098964886675, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▂▃▃▂▂▃▃▃▃▃▄▅▅▅▅▅▅▅▆▆▆▇▇▆▇▇███</td></tr><tr><td>Training Loss</td><td>█▆▅▅█▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▅▂▂▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▃▄▄▅▁▅▄▅▅▅▅▆▆▆▆▆▆▆▇▆▇▇▇███████</td></tr><tr><td>Validation Loss</td><td>▅▃▃▃█▄▄▄▄▄▃▃▂▃▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>73.50794</td></tr><tr><td>Training Loss</td><td>3655.94015</td></tr><tr><td>Validation Accuracy</td><td>69.59259</td></tr><tr><td>Validation Loss</td><td>1864.53704</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-sweep-8</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/wayfssol' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/wayfssol</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_234455-wayfssol/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h2goocmw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240705_235216-h2goocmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/h2goocmw' target=\"_blank\">radiant-sweep-9</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/h2goocmw' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/h2goocmw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 5935.748). Saving model...\n",
      "Validation loss decreased (5935.748 --> 5934.399). Saving model...\n",
      "Validation loss decreased (5934.399 --> 5934.095). Saving model...\n",
      "Validation loss decreased (5934.095 --> 5933.562). Saving model...\n",
      "Validation loss decreased (5933.562 --> 5933.284). Saving model...\n",
      "Validation loss decreased (5933.284 --> 5933.022). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f9a2980af34bcea889e59f94a355ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.056 MB uploaded\\r'), FloatProgress(value=0.3326408944284001, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▅▃▇█▃▆▄▃▄▄▂▅█▇▅▅▄▄▄▃█▆▅▇▆▁▄▆▇▇</td></tr><tr><td>Training Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>█▅▆▄▂▄▂▃▃▃▁▃▄▂▁▇█▄▆▄▃▅▅▂▇▄▄▂▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>11.01587</td></tr><tr><td>Training Loss</td><td>13850.44065</td></tr><tr><td>Validation Accuracy</td><td>11.11111</td></tr><tr><td>Validation Loss</td><td>5935.21261</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-sweep-9</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/h2goocmw' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/h2goocmw</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_235216-h2goocmw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kmip8dhm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240706_002930-kmip8dhm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/kmip8dhm' target=\"_blank\">fresh-sweep-10</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/kmip8dhm' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/kmip8dhm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2477.465). Saving model...\n",
      "Validation loss decreased (2477.465 --> 2392.905). Saving model...\n",
      "Validation loss decreased (2392.905 --> 2386.521). Saving model...\n",
      "Validation loss decreased (2386.521 --> 2322.492). Saving model...\n",
      "Validation loss decreased (2322.492 --> 2307.322). Saving model...\n",
      "Validation loss decreased (2307.322 --> 2220.819). Saving model...\n",
      "Validation loss decreased (2220.819 --> 2039.772). Saving model...\n",
      "Validation loss decreased (2039.772 --> 1948.349). Saving model...\n",
      "Validation loss decreased (1948.349 --> 1775.302). Saving model...\n",
      "Validation loss decreased (1775.302 --> 1697.912). Saving model...\n",
      "Validation loss decreased (1697.912 --> 1691.446). Saving model...\n",
      "Validation loss decreased (1691.446 --> 1606.661). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c594460ba22a406d9b69ff5265be4270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▂▂▂▂▂▃▂▃▃▃▃▄▄▅▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>Training Loss</td><td>█▆▆▆▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▁▁▁▁▁▁▂▂▂▂▄▅▅▆▇▇▇▇█████████▆█</td></tr><tr><td>Validation Loss</td><td>▇▇▇▇▇▇█▇▇▆▆▆▄▄▂▂▂▁▁▂▂▂▂▂▂▄▄▅▅▄</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>91.20635</td></tr><tr><td>Training Loss</td><td>1350.54969</td></tr><tr><td>Validation Accuracy</td><td>77.22222</td></tr><tr><td>Validation Loss</td><td>1984.74669</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-10</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/kmip8dhm' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/kmip8dhm</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240706_002930-kmip8dhm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: su3g6rbw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240706_005259-su3g6rbw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/su3g6rbw' target=\"_blank\">playful-sweep-11</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/su3g6rbw' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/su3g6rbw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2497.142). Saving model...\n",
      "Validation loss decreased (2497.142 --> 2394.024). Saving model...\n",
      "Validation loss decreased (2394.024 --> 2381.780). Saving model...\n",
      "Validation loss decreased (2381.780 --> 2352.765). Saving model...\n",
      "Validation loss decreased (2352.765 --> 2297.712). Saving model...\n",
      "Validation loss decreased (2297.712 --> 2028.818). Saving model...\n",
      "Validation loss decreased (2028.818 --> 1835.807). Saving model...\n",
      "Validation loss decreased (1835.807 --> 1722.941). Saving model...\n",
      "Validation loss decreased (1722.941 --> 1710.072). Saving model...\n",
      "Validation loss decreased (1710.072 --> 1680.368). Saving model...\n",
      "Validation loss decreased (1680.368 --> 1667.042). Saving model...\n",
      "Validation loss decreased (1667.042 --> 1662.684). Saving model...\n",
      "Validation loss decreased (1662.684 --> 1646.994). Saving model...\n",
      "Validation loss decreased (1646.994 --> 1608.124). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33f24ee9fa747df8c05fbfb794c6f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▃▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>Training Loss</td><td>█▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▁▁▁▂▂▄▅▆▆▆▇▇▇▇▇██████████████</td></tr><tr><td>Validation Loss</td><td>█▇▇▇▇▇▆▄▃▂▂▂▁▁▁▂▂▁▁▃▃▃▄▅▄▄▄▅▅▄</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>91.65079</td></tr><tr><td>Training Loss</td><td>1294.47525</td></tr><tr><td>Validation Accuracy</td><td>78.33333</td></tr><tr><td>Validation Loss</td><td>1978.22557</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-sweep-11</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/su3g6rbw' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/su3g6rbw</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240706_005259-su3g6rbw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2cerg1cy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240706_090641-2cerg1cy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/2cerg1cy' target=\"_blank\">driven-sweep-12</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/2cerg1cy' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/2cerg1cy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2978.162). Saving model...\n",
      "Validation loss decreased (2978.162 --> 2372.334). Saving model...\n",
      "Validation loss decreased (2372.334 --> 2235.492). Saving model...\n",
      "Validation loss decreased (2235.492 --> 2128.665). Saving model...\n",
      "Validation loss decreased (2128.665 --> 1993.219). Saving model...\n",
      "Validation loss decreased (1993.219 --> 1958.460). Saving model...\n",
      "Validation loss decreased (1958.460 --> 1862.414). Saving model...\n",
      "Validation loss decreased (1862.414 --> 1842.353). Saving model...\n",
      "Validation loss decreased (1842.353 --> 1824.502). Saving model...\n",
      "Validation loss decreased (1824.502 --> 1754.624). Saving model...\n",
      "Validation loss decreased (1754.624 --> 1702.340). Saving model...\n",
      "Validation loss decreased (1702.340 --> 1688.043). Saving model...\n",
      "Validation loss decreased (1688.043 --> 1664.600). Saving model...\n",
      "Validation loss decreased (1664.600 --> 1603.637). Saving model...\n",
      "Validation loss decreased (1603.637 --> 1584.064). Saving model...\n",
      "Validation loss decreased (1584.064 --> 1551.635). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1551.635 --> 1539.564). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e21e86aefde4a2493e79c5674b57135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.019 MB of 0.019 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (TransientError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>Training Loss</td><td>█▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Validation Loss</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▁▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>83.90476</td></tr><tr><td>Training Loss</td><td>2491.17176</td></tr><tr><td>Validation Accuracy</td><td>77.7037</td></tr><tr><td>Validation Loss</td><td>1608.28381</td></tr><tr><td>epoch</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">driven-sweep-12</strong> at: <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/2cerg1cy' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/2cerg1cy</a><br/> View project at: <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240706_090641-2cerg1cy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d8wuv96v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_decay_rate: -0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_lambda_epoch_threshold: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Applications/saggydev/projects_learning/amazon_support/notebooks/wandb/run-20240706_100718-d8wuv96v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinhasagar507/intent-classification/runs/d8wuv96v' target=\"_blank\">fanciful-sweep-13</a></strong> to <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinhasagar507/intent-classification' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/sweeps/xo10spg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinhasagar507/intent-classification/runs/d8wuv96v' target=\"_blank\">https://wandb.ai/sinhasagar507/intent-classification/runs/d8wuv96v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2537.062). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (2537.062 --> 2490.284). Saving model...\n",
      "Validation loss decreased (2490.284 --> 2430.366). Saving model...\n",
      "Validation loss decreased (2430.366 --> 2388.723). Saving model...\n",
      "Validation loss decreased (2388.723 --> 2338.892). Saving model...\n",
      "Validation loss decreased (2338.892 --> 2156.135). Saving model...\n",
      "Validation loss decreased (2156.135 --> 2067.222). Saving model...\n",
      "Validation loss decreased (2067.222 --> 2058.028). Saving model...\n",
      "Validation loss decreased (2058.028 --> 2027.226). Saving model...\n",
      "Validation loss decreased (2027.226 --> 1984.439). Saving model...\n",
      "Validation loss decreased (1984.439 --> 1940.864). Saving model...\n",
      "Validation loss decreased (1940.864 --> 1899.822). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff0828f8517497bb7ae09d2f873d0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.018 MB of 0.019 MB uploaded\\r'), FloatProgress(value=0.9353054597117921, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Define the Training Function\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=sweep_defaults) as run:\n",
    "        # Get hyperparameters\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Modify the trainer initialization and training process to use config parameters\n",
    "        trainer = ModelTrainer(padded_X_train.shape[1])\n",
    "        train_features, val_features = padded_X_train, padded_X_val\n",
    "        \n",
    "        # Assuming the trainer.train method is modified to accept epochs and batch_size\n",
    "        trainer.train(train_features, y_train_encoded, val_features, y_val_encoded)\n",
    "        \n",
    "# Step 4: Start the Sweep Agent\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data and related information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'architecture': 'LSTM-RNN', 'dataset': 'custom-intent-data', 'optimizer': 'Adam', 'epochs': 20, 'batch_size': 32, 'embedding_size': 100, 'hidden_size': 128, 'output_size': 9, 'num_layers': 2, 'dropout': 0.1, 'eval_metric': 'accuracy'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = IntentClassifier(seq_len, wandb.config[\"embedding_size\"], wandb.config[\"hidden_size\"], wandb.config[\"output_size\"], embedding_matrix)\n",
    "model.load_state_dict(torch.load(\"../models/intent_classification_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "def inference(text):\n",
    "    \"\"\"\n",
    "    Perform preprocessing and inference on the input text using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model for intent classification.\n",
    "    - text: The input text string.\n",
    "    - vocabulary: A dictionary mapping tokens to indices.\n",
    "    - seq_len: The fixed sequence length expected by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - pred_label: The predicted label index.\n",
    "    \"\"\"\n",
    "    # Preprocess the text\n",
    "    tokens = text.split()\n",
    "    indices = [vocabulary.get(token, 0) for token in tokens]  # Use 0 for unknown words\n",
    "    padded_indices = indices[:seq_len] + [0] * max(0, seq_len - len(indices))  # Pad with zeros\n",
    "    input_tensor = torch.tensor(padded_indices).unsqueeze(0)  # Add batch dimension\n",
    "    print(input_tensor.shape)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred_label = output\n",
    "    \n",
    "    return pred_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 61])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4125, -3.6488, -1.7868, -3.2215, -5.8661,  1.0337, -4.6215, -0.6400,\n",
       "         -0.6101]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(\"I want to book a flight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['quality'], dtype='<U20')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_label = label_encoder.inverse_transform([5])\n",
    "original_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon_support",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
