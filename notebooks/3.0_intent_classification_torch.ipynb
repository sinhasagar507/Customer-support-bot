{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Classification With PyTorch\n",
    "Previously, my focus in the notebooks was on obtaining labeled data for my chatbot. However, this current notebook is centered around utilizing PyTorch for the classification of intents within fresh, unseen user-generated data. The model has transitioned to a supervised learning approach, leveraging the labels derived from the unsupervised learning conducted in the preceding notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RASA Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasa trains this intent classification step with SVM and GridsearchCV because they can try different configurations ([source](https://medium.com/bhavaniravi/intent-classification-demystifying-rasanlu-part-4-685fc02f5c1d)). When deploying preprocessing pipeline should remain same between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.0.3\n",
      "Numpy: 1.26.2\n",
      "Sklearn: 1.3.2\n"
     ]
    }
   ],
   "source": [
    "# Standard \n",
    "import collections\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "import numpy as np\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "\n",
    "# Machine Learning\n",
    "import sklearn\n",
    "print(f\"Sklearn: {sklearn.__version__}\")\n",
    "\n",
    "# Deep Learning\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Preprocessing and Torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# Reading in training data\n",
    "train = pd.read_pickle('objects/train.pkl')\n",
    "print(f'Training data: {train.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train' is a DataFrame containing 'Utterance' and 'Intent' columns\n",
    "\n",
    "# Tokenize the text data using PyTorch's tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "X_train = [tokenizer(text) for text in train['Utterance']]\n",
    "y_train = train['Intent']\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, \n",
    "                                                  shuffle=True, stratify=y_train, random_state=7)\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_val = torch.tensor(y_val.values)\n",
    "\n",
    "print(f'\\nShape checks:\\nX_train: {len(X_train)} X_val: {len(X_val)}\\ny_train: {len(y_train)} y_val: {len(y_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchtext tokenizer \n",
    "- Add description later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of Action\n",
    "- Prepare the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Sagar Study\\\\ML and Learning\\\\Projects\\\\customer-support-bot\\\\amazon_customer_support\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps taken\n",
    "    -   the words would involve creating a vocabulary dictionary to map words to indices \n",
    "    -   For each sequence, the words are converted into their corresponding indices based on the word dictionary \n",
    "    - When feeding sentences into the model, ensure a consistent sequence length is crucial \n",
    "    - To achieve this, sequences are padded with zeros until they reach the length of the longest sequence \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize tge text data using PyTorch's tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Tokenize and encode the text data  \n",
    "X_train = [tokenizer(text) for text in train_df[\"Utterance\"]]\n",
    "y_train = train_df[\"Intent\"]\n",
    "\n",
    "# Split the data into train and validation sets \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size=0.3, shuffle=True, stratify = y_train, random_state = SEED_VALUE)\n",
    "\n",
    "# Convert labels to PyTorch Tensors \n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_val = torch.tensor(y_val.values)\n",
    "\n",
    "print(f\"\\nShape checks:\\nX_train: {len(X_train)} X_val: {len(X_val)} y_train: {len(y_train)} y_val: {len(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocab mapping for words \n",
    "word_to_idx = {word: index+1 for index, (word, _) in enumerate(t.word_index.items())}\n",
    "index_to_word = {index+1: word for index, (word, _) in enumerate(t.word_index.items())}\n",
    "\n",
    "# Convert documents to sequences of indices \n",
    "indexed_X_train = [[word_to_index[word] for word in doc] for doc in X_train]\n",
    "indexed_X_val = [[word_to_idx[word] for word in doc] for doc in X_val]\n",
    "\n",
    "# Pad sequences to a common length \n",
    "max_length = min(max(len(doc) for doc in indexed_X_train), 100)\n",
    "padded_X_train = pad_sequence([torch.tensor(doc[ :max_length]) + [0]*(max_length - len(doc) for doc in indexed_X_train], batch_first=True)\n",
    "padded_X_val = pad_sequence([torch.tensor(doc[: max_length] + [0] * (max_length - len(doc)) for doc in indexed_X_val], batch_first=True)\n",
    "                            \n",
    "# Define vocabulary size \n",
    "vocab_size = len(word_to_index) + 1 \n",
    "\n",
    "print(f\"Vocab size:\\n{vocab_size}\")\n",
    "print(f\"Max length:\\n{max_length}\")\n",
    "\n",
    "\n",
    "print(f\"padded_X_train\\n{padded_X_train}\")\n",
    "print(f\"padded_X_val\\n{padded_X_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example fits the Tokenizer with 5 small documents. The details of the fit Tokenizer are printed. Then the 5 documents are encoded using a word count \n",
    "\n",
    "Each document is encoded as a 9-element vector with one position for each word and the chosen encoding scheme value for each word position. In this case, a simple word count is used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models involve dealing with one-hot encoding for multiclass classification and using embeddings for document representations. Below are PyTorch specific of the provided statements\n",
    "\n",
    "If we are using Doc2Vec embeddings, how will we pass in our tweets? We may have to pass it in as full tweets. Check how we pass in the tweets. We may have to perform tokenization at a tweet level. If we pass it in, if it's Twweet 57, it will activate the node such that it gets multiplied out by the embeddings for the 57th document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are 4 different dimensionality options \n",
    "!ls models/glove.twitter.27B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we compute an index mapping of words to known embeddings by parsing the data dump of pre-trained embeddings \n",
    "- I use 50D because my X_train has a max_length of 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Glove Embeddings \n",
    "embeddings_index = {} \n",
    "\n",
    "with open(\"models/glove.twitter.27B/glove.twitter.27B.50d.txt\", encoding=\"utf8\") as f: \n",
    "    for line in f: \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "print(\"Found %s word vectors.\"%len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can leverage our embeddings_index dictionary and our word_to_index dictionary to create an embedding matrix that we can use to initialize our embedding layer. We will use the same dimensionality as our GloVe embeddings (50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target variable \n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_valid = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the text data\n",
    "X_train = [tokenizer(text) for text in train[\"Utterance\"]]\n",
    "y_train = train[\"Intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_df[\"Utterance\"], train_df[\"Intent\"], test_size=0.3,                                shuffle=True, stratify=train_df[\"Intent\"], random_state=SEED_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target variable \n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_valid = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io # for encoding\n",
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line.strip.split()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use Torch's tokenizer API \n",
    "# Train-test split of 95% train and 5% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for training\n",
    "# Change all of the following configurations as per the specifications in the original repo \n",
    "# Set a seed value \n",
    "seed_value = 12321 \n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "    \n",
    "class Config: \n",
    " \n",
    "    VOCAB_SIZE = 0\n",
    "    BATCH_SIZE = 512 \n",
    "    EMB_SIZE = 300 \n",
    "    OUT_SIZE = 2\n",
    "    NUM_FOLDS = 5 #  \n",
    "    NUM_EPOCHS = 10 \n",
    "    NUM_WORKERS = 8\n",
    "    \n",
    "# I want to update the pretrained embedding weights during training process \n",
    "# I want to use a pretrained embedding\n",
    "    EMB_WT_UPDATE = True\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False \n",
    "    PATIENCE = 6 \n",
    "    IS_BIDIRECTIONAL = True \n",
    "    \n",
    "    # Model hyperparameters\n",
    "    MODEL_PARAMS = {\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"drop_out\": 0.4258,\n",
    "        \"lr\": 0.000366,\n",
    "        \"weight_decay\": 0.00001\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset class for CSV/TSV files \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, vocab, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.data[idx][0]\n",
    "        text = self.data[idx][1]\n",
    "        tokens = self.tokenizer(text)[:self.max_length]\n",
    "        tokens = [self.vocab[token] for token in tokens]\n",
    "        return (torch.tensor(tokens), torch.tensor(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix \n",
    "def create_embedding_matrix(word_index, embedding_dict=None, dim=100): \n",
    "    num_words = len(word_index) + 1 # the word_index dictionary start from 1, not 0, since 0 is reserved for padding\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, idx in word_index.items(): \n",
    "        embedding_vector = embedding_dict.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and validation data\n",
    "def create_data(train_df, valid_df): \n",
    "    X_train = train_df[\"text\"].values\n",
    "    y_train = train_df[\"label\"].values\n",
    "    X_valid = valid_df[\"text\"].values\n",
    "    y_valid = valid_df[\"label\"].values\n",
    "    \n",
    "    ds_train = CustomDataset(X_train, tokenizer, vocab, max_length=100)\n",
    "    ds_valid = CustomDataset(X_valid, tokenizer, vocab, max_length=100)\n",
    "    \n",
    "    torch_train = DataLoader(ds_train, batch_size=CONFIG.batch_size, collate_fn = pad_collate, num_workers=Config.NUM_WORKERS, shuffle=True)\n",
    "    \n",
    "    torch_valid = DataLoader(ds_valid, batch_size=CONFIG.batch_size, collate_fn = pad_collate, num_workers=Config.NUM_WORKERS, shuffle=True)\n",
    "    \n",
    "    return torch_train, torch_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the Input Sequence.  If the goal is to train with mini-batches, one @ needs to pad the sequences in batch. \n",
    "# In other words, given a mini-batch of size N, if the length of the largest sequence is L, \n",
    "# one needs to pad every sequence with a length of smaller than L with zeros and make their \n",
    "# lengths equal to L. Moreover, it is important that the sequences in the batch are in the \n",
    "# descending order.\n",
    "\n",
    "from cProfile import label\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # Each element in the batch is a tuple (token_tensor, label) \n",
    "    # Sort the batch (based on word count) in descending order \n",
    "    \n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Also need to store the length of each sequence. This is later needed in order to unpad the sequences\n",
    "    seq_len = torch.Tensor([x[0].shape[0] for x in sorted_batch])\n",
    "    labels = torch.LongTensor([x[1] for x in sorted_batch]) \n",
    "    \n",
    "    return sequences_padded, seq_len, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the input data into a TensorDataset (see what other types of data are availabel as well)\n",
    "dataset = TensorDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture \n",
    "- Create a neural network in Torch for intent classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import lstm\n",
    "\n",
    "# Enhance the architecture later \n",
    "class IntentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim, output_dim, n_layers, dropout): \n",
    "        super().__init__() # In Python3, class specification ain't required in the super() call anymore \n",
    "        \n",
    "        # Emebdding layer with pretrained weights \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM layer \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=True, \n",
    "                            dropout=dropout)\n",
    "        \n",
    "        # Dense layers \n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 600)  # 2 for bidirectional \n",
    "        self.fc2 = nn.Linear(600, 600)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "        # Output layer \n",
    "        self.out = nn.Linear(600, output_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # text = [batch_size, embed_length]\n",
    "        \n",
    "        embeddings = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        # embedded = [batch_size, sent_length, emb_dim]\n",
    "        assert embeddings.shape == (inputs.shape[0], inputs.shape[1], self.embedding_dim)\n",
    "         \n",
    "        # pack_padded_sequence before feeding to the LSTM. This is required so PyTorch knows \n",
    "        # which elements of the sequence are padded and ignores them in the computation \n",
    "        # Accomplished only after the embedding step \n",
    "        embeds_pack = pack_padded_sequence(embeddings, inputs_lengths, batch_first=True)\n",
    "        \n",
    "        # Get the dimensions of the packed sequence \n",
    "        dimensions = embeds_pack.data.size()\n",
    "        _, (hidden, _) = self.lstm(embeds_pack)\n",
    "        # Ours task being a classification model, we are only interested in the final hidden state and not the LSTM output \n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        final_hidden_forward = hidden[-2,:,:] # [batch_size, hidden_dim]\n",
    "        final_hidden_backward = hidden[-1,:,:] # [bacth_size, hidden_dim]\n",
    "        \n",
    "        # Concat the final forward and hidden backward states \n",
    "        hidden = torch.cat((final_hidden_forward, final_hidden_backward), dim=1)\n",
    "                \n",
    "        # Dense Linear Layers \n",
    "        dense_outputs_1 = F.relu(self.fc1(hidden))\n",
    "        dense_outputs_2 = self.dropout(F.relu(self.fc2(dense_outputs_1)))\n",
    "        \n",
    "        # Final output classification layer\n",
    "        final_output = F.soft(self.out(dense_outputs_2))\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Instantiate the model\n",
    "intent_model = IntentClassifier(vocab_size=Config.VOCAB_SIZE, embedding_dim=Config.EMB_SIZE, embedding_matrix=embedding_matrix, hidden_dim=Config.MODEL_PARAMS[\"hidden_size\"], output_dim=Config.OUT_SIZE, n_layers=Config.MODEL_PARAMS[\"num_layers\"], dropout=Config.MODEL_PARAMS[\"drop_out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import lstm\n",
    "\n",
    "# Enhance the architecture later \n",
    "class IntentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim, output_dim, n_layers, dropout): \n",
    "        super().__init__() # In Python3, class specification ain't required in the super() call anymore \n",
    "        \n",
    "        # Emebdding layer with pretrained weights \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM layer \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=True, \n",
    "                            dropout=dropout)\n",
    "        \n",
    "        # Dense layers \n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 600)  # 2 for bidirectional \n",
    "        self.fc2 = nn.Linear(600, 600)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "        # Output layer \n",
    "        self.out = nn.Linear(600, output_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # text = [batch_size, embed_length]\n",
    "        \n",
    "        embeddings = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        # embedded = [batch_size, sent_length, emb_dim]\n",
    "        assert embeddings.shape == (inputs.shape[0], inputs.shape[1], self.embedding_dim)\n",
    "         \n",
    "        # pack_padded_sequence before feeding to the LSTM. This is required so PyTorch knows \n",
    "        # which elements of the sequence are padded and ignores them in the computation \n",
    "        # Accomplished only after the embedding step \n",
    "        embeds_pack = pack_padded_sequence(embeddings, inputs_lengths, batch_first=True)\n",
    "        \n",
    "        # Get the dimensions of the packed sequence \n",
    "        dimensions = embeds_pack.data.size()\n",
    "        _, (hidden, _) = self.lstm(embeds_pack)\n",
    "        # Ours task being a classification model, we are only interested in the final hidden state and not the LSTM output \n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        final_hidden_forward = hidden[-2,:,:] # [batch_size, hidden_dim]\n",
    "        final_hidden_backward = hidden[-1,:,:] # [bacth_size, hidden_dim]\n",
    "        \n",
    "        # Concat the final forward and hidden backward states \n",
    "        hidden = torch.cat((final_hidden_forward, final_hidden_backward), dim=1)\n",
    "                \n",
    "        # Dense Linear Layers \n",
    "        dense_outputs_1 = F.relu(self.fc1(hidden))\n",
    "        dense_outputs_2 = self.dropout(F.relu(self.fc2(dense_outputs_1)))\n",
    "        \n",
    "        # Final output classification layer\n",
    "        final_output = F.soft(self.out(dense_outputs_2))\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Instantiate the model\n",
    "intent_model = IntentClassifier(vocab_size=Config.VOCAB_SIZE, embedding_dim=Config.EMB_SIZE, embedding_matrix=embedding_matrix, hidden_dim=Config.MODEL_PARAMS[\"hidden_size\"], output_dim=Config.OUT_SIZE, n_layers=Config.MODEL_PARAMS[\"num_layers\"], dropout=Config.MODEL_PARAMS[\"drop_out\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-support",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
