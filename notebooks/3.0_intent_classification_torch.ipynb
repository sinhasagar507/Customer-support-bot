{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Classification With PyTorch\n",
    "Previously, my focus in the notebooks was on obtaining labeled data for my chatbot. However, this current notebook is centered around utilizing PyTorch for the classification of intents within fresh, unseen user-generated data. The model has transitioned to a supervised learning approach, leveraging the labels derived from the unsupervised learning conducted in the preceding notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RASA Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasa trains this intent classification step with SVM and GridsearchCV because they can try different configurations ([source](https://medium.com/bhavaniravi/intent-classification-demystifying-rasanlu-part-4-685fc02f5c1d)). When deploying preprocessing pipeline should remain same between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.2.2\n",
      "Numpy: 1.26.4\n",
      "Sklearn: 1.4.2\n",
      "Training data:                                                track  \\\n",
      "0                                  [no, information]   \n",
      "1  [issue, is, resolved, and, item, is, being, re...   \n",
      "2  [expected, delivery, date, is, th, october, tr...   \n",
      "3  [expected, delivery, date, is, th, october, tr...   \n",
      "4               [no, emails, no, reason, for, delay]   \n",
      "\n",
      "                                             support  \\\n",
      "0  [very, poor, feedback, very, disappointing, se...   \n",
      "1  [already, done, i, am, frankly, fed, up, with,...   \n",
      "2  [very, poor, feedback, very, disappointing, se...   \n",
      "3  [can, see, you, have, replied, to, others, who...   \n",
      "4  [my, issue, is, not, resolved, really, should,...   \n",
      "\n",
      "                                             quality  \\\n",
      "0   [done, attached, is, the, proof, of, completion]   \n",
      "1                 [return, pick, up, not, happening]   \n",
      "2  [your, target, completed, return, policy, expi...   \n",
      "3  [order, is, lost, no, one, taking, responsibil...   \n",
      "4  [tampered, supplied, defective, fake, failedde...   \n",
      "\n",
      "                                            discount  \\\n",
      "0                 [amazon, pay, transaction, amt, ₹]   \n",
      "1                                 [next, sale, date]   \n",
      "2          [mrp, on, product, rs, mrp, on, site, rs]   \n",
      "3  [but, flipkart, and, shop, clues, are, giving,...   \n",
      "4  [but, flipkart, and, shop, clues, are, giving,...   \n",
      "\n",
      "                                         account  \\\n",
      "0                      [email, account, details]   \n",
      "1                      [email, account, details]   \n",
      "2                      [email, account, details]   \n",
      "3  [the, credit, card, information, is, correct]   \n",
      "4                        [account, email, email]   \n",
      "\n",
      "                  speak_representative           greeting   goodbye  \\\n",
      "0                [talk, human, please]               [hi]   [goodb]   \n",
      "1  [let, me, talk, to, apple, support]            [hello]     [bye]   \n",
      "2       [can, i, speak, agent, person]        [whats, up]   [thank]   \n",
      "3                [talk, human, please]    [good, morning]  [thanks]   \n",
      "4  [let, me, talk, to, apple, support]  [good, afternoon]    [done]   \n",
      "\n",
      "     challenge_robot  \n",
      "0     [robot, human]  \n",
      "1  [are, you, robot]  \n",
      "2    [who, are, you]  \n",
      "3     [robot, human]  \n",
      "4  [are, you, robot]  \n"
     ]
    }
   ],
   "source": [
    "# Standard \n",
    "import collections\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "import numpy as np\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "\n",
    "# Machine Learning\n",
    "import sklearn\n",
    "print(f\"Sklearn: {sklearn.__version__}\")\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Preprocessing and Torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# from torchtext.data import get_tokenizer\n",
    "\n",
    "# Reading in training data\n",
    "train = pd.read_pickle('../objects/train.pkl')\n",
    "print(f'Training data: {train.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29f145710>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration for training\n",
    "# Change all of the following configurations as per the specifications in the original repo \n",
    "# Set a seed value \n",
    "seed_value = 12321 \n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.melt(train)\n",
    "train.columns = [\"intent\", \"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>challenge_robot</td>\n",
       "      <td>[who, are, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>challenge_robot</td>\n",
       "      <td>[are, you, robot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>support</td>\n",
       "      <td>[i, tried, to, find, a, customer, services, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speak_representative</td>\n",
       "      <td>[let, me, talk, to, apple, support]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>challenge_robot</td>\n",
       "      <td>[who, are, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>support</td>\n",
       "      <td>[really, disappointed, with, your, service, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>account</td>\n",
       "      <td>[does, amazon, no, longer, provide, refund, if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>account</td>\n",
       "      <td>[i, should, not, have, to, speak, to, now, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>account</td>\n",
       "      <td>[i, have, found, out, some, discrepancy, in, y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    intent                                             tokens\n",
       "0          challenge_robot                                    [who, are, you]\n",
       "1          challenge_robot                                  [are, you, robot]\n",
       "2                  support  [i, tried, to, find, a, customer, services, em...\n",
       "3     speak_representative                [let, me, talk, to, apple, support]\n",
       "4          challenge_robot                                    [who, are, you]\n",
       "...                    ...                                                ...\n",
       "8995               support  [really, disappointed, with, your, service, co...\n",
       "8996               account  [does, amazon, no, longer, provide, refund, if...\n",
       "8997               account  [i, should, not, have, to, speak, to, now, a, ...\n",
       "8998               goodbye                                            [thank]\n",
       "8999               account  [i, have, found, out, some, discrepancy, in, y...\n",
       "\n",
       "[9000 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df = train.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent    object\n",
      "tokens    object\n",
      "dtype: object\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Print the data types of the columns\n",
    "print(shuffled_df.dtypes)\n",
    "\n",
    "# Check the data types of each row in the \"tokens\" column and if its not a list, highlight the the error \n",
    "# Don't print it, log it \n",
    "print(\" \")\n",
    "for index, row in shuffled_df.iterrows():\n",
    "    if not isinstance(row[\"tokens\"], list):\n",
    "        print(f\"Error: {row['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [token_lst for token_lst in shuffled_df['tokens']]\n",
    "X = [*X]\n",
    "y = [*shuffled_df['intent'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchtext tokenizer \n",
    "- Add description later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of Action\n",
    "- Prepare the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Applications/saggydev/projects_learning/amazon_support/notebooks'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps taken\n",
    "    -   the words would involve creating a vocabulary dictionary to map words to indices \n",
    "    -   For each sequence, the words are converted into their corresponding indices based on the word dictionary \n",
    "    - When feeding sentences into the model, ensure a consistent sequence length is crucial \n",
    "    - To achieve this, sequences are padded with zeros until they reach the length of the longest sequence \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n",
    "    - This padding ensures uniformity, and shorter maximum lengths are typically preferred for ease of training, as longer sequences can pose challenges \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape checks:\n",
      "X_train: 6300 X_val: 2700\n",
      "y_train: 6300 y_val: 2700\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'train' is a DataFrame containing 'Utterance' and 'Intent' columns\n",
    "\n",
    "# Tokenize the text data using PyTorch's tokenizer\n",
    "# The text already seems to be tokenized \n",
    "\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, \n",
    "                                                  shuffle=True, stratify=y, random_state=7)\n",
    "\n",
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "\n",
    "# Convert encoded targets to PyTorch tensors\n",
    "y_train_encoded = torch.tensor(y_train_encoded) \n",
    "y_val_encoded = torch.tensor(y_val_encoded)\n",
    "\n",
    "print(f'\\nShape checks:\\nX_train: {len(X_train)} X_val: {len(X_val)}\\ny_train: {len(y_train_encoded)} y_val: {len(y_val_encoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I already have a list of tokens \n",
    "# Convert X_train and X_val back to a list of strings\n",
    "X_train = [' '.join(tokens) for tokens in X_train]\n",
    "X_val = [' '.join(tokens) for tokens in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2773"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "    \n",
    "class Config: \n",
    "    VOCAB_SIZE = 0\n",
    "    BATCH_SIZE = 512 \n",
    "    EMB_SIZE = 300 \n",
    "    OUT_SIZE = 2\n",
    "    NUM_FOLDS = 5 #  \n",
    "    NUM_EPOCHS = 10 \n",
    "    NUM_WORKERS = 8\n",
    "    \n",
    "# I want to update the pretrained embedding weights during training process \n",
    "# I want to use a pretrained embedding\n",
    "    EMB_WT_UPDATE = True\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False \n",
    "    PATIENCE = 6 \n",
    "    IS_BIDIRECTIONAL = True \n",
    "    \n",
    "    # Model hyperparameters\n",
    "    MODEL_PARAMS = {\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"drop_out\": 0.4258,\n",
    "        \"lr\": 0.000366,\n",
    "        \"weight_decay\": 0.00001\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import lstm\n",
    "\n",
    "# Enhance the architecture later \n",
    "class IntentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim, output_dim, n_layers, dropout): \n",
    "        super().__init__() # In Python3, class specification ain't required in the super() call anymore \n",
    "        \n",
    "        # Emebdding layer with pretrained weights \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM layer \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=True, \n",
    "                            dropout=dropout)\n",
    "        \n",
    "        # Dense layers \n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 600)  # 2 for bidirectional \n",
    "        self.fc2 = nn.Linear(600, 600)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "        # Output layer \n",
    "        self.out = nn.Linear(600, output_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # text = [batch_size, embed_length]\n",
    "        \n",
    "        # embeddings = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        # embedded = [batch_size, sent_length, emb_dim]\n",
    "        assert self.embeddings.shape == (inputs.shape[0], inputs.shape[1], self.embedding_dim)\n",
    "         \n",
    "        # pack_padded_sequence before feeding to the LSTM. This is required so PyTorch knows \n",
    "        # which elements of the sequence are padded and ignores them in the computation \n",
    "        # Accomplished only after the embedding step \n",
    "        # embeds_pack = pack_padded_sequence(embeddings, inputs_lengths, batch_first=True)\n",
    "        \n",
    "        # Get the dimensions of the packed sequence \n",
    "        # dimensions = embeds_pack.data.size()\n",
    "        _, (hidden, _) = self.lstm(self.embeddings)\n",
    "        \n",
    "        # Ours task being a classification model, we are only interested in the final hidden state and not the LSTM output \n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        final_hidden_forward = hidden[-2,:,:] # [batch_size, hidden_dim]\n",
    "        final_hidden_backward = hidden[-1,:,:] # [bacth_size, hidden_dim]\n",
    "        \n",
    "        # Concat the final forward and hidden backward states \n",
    "        hidden = torch.cat((final_hidden_forward, final_hidden_backward), dim=1)\n",
    "                \n",
    "        # Dense Linear Layers \n",
    "        dense_outputs_1 = nn.ReLU(self.fc1(hidden))\n",
    "        dense_outputs_2 = self.dropout(nn.ReLU(self.fc2(dense_outputs_1)))\n",
    "        \n",
    "        # Final output classification layer\n",
    "        final_output = nn.Softmax(self.out(dense_outputs_2))\n",
    "    \n",
    "        return final_output\n",
    "    \n",
    "# Execute the model \n",
    "class TrainingLoop: \n",
    "    def __init__(self, )\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "intent_model = IntentClassifier(vocab_size=Config.VOCAB_SIZE, embedding_dim=Config.EMB_SIZE, embedding_matrix=embedding_matrix, hidden_dim=Config.MODEL_PARAMS[\"hidden_size\"], output_dim=Config.OUT_SIZE, n_layers=Config.MODEL_PARAMS[\"num_layers\"], dropout=Config.MODEL_PARAMS[\"drop_out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, criterion, optimizer, epochs=100):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.valid_loss_min = np.Inf\n",
    "        self.epoch_lst = []\n",
    "        self.train_loss_lst = []\n",
    "        self.valid_loss_lst = []\n",
    "        self.train_acc_lst = []\n",
    "        self.valid_acc_lst = []\n",
    "\n",
    "\n",
    "        # train_loader, valid_loader will be created inside of the class \n",
    "        # criterion will also be declared internally itself \n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            for data, target in self.train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                pred_labels = torch.argmax(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (pred_labels == target).sum().item()\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            train_loss /= len(self.train_loader.dataset)\n",
    "            train_accuracy = 100 * correct / total\n",
    "            self.train_loss_lst.append(train_loss)\n",
    "            self.train_acc_lst.append(train_accuracy)\n",
    "\n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in self.valid_loader:\n",
    "                    output = self.model(data)\n",
    "                    pred_labels = torch.argmax(output, 1)\n",
    "                    total += target.size(0)\n",
    "                    correct += (pred_labels == target).sum().item()\n",
    "                    loss = self.criterion(output, target)\n",
    "                    valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "            valid_loss /= len(self.valid_loader.dataset)\n",
    "            valid_accuracy = 100 * correct / total\n",
    "            self.valid_loss_lst.append(valid_loss)\n",
    "            self.valid_acc_lst.append(valid_accuracy)\n",
    "\n",
    "            print(f\"Epoch: {epoch+1}/{self.epochs}.. Training Loss: {train_loss:.3f}.. Validation Loss: {valid_loss:.3f}.. Validation Accuracy: {valid_accuracy:.3f}%\")\n",
    "\n",
    "            if valid_loss <= self.valid_loss_min:\n",
    "                print(f\"Validation loss decreased ({self.valid_loss_min:.3f} --> {valid_loss:.3f}). Saving model...\")\n",
    "                torch.save(self.model.state_dict(), \"models/intent_classification_model.pt\")\n",
    "                self.valid_loss_min = valid_loss\n",
    "\n",
    "            self.epoch_lst.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=5000)\n",
    "train_features = vectorizer.fit_transform(X_train)\n",
    "val_features = vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2700x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 34073 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Write a function to average all of the word vectors in a given paragraph\n",
    "def get_sentence_vector(sentence, word_vectors):\n",
    "    \n",
    "    # Initialize the vector as all zeros\n",
    "    vector = np.zeros(word_vectors.vector_size)\n",
    "    \n",
    "    num_words = 0\n",
    "\n",
    "    # Loop over each word in the sentence\n",
    "    for word in sentence:\n",
    "        if word in word_vectors:\n",
    "            vector += word_vectors[word]\n",
    "            num_words += 1\n",
    "\n",
    "    # Average the vector\n",
    "    if num_words:\n",
    "        vector /= num_words\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocab mapping for words \n",
    "word_to_idx = {word: index+1 for index, (word, _) in enumerate(t.word_index.items())}\n",
    "index_to_word = {index+1: word for index, (word, _) in enumerate(t.word_index.items())}\n",
    "\n",
    "# Convert documents to sequences of indices \n",
    "indexed_X_train = [[word_to_idx[word] for word in doc] for doc in X_train]\n",
    "indexed_X_val = [[word_to_idx[word] for word in doc] for doc in X_val]\n",
    "\n",
    "# Pad sequences to a common length \n",
    "max_length = min(max(len(doc) for doc in indexed_X_train), 100)\n",
    "padded_X_train = pad_sequence([torch.tensor(doc[ :max_length]) + [0]*(max_length - len(doc) for doc in indexed_X_train], batch_first=True)\n",
    "padded_X_val = pad_sequence([torch.tensor(doc[: max_length] + [0] * (max_length - len(doc)) for doc in indexed_X_val], batch_first=True)\n",
    "                            \n",
    "# Define vocabulary size \n",
    "vocab_size = len(word_to_index) + 1 \n",
    "\n",
    "print(f\"Vocab size:\\n{vocab_size}\")\n",
    "print(f\"Max length:\\n{max_length}\")\n",
    "\n",
    "\n",
    "print(f\"padded_X_train\\n{padded_X_train}\")\n",
    "print(f\"padded_X_val\\n{padded_X_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example fits the Tokenizer with 5 small documents. The details of the fit Tokenizer are printed. Then the 5 documents are encoded using a word count \n",
    "\n",
    "Each document is encoded as a 9-element vector with one position for each word and the chosen encoding scheme value for each word position. In this case, a simple word count is used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models involve dealing with one-hot encoding for multiclass classification and using embeddings for document representations. Below are PyTorch specific of the provided statements\n",
    "\n",
    "If we are using Doc2Vec embeddings, how will we pass in our tweets? We may have to pass it in as full tweets. Check how we pass in the tweets. We may have to perform tokenization at a tweet level. If we pass it in, if it's Twweet 57, it will activate the node such that it gets multiplied out by the embeddings for the 57th document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are 4 different dimensionality options \n",
    "!ls models/glove.twitter.27B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we compute an index mapping of words to known embeddings by parsing the data dump of pre-trained embeddings \n",
    "- I use 50D because my X_train has a max_length of 32 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just include `weights and biases tracking` as a part of training mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Glove Embeddings \n",
    "embeddings_index = {} \n",
    "\n",
    "with open(\"models/glove.twitter.27B/glove.twitter.27B.50d.txt\", encoding=\"utf8\") as f: \n",
    "    for line in f: \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "print(\"Found %s word vectors.\"%len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can leverage our embeddings_index dictionary and our word_to_index dictionary to create an embedding matrix that we can use to initialize our embedding layer. We will use the same dimensionality as our GloVe embeddings (50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing required objects \n",
    "word_index = t.word_index \n",
    "EMBEDDING_DIM = 50 # Because we are using the 50D glove embeddings \n",
    "\n",
    "# Getting my embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, idx in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all zeros\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape, embeddings_index.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can start the modeling \n",
    "\n",
    "In a regular word embedding, the order of the embeddings in the matrix has to be setup so that it matches how the words \n",
    "\n",
    "I also made sure the order of the embeddings are the same order of the words as they are in the model \n",
    "\n",
    "Here, I also made sure that domain-specific words like customer support is in my Twitter embeddings. One example of this is 'customer' and you can clearly see that it is in indeed in my embeddings file, which is cool \n",
    "\n",
    "<img src=\"visualizations/amazon.png\" alt=\"Drawing\" style=\"width: 400px; \"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target variable \n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_valid = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the text data\n",
    "X_train = [tokenizer(text) for text in train[\"Utterance\"]]\n",
    "y_train = train[\"Intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df[\"Utterance\"], train_df[\"Intent\"], test_size=0.3,    shuffle=True, stratify=train_df[\"Intent\"], random_state=SEED_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build train and test dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch datasets from train and test sets \n",
    "train = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "valid = torch.utils.data.TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architect the Neural Network  \n",
    "- I will create a neural network with PyTorch with the output layer having the same no. of nodes as they are intents. The following is the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassificationModel(nn.Module): \n",
    "    def __init__(sel): \n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "\n",
    "        # Bidirectional LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, bidirectional=True)\n",
    "\n",
    "        # Dense layers\n",
    "        self.dense1 = nn.Linear(hidden_size * 2, dense_size)\n",
    "        self.dense2 = nn.Linear(dense_size, dense_size)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(dense_size, num_intents)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        embedded = self.embedding(input_data)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = torch.cat((lstm_out[:, -1, :hidden_size], lstm_out[:, 0, hidden_size:]), dim=1)\n",
    "        dense1_out = nn.functional.relu(self.dense1(lstm_out))\n",
    "        dense2_out = nn.functional.relu(self.dense2(dense1_out))\n",
    "        dropped_out = self.dropout(dense2_out)\n",
    "        output = nn.functional.softmax(self.output_layer(dropped_out), dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat the model with 32 as the max token length \n",
    "model = IntentClassificationModel(vocab_size, torch.Tensor(embedding_matrix), hidden_state=128, dense_size=600, num_intents=10, dropout_rate=0.5)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 \n",
    "\n",
    "# Initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # Set initial \"min\" to infinity\n",
    "\n",
    "# Some lists to keep track of loss and accuracy during each epoch\n",
    "epoch_lst = []\n",
    "train_loss_lst = []\n",
    "valid_loss_lst = []\n",
    "train_acc_lst = []\n",
    "valid_acc_lst = []\n",
    "\n",
    "# Start looping through epochs\n",
    "for epoch in range(epochs): \n",
    "    \n",
    "    # Monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # Train the model \n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Load train images with labels(targets)\n",
    "    for data, target in train_loader: \n",
    "        \n",
    "        # Clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        pred_labels = torch.argmax(output, 1)\n",
    "        \n",
    "        # Compute total no. of labels \n",
    "        total += len(target)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        correct += (pred_labels == target).sum()\n",
    "        \n",
    "        # Calculate the loss \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        # Calculate average training loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        \n",
    "    # Average accuracy \n",
    "    accuracy = 100 * correct/float(total)\n",
    "    \n",
    "    # Append them to a list for plotting and printing purposes\n",
    "    train_loss_lst.append(train_loss)\n",
    "    train_acc_lst.append(accuracy)\n",
    "    \n",
    "    \n",
    "    # Set the model to evaluation mode \n",
    "    model.eval()\n",
    "    \n",
    "    # Caclulate validation accuracy\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            pred_labels = torch.argmax(output, 1)\n",
    "            \n",
    "            # Compute total no. of labels \n",
    "            total += len(target)\n",
    "            \n",
    "            # Total correct predictions\n",
    "            correct += (pred_labels == target).sum()\n",
    "            \n",
    "            # Calculate the loss \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Update running validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "            # Calculate average validation loss over an epoch\n",
    "            valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "            \n",
    "            # Total no. of labels\n",
    "            total += len(target)    \n",
    "            \n",
    "            # Total correct predictions\n",
    "            correct += (pred_labels == target).sum()\n",
    "            \n",
    "# Calculate average validaton loss and accuracy over an epoch\n",
    "val_loss = val_loss / len(valid_loader.dataset)\n",
    "accuracy = 100 * correct/float(total)\n",
    "\n",
    "# Put them in their list \n",
    "valid_acc_lst.append(accuracy)\n",
    "val_loss_lst.append(val_loss)      \n",
    "\n",
    "# Print the epoch and training loss details with validation accuracy \n",
    "print(f\"Epoch: {epoch+1}/{epochs}.. Training loss: {train_loss:.3f}.. Validation Loss: {val_loss:.3f}.. Validation Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Save the model if validation loss has decreased  \n",
    "if val_loss <= valid_loss_min: \n",
    "    print(f\"Validation loss decreased ({valid_loss_min:.3f} --> {val_loss:.3f}). Saving model...\")\n",
    "    torch.save(model.state_dict(), \"models/intent_classification_model.pt\")\n",
    "    valid_loss_min = val_loss\n",
    "# Move to next epoch \n",
    "epoch_lst.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the model with the lowest validation loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"models/intent_classification_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training loss vs validation loss (the loss is how bad the model is during training)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss_lst, label=\"Training Loss\", color=\"cyan\")\n",
    "plt.plot(valid_loss_lst, label=\"Validation Loss\", color=\"magenta\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize training accuracy vs validation accuracy \n",
    "plt.figure(figsize=(10, 7)) \n",
    "plt.plot(train_acc_lst, label=\"Training Accuracy\", color=\"magenta\")   \n",
    "plt.plot(valid_acc_lst, label=\"Validation Accuracy\", color=\"cyan\")\n",
    "plt.title(\"Training accuracy vs validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At after 20 epochs, the slope becomes a flat line, not really much change in the loss. Floor effect is you can't get any lower than 0 loss. It really quickly learns from the training data what it needs to learn. If you continue to train, you 're basically overfitting to the training data, you are fitting to the unimportant signal. \n",
    "\n",
    "For example, in the context of images, if the model learns to recognize what a cat is, it might now be too detailed and learn that cats have to be the color black as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model improvements\n",
    "The model overfits at a low epoch. Model is significantly overfitting. Plot out accuracies.\n",
    "\n",
    "Don't need 100 training epoch.\n",
    "\n",
    "Look at learning rate scheduling, after certain number of epochs, decrease learning rate.\n",
    "* Learning rate scheduling\n",
    "* Early stopping or reducing epochs\n",
    "* Dropout layers\n",
    "* Regularization\n",
    "* Improve distinctiveness between intent data\n",
    "\n",
    "After I have applied these improvements, my accuracy went up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_intent(user_input): \n",
    "    # Tokenize the input \n",
    "    user_input = tokenizer(user_input)\n",
    "    \n",
    "    # Convert the input to indices \n",
    "    indexed_input = [word_to_idx[word] for word in user_input]\n",
    "    \n",
    "    # Pad the input \n",
    "    padded_input = pad_sequence([torch.tensor(indexed_input)], batch_first=True)\n",
    "    \n",
    "    # Make predictions \n",
    "    output = model(padded_input)\n",
    "    \n",
    "    # Get the predicted label \n",
    "    pred_label = torch.argmax(output, 1)\n",
    "    \n",
    "    # Return the predicted label \n",
    "    return le.inverse_transform(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing checkpoint settings to view progress and save model \n",
    "filename = 'models/intent_classification_model.h5'\n",
    "\n",
    "# Learning Rate Scheduling \n",
    "# This function keeps the initial learning rate for the first ten epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target variable \n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_valid = le.transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io # for encoding\n",
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line.strip.split()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use Torch's tokenizer API \n",
    "# Train-test split of 95% train and 5% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "    \n",
    "class Config: \n",
    " \n",
    "    VOCAB_SIZE = 0\n",
    "    BATCH_SIZE = 512 \n",
    "    EMB_SIZE = 300 \n",
    "    OUT_SIZE = 2\n",
    "    NUM_FOLDS = 5 #  \n",
    "    NUM_EPOCHS = 10 \n",
    "    NUM_WORKERS = 8\n",
    "    \n",
    "# I want to update the pretrained embedding weights during training process \n",
    "# I want to use a pretrained embedding\n",
    "    EMB_WT_UPDATE = True\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False \n",
    "    PATIENCE = 6 \n",
    "    IS_BIDIRECTIONAL = True \n",
    "    \n",
    "    # Model hyperparameters\n",
    "    MODEL_PARAMS = {\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"drop_out\": 0.4258,\n",
    "        \"lr\": 0.000366,\n",
    "        \"weight_decay\": 0.00001\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset class for CSV/TSV files \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, vocab, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.data[idx][0]\n",
    "        text = self.data[idx][1]\n",
    "        tokens = self.tokenizer(text)[:self.max_length]\n",
    "        tokens = [self.vocab[token] for token in tokens]\n",
    "        return (torch.tensor(tokens), torch.tensor(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix \n",
    "def create_embedding_matrix(word_index, embedding_dict=None, dim=100): \n",
    "    num_words = len(word_index) + 1 # the word_index dictionary start from 1, not 0, since 0 is reserved for padding\n",
    "    embedding_matrix = np.zeros((num_words, dim))\n",
    "    for word, idx in word_index.items(): \n",
    "        embedding_vector = embedding_dict.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and validation data\n",
    "def create_data(train_df, valid_df): \n",
    "    X_train = train_df[\"text\"].values\n",
    "    y_train = train_df[\"label\"].values\n",
    "    X_valid = valid_df[\"text\"].values\n",
    "    y_valid = valid_df[\"label\"].values\n",
    "    \n",
    "    ds_train = CustomDataset(X_train, tokenizer, vocab, max_length=100)\n",
    "    ds_valid = CustomDataset(X_valid, tokenizer, vocab, max_length=100)\n",
    "    \n",
    "    torch_train = DataLoader(ds_train, batch_size=CONFIG.batch_size, collate_fn = pad_collate, num_workers=Config.NUM_WORKERS, shuffle=True)\n",
    "    \n",
    "    torch_valid = DataLoader(ds_valid, batch_size=CONFIG.batch_size, collate_fn = pad_collate, num_workers=Config.NUM_WORKERS, shuffle=True)\n",
    "    \n",
    "    return torch_train, torch_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the Input Sequence.  If the goal is to train with mini-batches, one @ needs to pad the sequences in batch. \n",
    "# In other words, given a mini-batch of size N, if the length of the largest sequence is L, \n",
    "# one needs to pad every sequence with a length of smaller than L with zeros and make their \n",
    "# lengths equal to L. Moreover, it is important that the sequences in the batch are in the \n",
    "# descending order.\n",
    "\n",
    "from cProfile import label\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # Each element in the batch is a tuple (token_tensor, label) \n",
    "    # Sort the batch (based on word count) in descending order \n",
    "    \n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Also need to store the length of each sequence. This is later needed in order to unpad the sequences\n",
    "    seq_len = torch.Tensor([x[0].shape[0] for x in sorted_batch])\n",
    "    labels = torch.LongTensor([x[1] for x in sorted_batch]) \n",
    "    \n",
    "    return sequences_padded, seq_len, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the input data into a TensorDataset (see what other types of data are availabel as well)\n",
    "dataset = TensorDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture \n",
    "- Create a neural network in Torch for intent classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lstm\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Enhance the architecture later \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIntentClassifier\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import lstm\n",
    "\n",
    "# Enhance the architecture later \n",
    "class IntentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim, output_dim, n_layers, dropout): \n",
    "        super().__init__() # In Python3, class specification ain't required in the super() call anymore \n",
    "        \n",
    "        # Emebdding layer with pretrained weights \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM layer \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=True, \n",
    "                            dropout=dropout)\n",
    "        \n",
    "        # Dense layers \n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 600)  # 2 for bidirectional \n",
    "        self.fc2 = nn.Linear(600, 600)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "        # Output layer \n",
    "        self.out = nn.Linear(600, output_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # text = [batch_size, embed_length]\n",
    "        \n",
    "        embeddings = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        # embedded = [batch_size, sent_length, emb_dim]\n",
    "        assert embeddings.shape == (inputs.shape[0], inputs.shape[1], self.embedding_dim)\n",
    "         \n",
    "        # pack_padded_sequence before feeding to the LSTM. This is required so PyTorch knows \n",
    "        # which elements of the sequence are padded and ignores them in the computation \n",
    "        # Accomplished only after the embedding step \n",
    "        embeds_pack = pack_padded_sequence(embeddings, inputs_lengths, batch_first=True)\n",
    "        \n",
    "        # Get the dimensions of the packed sequence \n",
    "        dimensions = embeds_pack.data.size()\n",
    "        _, (hidden, _) = self.lstm(embeds_pack)\n",
    "        # Ours task being a classification model, we are only interested in the final hidden state and not the LSTM output \n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        final_hidden_forward = hidden[-2,:,:] # [batch_size, hidden_dim]\n",
    "        final_hidden_backward = hidden[-1,:,:] # [bacth_size, hidden_dim]\n",
    "        \n",
    "        # Concat the final forward and hidden backward states \n",
    "        hidden = torch.cat((final_hidden_forward, final_hidden_backward), dim=1)\n",
    "                \n",
    "        # Dense Linear Layers \n",
    "        dense_outputs_1 = F.relu(self.fc1(hidden))\n",
    "        dense_outputs_2 = self.dropout(F.relu(self.fc2(dense_outputs_1)))\n",
    "        \n",
    "        # Final output classification layer\n",
    "        final_output = F.soft(self.out(dense_outputs_2))\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Instantiate the model\n",
    "intent_model = IntentClassifier(vocab_size=Config.VOCAB_SIZE, embedding_dim=Config.EMB_SIZE, embedding_matrix=embedding_matrix, hidden_dim=Config.MODEL_PARAMS[\"hidden_size\"], output_dim=Config.OUT_SIZE, n_layers=Config.MODEL_PARAMS[\"num_layers\"], dropout=Config.MODEL_PARAMS[\"drop_out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import lstm\n",
    "\n",
    "# Enhance the architecture later \n",
    "class IntentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim, output_dim, n_layers, dropout): \n",
    "        super().__init__() # In Python3, class specification ain't required in the super() call anymore \n",
    "        \n",
    "        # Emebdding layer with pretrained weights \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM layer \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=True, \n",
    "                            dropout=dropout)\n",
    "        \n",
    "        # Dense layers \n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 600)  # 2 for bidirectional \n",
    "        self.fc2 = nn.Linear(600, 600)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "        # Output layer \n",
    "        self.out = nn.Linear(600, output_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # text = [batch_size, embed_length]\n",
    "        \n",
    "        embeddings = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        # embedded = [batch_size, sent_length, emb_dim]\n",
    "        assert embeddings.shape == (inputs.shape[0], inputs.shape[1], self.embedding_dim)\n",
    "         \n",
    "        # pack_padded_sequence before feeding to the LSTM. This is required so PyTorch knows \n",
    "        # which elements of the sequence are padded and ignores them in the computation \n",
    "        # Accomplished only after the embedding step \n",
    "        embeds_pack = pack_padded_sequence(embeddings, inputs_lengths, batch_first=True)\n",
    "        \n",
    "        # Get the dimensions of the packed sequence \n",
    "        dimensions = embeds_pack.data.size()\n",
    "        _, (hidden, _) = self.lstm(embeds_pack)\n",
    "        # Ours task being a classification model, we are only interested in the final hidden state and not the LSTM output \n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        final_hidden_forward = hidden[-2,:,:] # [batch_size, hidden_dim]\n",
    "        final_hidden_backward = hidden[-1,:,:] # [bacth_size, hidden_dim]\n",
    "        \n",
    "        # Concat the final forward and hidden backward states \n",
    "        hidden = torch.cat((final_hidden_forward, final_hidden_backward), dim=1)\n",
    "                \n",
    "        # Dense Linear Layers \n",
    "        dense_outputs_1 = F.relu(self.fc1(hidden))\n",
    "        dense_outputs_2 = self.dropout(F.relu(self.fc2(dense_outputs_1)))\n",
    "        \n",
    "        # Final output classification layer\n",
    "        final_output = F.soft(self.out(dense_outputs_2))\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Instantiate the model\n",
    "intent_model = IntentClassifier(vocab_size=Config.VOCAB_SIZE, embedding_dim=Config.EMB_SIZE, embedding_matrix=embedding_matrix, hidden_dim=Config.MODEL_PARAMS[\"hidden_size\"], output_dim=Config.OUT_SIZE, n_layers=Config.MODEL_PARAMS[\"num_layers\"], dropout=Config.MODEL_PARAMS[\"drop_out\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon_support",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
