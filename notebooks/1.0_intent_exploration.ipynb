{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization\n",
    "The easiest way to represent a document as a vector is with a **bag of words Count Vectorizer**. This will turn each document to be a 1-D array, which I think is a good starting point for my clustering algorithms to work. Let's see how it works. \n",
    "\n",
    "The Count Vectorizer exclusively accepts series in string format, not as tokenized lists. Each row, representing a document, must be a string. This ensures that every point in the clustering encapsulates an entire sequence, rather than isolated vectorized words.\n",
    "\n",
    "I set the `min_df` parameter to be equal to 5 to only include terms that occur more than 5 times in my **count vectorized** data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpkl\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Development environment passes all tests!\n"
     ]
    }
   ],
   "source": [
    "# %load C:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\test_environment.py\n",
    "import sys\n",
    "import numpy as np \n",
    "\n",
    "REQUIRED_PYTHON = \"python3\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    system_major = sys.version_info.major\n",
    "    if REQUIRED_PYTHON == \"python\":\n",
    "        required_major = 2\n",
    "    elif REQUIRED_PYTHON == \"python3\":\n",
    "        required_major = 3\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized python interpreter: {}\".format(\n",
    "            REQUIRED_PYTHON))\n",
    "\n",
    "    if system_major != required_major:\n",
    "        raise TypeError(\n",
    "            \"This project requires Python {}. Found: Python {}\".format(\n",
    "                required_major, sys.version))\n",
    "    else:\n",
    "        print(\">>> Development environment passes all tests!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3\n"
     ]
    }
   ],
   "source": [
    "print(REQUIRED_PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtest_environment\u001b[39;00m \u001b[39mimport\u001b[39;00m REQUIRED_PYTHON\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(corpus)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Get the feature names\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m feature_names \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mget_feature_names()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Print the count vectorized representation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(X\u001b[39m.\u001b[39mtoarray())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Print the count vectorized representation\n",
    "print(X.toarray())\n",
    "\n",
    "# Print the feature names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Vectorizing the data with Count Vectorizer \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bag_of_words \u001b[39m=\u001b[39m CountVectorizer(min_df\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Vectorizing the data with Count Vectorizer \n",
    "bag_of_words = CountVectorizer(min_df=5)\n",
    "inbound_cv = bag_of_words.fit_transform(string_processed_inbound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, ngram_range = (1,3))\n",
    "# Storing tfidf data and transforming them into sparse matrices\n",
    "inbound_tfidf = tfidf.fit_transform(string_processed_inbound)\n",
    "inbound_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained word embeddings\n",
    "While bag-of-words approaches can capture the presence of words in a text, they fail to preserve the crucial aspect of word order. This limitation necessitates the exploration of more sophisticated text vectorization techniques. With the advent of advanced methods, we can now effectively encode intent clusters into meaningful vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Glove "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll explore various word embeddings to encode text differently and assess their effectiveness, beginning with GloVe word embeddings—an unsupervised learning algorithm for generating word vector representations.\n",
    "\n",
    "Gensim simplifies the use of pretrained word embeddings, offering a specialized data format for easy loading into a numpy array.\n",
    "\n",
    "Note: I won't utilize this method because my clustering algorithms require tweets to represent single points, while this method transforms individual words. I'm keeping this note in my notebook to track progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.12 s\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"C:\\\\Sagar Study\\\\ML and Learning\\\\Projects\\\\customer-support-bot\\\\amazon_customer_support\\\\objects\\\\glove.6B.50d.txt\", \"r\", encoding=\"utf-8\") as file: \n",
    "    word_to_vec_map = {} \n",
    "    for line in file: \n",
    "        values = line.split() \n",
    "        curr_word = values[0] \n",
    "        coefs = np.array(values[1:], dtype=np.float32)  # Fix: Convert values to float\n",
    "        word_to_vec_map[curr_word] = coefs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70153 , -0.43853 ,  1.0509  , -0.33431 ,  0.67151 , -0.17677 ,\n",
       "        0.80079 ,  0.90158 , -0.29513 , -0.65586 , -0.083404,  0.35146 ,\n",
       "       -0.41071 ,  0.29446 , -1.1955  , -0.45611 ,  0.56877 ,  0.073522,\n",
       "       -1.2616  ,  0.22276 , -0.57735 ,  0.12075 ,  0.54712 , -0.34094 ,\n",
       "        0.2164  , -1.804   , -0.70362 , -0.56337 ,  1.8773  ,  0.1301  ,\n",
       "        2.271   , -0.25882 , -0.46309 , -0.7759  , -0.22926 ,  0.62156 ,\n",
       "       -0.043353, -0.60943 , -1.6791  , -0.018271,  0.53893 , -0.50689 ,\n",
       "        0.88454 , -0.11158 ,  0.57013 , -0.69098 , -0.43072 , -0.45332 ,\n",
       "       -0.27984 , -0.056133], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map[\"enemy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sea'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m word_to_vec_map[\u001b[39m\"\u001b[39;49m\u001b[39mSea\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sea'"
     ]
    }
   ],
   "source": [
    "word_to_vec_map[\"Sea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for interpretation later on, since this being the main embedding algorithm in my pipeline, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words_to_vec_map[\"sea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Doc2Vec\n",
    "Since this is the main embedding method I will use for my pipeline, I display how I capitalized this embedding in the next notebook. {to be edited later on}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hugging Face\n",
    "This is a startup that does a lot with NLP. I explore their encoders.\n",
    "\n",
    "BERT wouldn't really be a good option because a large part of that was trained with Wikipedia data.\n",
    "\n",
    "I am not sure what doc2vec is trained on, I think my results will be better if I find a Twitter based word embedding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fast-text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding methodology developed by Facebook AI team\n",
    "\n",
    "This one as well, like BERT, doesn't seem to be a good option at first glance due to its dependence on Wikipedia for training. \n",
    "\n",
    "Nevertheless, we will try and evaluate if it works well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we cluster, let's make scaled versions of our dataset first, which would be good for distance-based clustering methods. In general, these vectors shouldn't really need scaling, but it may help for computational purposes. I only do this for my count vectorized and tfidf vectorized data, not ones with the more meaningful word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and transforming to create standard scald versions of the data through the use of MaxAbsScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "inbound_cv = scaler.fit_transform(inbound_cv)\n",
    "outbound_cv = scaler.fit_transform(outbound_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbound_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbound_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Collection With Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Consideration: Because I will be working with neural networks in the next phase of my project, my objective is to identify approaximately `x` distinct intents in the data through Exploratory Data Analysis(EDA). Each of these should ideally consist of 1000 tweets similar to it to generate your training data for that intent with Gensim's `model.docvecs...` (complete next) function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aim to employ clustering methods and topic modeling techniques on my dataset to extract prominent themes, intending to manually assign labels to these clusters.\n",
    "\n",
    "Achieving successful outcomes will be more feasible if my dataset pertains uniformly to the domain of customer service. I've taken care to ensure that this Twitter data related to Amazon primarily focuses on this domain. This careful curation aims to enable my upcoming model to capture the subtleties required for intent classification. In essence, the conversational efficacy of the bot largely depends on the language it's been trained on.\n",
    "\n",
    "With respect to the nature of clustering algorithms, I understand that they may not precisely group the data as anticipated beforehand. It's an algorithmic process and might not seamlessly cluster intents together as desired. However, I will back my curiosity to discover insightful trends - if found any!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>1. K Means</color>\n",
    "My first approach for the clustering my word vectors is K-Means, which tends to perform well on blobs.\n",
    "\n",
    "A drawback is that it is very slow, and picking the value for K is hard - I don't even know how many intents there are in the data. This is why I start with larger jumps of K to get a higher level idea of which performs the best, then I dive deeper to finally decide what K works the best for finding the optimal number of intents in my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. K-Means for my TFIDF and Count Vectorized Data\n",
    "First I cluster the TFIDF and Count Vectorized data. Honestly, I won't really be expecting good results from it, so I won't spend a lot of effort doing this actual clustering on these two. But it's worth a shot to demonstrate an older and suboptimal approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I'm currently running the K-Means algorithm on my entire dataset. Alongside this, I'm performing hyperparameter optimization specifically on the n_clusters parameter. The initial progress bar reflects its progress with the dataset, while the second bar shows its completion concerning different values of n_clusters.\n",
    "\n",
    "The process of applying K-Means across 10 iterations spanning from 10 to 100 for both data types consumed approximately three hours before the scaling step. Thankfully, post-scaling, the training procedure noticeably sped up.\n",
    "\n",
    "Below this section, I've saved my results using Python's serialization package called Pickle to avoid the need to rerun the process again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# My grand dictionaries that will store all my results\n",
    "wcss_grand = {}\n",
    "labels_grand = {}\n",
    "silhouette_scores_grand = {}\n",
    "n_clusters = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "# Iterating through all the differently embedded data\n",
    "for i,j in tqdm(enumerate(vectorized_data.items())): \n",
    "    name = j[0] # Here j[0] is the name of the dataset\n",
    "    dataset = j[1] # And j[1] is the actual data\n",
    "    \n",
    "    # I store my metrics at these following lists\n",
    "    wcss = []\n",
    "    labels = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    # Looping through values of k\n",
    "    for k in tqdm(n_clusters):    \n",
    "        print(f'Currently fitting {name} with {k} clusters... Please wait')\n",
    "        \n",
    "        # Initializing with k-means++ ensures that you get don’t fall into the random initialization trap.\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state = 10)\n",
    "        kmeans.fit(dataset)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "        # Getting the silhouette score\n",
    "        labels.append(kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_score(dataset, kmeans.labels_))\n",
    "        \n",
    "        # Saving the models\n",
    "        filename = f'models/kmeans/{name}-{k}neighbors.sav'\n",
    "        joblib.dump(kmeans, filename)\n",
    "        \n",
    "    # Updating grand dictionary\n",
    "    wcss_grand[name + '_wcss'] = wcss\n",
    "    labels_grand[name + '_labels'] = labels\n",
    "    silhouette_scores_grand[name + '_silhouettes'] = silhouette_scores\n",
    "\n",
    "# Saving all my results\n",
    "with open('objects/wcss_grand.pkl', 'wb') as handle:\n",
    "    pickle.dump(wcss_grand, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('objects/labels_grand.pkl', 'wb') as handle:\n",
    "    pickle.dump(labels_grand, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('objects/silhouette_scores_grand.pkl', 'wb') as handle:\n",
    "    pickle.dump(silhouette_scores_grand, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading back in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing it into objects I can use in this notebook\n",
    "\n",
    "with open('objects/wcss_grand.pkl', 'rb') as handle: # Change path \n",
    "    wcss_grand = pkl.load(handle)\n",
    "with open('objects/labels_grand.pkl','rb') as handle: # Cnange path\n",
    "    labels_grand = pkl.load(handle)\n",
    "with open('objects/silhouette_scores_grand.pkl','rb') as handle: # Change path \n",
    "    silhouette_scores_grand = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Best K-Means Model\n",
    "I will try creating an elbow plot to see if there exists a clear elbow. I will do so on both the **count vectorized** and **tfidf** features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Plot count vectorized\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), wcss_grand['inbound_cv_ma_wcss'], color = 'magenta')\n",
    "plt.title('Elbow Method (Count Vectorized)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Elbow Plot tfidf\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), wcss_grand['inbound_tfidf_ma_wcss'], color = 'magenta')\n",
    "plt.title('Elbow Method (TFIDF)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silouette Plot count vectorized\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), silhouette_scores_grand['inbound_cv_ma_silhouettes'], color = 'red')\n",
    "plt.title('Silhouette Method (Count Vectorized)')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silouette Plot tfidf\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), silhouette_scores_grand['inbound_tfidf_ma_silhouettes'], color = 'red')\n",
    "plt.title('Silhouette Method (TFIDF)')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these plots don't really chaneg much between the TFIDF and Count Vectorized data, furthering my statement above that they aren't really going to be the most useful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing my clusters with t-SNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I try different color maps and choose one so its easier to distinguish between clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-support",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
