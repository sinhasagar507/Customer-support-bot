{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization\n",
    "The easiest way to represent a document as a vector is with a **bag of words Count Vectorizer**. This will turn each document to be a 1-D array, which I think is a good starting point for my clustering algorithms to work. Let's see how it works. \n",
    "\n",
    "The Count Vectorizer exclusively accepts series in string format, not as tokenized lists. Each row, representing a document, must be a string. This ensures that every point in the clustering encapsulates an entire sequence, rather than isolated vectorized words.\n",
    "\n",
    "I set the `min_df` parameter to be equal to 5 to only include terms that occur more than 5 times in my **count vectorized** data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpkl\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Development environment passes all tests!\n"
     ]
    }
   ],
   "source": [
    "# %load C:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\test_environment.py\n",
    "import sys\n",
    "import numpy as np \n",
    "\n",
    "REQUIRED_PYTHON = \"python3\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    system_major = sys.version_info.major\n",
    "    if REQUIRED_PYTHON == \"python\":\n",
    "        required_major = 2\n",
    "    elif REQUIRED_PYTHON == \"python3\":\n",
    "        required_major = 3\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized python interpreter: {}\".format(\n",
    "            REQUIRED_PYTHON))\n",
    "\n",
    "    if system_major != required_major:\n",
    "        raise TypeError(\n",
    "            \"This project requires Python {}. Found: Python {}\".format(\n",
    "                required_major, sys.version))\n",
    "    else:\n",
    "        print(\">>> Development environment passes all tests!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3\n"
     ]
    }
   ],
   "source": [
    "print(REQUIRED_PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtest_environment\u001b[39;00m \u001b[39mimport\u001b[39;00m REQUIRED_PYTHON\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(corpus)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Get the feature names\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m feature_names \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mget_feature_names()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Print the count vectorized representation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(X\u001b[39m.\u001b[39mtoarray())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Print the count vectorized representation\n",
    "print(X.toarray())\n",
    "\n",
    "# Print the feature names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Vectorizing the data with Count Vectorizer \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bag_of_words \u001b[39m=\u001b[39m CountVectorizer(min_df\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Vectorizing the data with Count Vectorizer \n",
    "bag_of_words = CountVectorizer(min_df=5)\n",
    "inbound_cv = bag_of_words.fit_transform(string_processed_inbound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, ngram_range = (1,3))\n",
    "# Storing tfidf data and transforming them into sparse matrices\n",
    "inbound_tfidf = tfidf.fit_transform(string_processed_inbound)\n",
    "inbound_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained word embeddings\n",
    "While bag-of-words approaches can capture the presence of words in a text, they fail to preserve the crucial aspect of word order. This limitation necessitates the exploration of more sophisticated text vectorization techniques. With the advent of advanced methods, we can now effectively encode intent clusters into meaningful vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Glove "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll explore various word embeddings to encode text differently and assess their effectiveness, beginning with GloVe word embeddings—an unsupervised learning algorithm for generating word vector representations.\n",
    "\n",
    "Gensim simplifies the use of pretrained word embeddings, offering a specialized data format for easy loading into a numpy array.\n",
    "\n",
    "Note: I won't utilize this method because my clustering algorithms require tweets to represent single points, while this method transforms individual words. I'm keeping this note in my notebook to track progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.12 s\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"C:\\\\Sagar Study\\\\ML and Learning\\\\Projects\\\\customer-support-bot\\\\amazon_customer_support\\\\objects\\\\glove.6B.50d.txt\", \"r\", encoding=\"utf-8\") as file: \n",
    "    word_to_vec_map = {} \n",
    "    for line in file: \n",
    "        values = line.split() \n",
    "        curr_word = values[0] \n",
    "        coefs = np.array(values[1:], dtype=np.float32)  # Fix: Convert values to float\n",
    "        word_to_vec_map[curr_word] = coefs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70153 , -0.43853 ,  1.0509  , -0.33431 ,  0.67151 , -0.17677 ,\n",
       "        0.80079 ,  0.90158 , -0.29513 , -0.65586 , -0.083404,  0.35146 ,\n",
       "       -0.41071 ,  0.29446 , -1.1955  , -0.45611 ,  0.56877 ,  0.073522,\n",
       "       -1.2616  ,  0.22276 , -0.57735 ,  0.12075 ,  0.54712 , -0.34094 ,\n",
       "        0.2164  , -1.804   , -0.70362 , -0.56337 ,  1.8773  ,  0.1301  ,\n",
       "        2.271   , -0.25882 , -0.46309 , -0.7759  , -0.22926 ,  0.62156 ,\n",
       "       -0.043353, -0.60943 , -1.6791  , -0.018271,  0.53893 , -0.50689 ,\n",
       "        0.88454 , -0.11158 ,  0.57013 , -0.69098 , -0.43072 , -0.45332 ,\n",
       "       -0.27984 , -0.056133], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map[\"enemy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sea'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m word_to_vec_map[\u001b[39m\"\u001b[39;49m\u001b[39mSea\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sea'"
     ]
    }
   ],
   "source": [
    "word_to_vec_map[\"Sea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for interpretation later on, since this being the main embedding algorithm in my pipeline, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words_to_vec_map[\"sea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Doc2Vec\n",
    "Since this is the main embedding method I will use for my pipeline, I display how I capitalized this embedding in the next notebook. {to be edited later on}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hugging Face\n",
    "This is a startup that does a lot with NLP. I explore their encoders.\n",
    "\n",
    "BERT wouldn't really be a good option because a large part of that was trained with Wikipedia data.\n",
    "\n",
    "I am not sure what doc2vec is trained on, I think my results will be better if I find a Twitter based word embedding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fast-text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding methodology developed by Facebook AI team\n",
    "\n",
    "This one as well, like BERT, doesn't seem to be a good option at first glance due to its dependence on Wikipedia for training. \n",
    "\n",
    "Nevertheless, we will try and evaluate if it works well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we cluster, let's make scaled versions of our dataset first, which would be good for distance-based clustering methods. In general, these vectors shouldn't really need scaling, but it may help for computational purposes. I only do this for my count vectorized and tfidf vectorized data, not ones with the more meaningful word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and transforming to create standard scald versions of the data through the use of MaxAbsScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "inbound_cv = scaler.fit_transform(inbound_cv)\n",
    "outbound_cv = scaler.fit_transform(outbound_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbound_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbound_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Collection With Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Consideration: Because I will be working with neural networks in the next phase of my project, my objective is to identify approaximately `x` distinct intents in the data through Exploratory Data Analysis(EDA). Each of these should ideally consist of 1000 tweets similar to it to generate your training data for that intent with Gensim's `model.docvecs...` (complete next) function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aim to employ clustering methods and topic modeling techniques on my dataset to extract prominent themes, intending to manually assign labels to these clusters.\n",
    "\n",
    "Achieving successful outcomes will be more feasible if my dataset pertains uniformly to the domain of customer service. I've taken care to ensure that this Twitter data related to Amazon primarily focuses on this domain. This careful curation aims to enable my upcoming model to capture the subtleties required for intent classification. In essence, the conversational efficacy of the bot largely depends on the language it's been trained on.\n",
    "\n",
    "With respect to the nature of clustering algorithms, I understand that they may not precisely group the data as anticipated beforehand. It's an algorithmic process and might not seamlessly cluster intents together as desired. However, I will back my curiosity to discover insightful trends - if found any!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>1. K Means</color>\n",
    "My first approach for the clustering my word vectors is K-Means, which tends to perform well on blobs.\n",
    "\n",
    "A drawback is that it is very slow, and picking the value for K is hard - I don't even know how many intents there are in the data. This is why I start with larger jumps of K to get a higher level idea of which performs the best, then I dive deeper to finally decide what K works the best for finding the optimal number of intents in my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. K-Means for my TFIDF and Count Vectorized Data\n",
    "First I cluster the TFIDF and Count Vectorized data. Honestly, I won't really be expecting good results from it, so I won't spend a lot of effort doing this actual clustering on these two. But it's worth a shot to demonstrate an older and suboptimal approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I'm currently running the K-Means algorithm on my entire dataset. Alongside this, I'm performing hyperparameter optimization specifically on the n_clusters parameter. The initial progress bar reflects its progress with the dataset, while the second bar shows its completion concerning different values of n_clusters.\n",
    "\n",
    "The process of applying K-Means across 10 iterations spanning from 10 to 100 for both data types consumed approximately three hours before the scaling step. Thankfully, post-scaling, the training procedure noticeably sped up.\n",
    "\n",
    "Below this section, I've saved my results using Python's serialization package called Pickle to avoid the need to rerun the process again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# My grand dictionaries that will store all my results\n",
    "wcss_grand = {}\n",
    "labels_grand = {}\n",
    "silhouette_scores_grand = {}\n",
    "n_clusters = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "# Iterating through all the differently embedded data\n",
    "for i,j in tqdm(enumerate(vectorized_data.items())): \n",
    "    name = j[0] # Here j[0] is the name of the dataset\n",
    "    dataset = j[1] # And j[1] is the actual data\n",
    "    \n",
    "    # I store my metrics at these following lists\n",
    "    wcss = []\n",
    "    labels = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    # Looping through values of k\n",
    "    for k in tqdm(n_clusters):    \n",
    "        print(f'Currently fitting {name} with {k} clusters... Please wait')\n",
    "        \n",
    "        # Initializing with k-means++ ensures that you get don’t fall into the random initialization trap.\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state = 10)\n",
    "        kmeans.fit(dataset)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "        # Getting the silhouette score\n",
    "        labels.append(kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_score(dataset, kmeans.labels_))\n",
    "        \n",
    "        # Saving the models\n",
    "        filename = f'models/kmeans/{name}-{k}neighbors.sav'\n",
    "        joblib.dump(kmeans, filename)\n",
    "        \n",
    "    # Updating grand dictionary\n",
    "    wcss_grand[name + '_wcss'] = wcss\n",
    "    labels_grand[name + '_labels'] = labels\n",
    "    silhouette_scores_grand[name + '_silhouettes'] = silhouette_scores\n",
    "\n",
    "# Saving all my results\n",
    "with open('objects/wcss_grand.pkl', 'wb') as handle:\n",
    "    pickle.dump(wcss_grand, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('objects/labels_grand.pkl', 'wb') as handle:\n",
    "    pickle.dump(labels_grand, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('objects/silhouette_scores_grand.pkl', 'wb') as handle:\n",
    "    pickle.dump(silhouette_scores_grand, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading back in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing it into objects I can use in this notebook\n",
    "\n",
    "with open('objects/wcss_grand.pkl', 'rb') as handle: # Change path \n",
    "    wcss_grand = pkl.load(handle)\n",
    "with open('objects/labels_grand.pkl','rb') as handle: # Cnange path\n",
    "    labels_grand = pkl.load(handle)\n",
    "with open('objects/silhouette_scores_grand.pkl','rb') as handle: # Change path \n",
    "    silhouette_scores_grand = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Best K-Means Model\n",
    "I will try creating an elbow plot to see if there exists a clear elbow. I will do so on both the **count vectorized** and **tfidf** features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Plot count vectorized\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), wcss_grand['inbound_cv_ma_wcss'], color = 'magenta')\n",
    "plt.title('Elbow Method (Count Vectorized)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Elbow Plot tfidf\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), wcss_grand['inbound_tfidf_ma_wcss'], color = 'magenta')\n",
    "plt.title('Elbow Method (TFIDF)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silouette Plot count vectorized\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), silhouette_scores_grand['inbound_cv_ma_silhouettes'], color = 'red')\n",
    "plt.title('Silhouette Method (Count Vectorized)')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silouette Plot tfidf\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), silhouette_scores_grand['inbound_tfidf_ma_silhouettes'], color = 'red')\n",
    "plt.title('Silhouette Method (TFIDF)')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these plots don't really chaneg much between the TFIDF and Count Vectorized data, furthering my statement above that they aren't really going to be the most useful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing my clusters with t-SNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I try different color maps and choose one so its easier to distinguish between clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available sequential colormaps:\n",
    "```['viridis', 'plasma', 'inferno', 'magma', 'cividis']```\n",
    "\n",
    "```['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']```\n",
    "\n",
    "Available qualitative colormaps:\n",
    "```['Pastel1', 'Pastel2', 'Paired', 'Accent',\n",
    "                        'Dark2', 'Set1', 'Set2', 'Set3',\n",
    "                        'tab10', 'tab20', 'tab20b', 'tab20c']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the shape of current data \n",
    "inbound_cv.shape, inbound_tfidf_ma.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-SNE is a probabilistic model, so it will take some time to run, especially because we have a large no of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Instantiate t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=1, n_jobs=-1)\n",
    "\n",
    "# Fit t-SNE\n",
    "inbound_cv_ma_tsne = tsne.fit_transform(inbound_cv_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting my visualization for each of my n_neighbors with my count vectorized data\n",
    "\n",
    "for k in range(10,101,10):\n",
    "    # Getting the right K-Means cluster labels.\n",
    "    labels = joblib.load(f'models/kmeans/inbound_cv_ma-{str(k)}neighbors.sav').labels_ # See where these labels are saved \n",
    "    \n",
    "    # Visualize high-dimensional data\n",
    "    plt.figure(figsize=(13,12))\n",
    "    plt.scatter(inbound_cv_ma_tsne[:,0], inbound_cv_ma_tsne[:,1], s=20, c = labels, cmap = 'magma')\n",
    "    plt.title(f'2-D t-SNE Representation of my Count Vectorized Inbound data with {k} Clusters')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For now, we are seeing so many clusters embedded into each other without any clear demarcation between them. Should I play with `perspective` parameter to gain any more insights? \n",
    "- Also, at this stage it would be difficult to observe clusters separately since there are many dimensions and we are representing them on a 2D space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting my visualization for each of my n_neighbors with my tfidf data\n",
    "\n",
    "for k in range(10,101,10):\n",
    "    # Getting the right K-Means cluster labels.\n",
    "    labels = joblib.load(f'models/kmeans/inbound_tfidf_ma-{str(k)}neighbors.sav').labels_\n",
    "    \n",
    "    # Visualize high-dimensional data\n",
    "    plt.figure(figsize=(13,12))\n",
    "    plt.scatter(inbound_tfidf_ma_tsne[:,0], inbound_tfidf_ma_tsne[:,1], s=20, c = labels, cmap = 'magma')\n",
    "    plt.title(f'2-D t-SNE Representation of my TFIDF Inbound data with {k} Clusters')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be much difference in the outputs of CountVectorized clusters and TFIDF clusters. Let's check for Doc2Vec and other semantic embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. K-Means for my Doc2Vec data\n",
    "\n",
    "Notice that I did not scale my d2v data on purpose because I do not want to skew the distances that the pretrained model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized data\n",
    "vectorized_data = {'inbound_cv_d2v': inbound_d2v}\n",
    "\n",
    "# Briefly showing the contents of i and j\n",
    "for i, j in enumerate(vectorized_data.items()):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My d2v dictionaries that will store all my results\n",
    "wcss_d2v = {}\n",
    "labels_d2v = {}\n",
    "silhouette_scores_d2v = {}\n",
    "n_clusters = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "# Iterating through all the differently embedded data\n",
    "for i,j in tqdm(enumerate(vectorized_data.items())): \n",
    "    name = j[0] # Here j[0] is the name of the dataset\n",
    "    dataset = j[1] # And j[1] is the actual data\n",
    "    \n",
    "    # I store my metrics at these following lists\n",
    "    wcss = []\n",
    "    labels = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    # Looping through values of k\n",
    "    for k in tqdm(n_clusters):    \n",
    "        print(f'Currently fitting {name} with {k} clusters... Please wait')\n",
    "        \n",
    "        # Initializing with k-means++ ensures that you get don’t fall into the random initialization trap.\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state = 10)\n",
    "        kmeans.fit(dataset)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "        # Getting the silhouette score\n",
    "        labels.append(kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_score(dataset, kmeans.labels_))\n",
    "        \n",
    "        # Saving the models\n",
    "        filename = f'models/kmeans/{name}-{k}neighbors.sav'\n",
    "        joblib.dump(kmeans, filename)\n",
    "        \n",
    "    # Updating d2v dictionary\n",
    "    wcss_d2v[name + '_wcss'] = wcss\n",
    "    labels_d2v[name + '_labels'] = labels\n",
    "    silhouette_scores_d2v[name + '_silhouettes'] = silhouette_scores\n",
    "\n",
    "# Saving all my results, now with a d2v tag\n",
    "with open('objects/wcss_d2v.pkl', 'wb') as handle:\n",
    "    pickle.dump(wcss_d2v, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('objects/labels_d2v.pkl', 'wb') as handle:\n",
    "    pickle.dump(labels_d2v, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('objects/silhouette_scores_d2v.pkl', 'wb') as handle:\n",
    "    pickle.dump(silhouette_scores_d2v, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading back in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing it into objects I can use in this notebook\n",
    "\n",
    "with open('objects/wcss_d2v.pkl', 'rb') as handle:\n",
    "    wcss_d2v = pickle.load(handle)\n",
    "with open('objects/labels_d2v.pkl','rb') as handle:\n",
    "    labels_d2v = pickle.load(handle)\n",
    "with open('objects/silhouette_scores_d2v.pkl','rb') as handle:\n",
    "    silhouette_scores_d2v = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are my plots: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Plot d2v\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), wcss_d2v['inbound_cv_d2v_wcss'], color = 'magenta')\n",
    "plt.title('Elbow Method (Doc2Vec)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silouette Plot d2v\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), silhouette_scores_d2v['inbound_cv_d2v_silhouettes'], color = 'red')\n",
    "plt.title('Silhouette Method (Doc2Vec)')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we see that we have a slightly higher silhouette score which isn't negative now. It looks like K = 20 would be the best in this case as it has the highest silhouette score and there is sort of an elbow in the elbow plot, definitely more than at 80 clusters where it seems to be completely smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Instantiate t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=1, n_jobs=-1)\n",
    "\n",
    "# Fit t-SNE\n",
    "inbound_d2v_tsne = tsne.fit_transform(inbound_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting my visualization for each of my n_neighbors, now with Doc2Vec embedded data\n",
    "\n",
    "for k in range(10,101,10):\n",
    "    # Getting the right K-Means cluster labels.\n",
    "    labels = joblib.load(f'models/kmeans/inbound_cv_d2v-{str(k)}neighbors.sav').labels_\n",
    "    \n",
    "    # Visualize high-dimensional data\n",
    "    plt.figure(figsize=(13,12))\n",
    "    plt.scatter(inbound_d2v_tsne[:,0], inbound_d2v_tsne[:,1], s=20, c = labels, cmap = 'magma')\n",
    "    plt.title(f'2-D t-SNE Representation of my Doc2Vec Inbound data with {k} Clusters')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's super hard to tell based on a t-SNE plot, so for that reason, I will evaluate how it clustered at a later section by actually looking at the labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>2. LDA (Latent Dirichlet Allocation) </color>\n",
    "My second approach for the clustering is LDA topic modelling. It basically takes your data and splits it into topics. My goal is still to cluster, but with this method I hope to get more useful, distinct topics.\n",
    "\n",
    "Useful articles:\n",
    "* https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "* https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05\n",
    "\n",
    "There are also newer, deep-learning based methods called LDA2Vec which could be interesting to explore as well.\n",
    "\n",
    "However, due to prorities shifting, I will employ this step as a future step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 5\n",
    "number_words = 10\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "with open(LDAvis_data_filepath, 'w') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time constraint's sake, I decided not to use DBScan because they will achieve a clustering result similar to K-Means. I could have also use Gaussian Mixed Models or Heirarchical Clustering to achieve this clustering result. **Use GMMs and analyze your result on them as well**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Visualizing Intent Differences Between Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved all my models in a folder in this directory called folder. All that I have to do is pick a hyperparamater setting for that model and visualize what words are in those clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_inbound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it's really useful to see the top 10 words in a cluster to get a good idea of what the intents in that cluster is! (Check out the intents...Edit sentences in the final notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-support",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
