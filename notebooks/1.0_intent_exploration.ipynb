{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization\n",
    "The easiest way to represent a document as a vector is with a **bag of words Count Vectorizer**. This will turn each document to be a 1-D array, which I think is a good starting point for my clustering algorithms to work. Let's see how it works. \n",
    "\n",
    "The Count Vectorizer exclusively accepts series in string format, not as tokenized lists. Each row, representing a document, must be a string. This ensures that every point in the clustering encapsulates an entire sequence, rather than isolated vectorized words.\n",
    "\n",
    "I set the `min_df` parameter to be equal to 5 to only include terms that occur more than 5 times in my **count vectorized** data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Development environment passes all tests!\n"
     ]
    }
   ],
   "source": [
    "# %load C:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\test_environment.py\n",
    "import sys\n",
    "import numpy as np \n",
    "\n",
    "REQUIRED_PYTHON = \"python3\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    system_major = sys.version_info.major\n",
    "    if REQUIRED_PYTHON == \"python\":\n",
    "        required_major = 2\n",
    "    elif REQUIRED_PYTHON == \"python3\":\n",
    "        required_major = 3\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized python interpreter: {}\".format(\n",
    "            REQUIRED_PYTHON))\n",
    "\n",
    "    if system_major != required_major:\n",
    "        raise TypeError(\n",
    "            \"This project requires Python {}. Found: Python {}\".format(\n",
    "                required_major, sys.version))\n",
    "    else:\n",
    "        print(\">>> Development environment passes all tests!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3\n"
     ]
    }
   ],
   "source": [
    "print(REQUIRED_PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtest_environment\u001b[39;00m \u001b[39mimport\u001b[39;00m REQUIRED_PYTHON\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(corpus)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Get the feature names\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m feature_names \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mget_feature_names()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Print the count vectorized representation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(X\u001b[39m.\u001b[39mtoarray())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Print the count vectorized representation\n",
    "print(X.toarray())\n",
    "\n",
    "# Print the feature names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Vectorizing the data with Count Vectorizer \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bag_of_words \u001b[39m=\u001b[39m CountVectorizer(min_df\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Vectorizing the data with Count Vectorizer \n",
    "bag_of_words = CountVectorizer(min_df=5)\n",
    "inbound_cv = bag_of_words.fit_transform(string_processed_inbound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, ngram_range = (1,3))\n",
    "# Storing tfidf data and transforming them into sparse matrices\n",
    "inbound_tfidf = tfidf.fit_transform(string_processed_inbound)\n",
    "inbound_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained word embeddings\n",
    "While bag-of-words approaches can capture the presence of words in a text, they fail to preserve the crucial aspect of word order. This limitation necessitates the exploration of more sophisticated text vectorization techniques. With the advent of advanced methods, we can now effectively encode intent clusters into meaningful vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Glove "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll explore various word embeddings to encode text differently and assess their effectiveness, beginning with GloVe word embeddingsâ€”an unsupervised learning algorithm for generating word vector representations.\n",
    "\n",
    "Gensim simplifies the use of pretrained word embeddings, offering a specialized data format for easy loading into a numpy array.\n",
    "\n",
    "Note: I won't utilize this method because my clustering algorithms require tweets to represent single points, while this method transforms individual words. I'm keeping this note in my notebook to track progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.12 s\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"C:\\\\Sagar Study\\\\ML and Learning\\\\Projects\\\\customer-support-bot\\\\amazon_customer_support\\\\objects\\\\glove.6B.50d.txt\", \"r\", encoding=\"utf-8\") as file: \n",
    "    word_to_vec_map = {} \n",
    "    for line in file: \n",
    "        values = line.split() \n",
    "        curr_word = values[0] \n",
    "        coefs = np.array(values[1:], dtype=np.float32)  # Fix: Convert values to float\n",
    "        word_to_vec_map[curr_word] = coefs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70153 , -0.43853 ,  1.0509  , -0.33431 ,  0.67151 , -0.17677 ,\n",
       "        0.80079 ,  0.90158 , -0.29513 , -0.65586 , -0.083404,  0.35146 ,\n",
       "       -0.41071 ,  0.29446 , -1.1955  , -0.45611 ,  0.56877 ,  0.073522,\n",
       "       -1.2616  ,  0.22276 , -0.57735 ,  0.12075 ,  0.54712 , -0.34094 ,\n",
       "        0.2164  , -1.804   , -0.70362 , -0.56337 ,  1.8773  ,  0.1301  ,\n",
       "        2.271   , -0.25882 , -0.46309 , -0.7759  , -0.22926 ,  0.62156 ,\n",
       "       -0.043353, -0.60943 , -1.6791  , -0.018271,  0.53893 , -0.50689 ,\n",
       "        0.88454 , -0.11158 ,  0.57013 , -0.69098 , -0.43072 , -0.45332 ,\n",
       "       -0.27984 , -0.056133], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map[\"enemy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sea'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\notebooks\\1.0_intent_exploration.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sagar%20Study/ML%20and%20Learning/Projects/customer-support-bot/amazon_customer_support/notebooks/1.0_intent_exploration.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m word_to_vec_map[\u001b[39m\"\u001b[39;49m\u001b[39mSea\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sea'"
     ]
    }
   ],
   "source": [
    "word_to_vec_map[\"Sea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for interpretation later on, since this being the main embedding algorithm in my pipeline, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words_to_vec_map[\"sea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Doc2Vec\n",
    "Since this is the main embedding method I will use for my pipeline, I display how I capitalized this embedding in the next notebook. {to be edited later on}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hugging Face\n",
    "This is a startup that does a lot with NLP. I explore their encoders.\n",
    "\n",
    "BERT wouldn't really be a good option because a large part of that was trained with Wikipedia data.\n",
    "\n",
    "I am not sure what doc2vec is trained on, I think my results will be better if I find a Twitter based word embedding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fast-text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding methodology developed by Facebook AI team\n",
    "\n",
    "This one as well, like BERT, doesn't seem to be a good option at first glance due to its dependence on Wikipedia for training. \n",
    "\n",
    "Nevertheless, we will try and evaluate if it works well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we cluster, let's make scaled versions of our dataset first, which would be good for distance-based clustering methods. In general, these vectors shouldn't really need scaling, but it may help for computational purposes. I only do this for my count vectorized and tfidf vectorized data, not ones with the more meaningful word embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-support",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
