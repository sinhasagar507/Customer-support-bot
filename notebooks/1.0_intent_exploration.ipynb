{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Clustering, Document Embeddings, and Unsupervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, each of the preprocessed tweets from the previous notebook is assigned a label for each tweet in the dataset by using meaningful document embedding methods and other unsupervised learning techniques such as K-Means, DBScan, LDA, T-SNE, etc. \n",
    "\n",
    "My objective is to perform heuristic search using clustering algorithms and let them determine the intents (although I won't be able to determine which one they actually represent). This way atleast I will be able to estimate the number of different intents in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Development environment passes all tests!\n"
     ]
    }
   ],
   "source": [
    "# %load C:\\Sagar Study\\ML and Learning\\Projects\\customer-support-bot\\amazon_customer_support\\test_environment.py\n",
    "import sys\n",
    "\n",
    "REQUIRED_PYTHON = \"python3\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    system_major = sys.version_info.major\n",
    "    if REQUIRED_PYTHON == \"python\":\n",
    "        required_major = 2\n",
    "    elif REQUIRED_PYTHON == \"python3\":\n",
    "        required_major = 3\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized python interpreter: {}\".format(\n",
    "            REQUIRED_PYTHON))\n",
    "\n",
    "    if system_major != required_major:\n",
    "        raise TypeError(\n",
    "            \"This project requires Python {}. Found: Python {}\".format(\n",
    "                required_major, sys.version))\n",
    "    else:\n",
    "        print(\">>> Development environment passes all tests!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3\n"
     ]
    }
   ],
   "source": [
    "print(REQUIRED_PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os \n",
    "import time \n",
    "import pickle\n",
    "import joblib \n",
    "\n",
    "\n",
    "# Data Munging and Mathematical Operation\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# tqdm progress bars \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "# Sklearn Transformers, Utilities, and Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "# Data Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Word Embeddings (CountVectorizer, TFIDF, HashingVectorizer, Word2Vec, GloVe, FastText, Doc2Vec)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "import gensim\n",
    "\n",
    "\n",
    "# Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile, common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inbound_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>outbound_text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>inbound_lang</th>\n",
       "      <th>inbound_hashtags</th>\n",
       "      <th>outbound_hashtags</th>\n",
       "      <th>clean_inbound_text</th>\n",
       "      <th>clean_outbound_text</th>\n",
       "      <th>outbound_tokens_pos</th>\n",
       "      <th>inbound_tokens_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@AmazonHelp 3 different people have given 3 di...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-10-31 23:28:00+00:00</td>\n",
       "      <td>@115820 We'd like to take a further look into ...</td>\n",
       "      <td>619</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>different people have given different answers ...</td>\n",
       "      <td>wed like to take a further look into this with...</td>\n",
       "      <td>[-PRON-: NOUN, d: VERB, like: VERB, to: NOUN, ...</td>\n",
       "      <td>[different: NOUN, people: NOUN, have: NOUN, gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Way to drop the ball on customer service @1158...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-10-31 22:29:00+00:00</td>\n",
       "      <td>@115820 I'm sorry we've let you down! Without ...</td>\n",
       "      <td>616</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>way to drop the ball on customer service so pi...</td>\n",
       "      <td>i am sorry we have let you down without provid...</td>\n",
       "      <td>[i: NOUN, be: NOUN, sorry: NOUN, -PRON-: NOUN,...</td>\n",
       "      <td>[way: NOUN, to: NOUN, drop: VERB, the: NOUN, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@115823 I want my amazon payments account CLOS...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-10-31 22:28:34+00:00</td>\n",
       "      <td>@115822 I am unable to affect your account via...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>i want my amazon payments account closed dm me...</td>\n",
       "      <td>i am unable to affect your account via twitter...</td>\n",
       "      <td>[i: NOUN, be: NOUN, unable: NOUN, to: NOUN, af...</td>\n",
       "      <td>[i: NOUN, want: VERB, -PRON-: NOUN, amazon: NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@AmazonHelp @115826 Yeah this is crazy weâ€™re l...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-11-01 12:53:34+00:00</td>\n",
       "      <td>@115827 Thanks for your patience. ^KM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>yeah this is crazy were less than a week away ...</td>\n",
       "      <td>thanks for your patience km</td>\n",
       "      <td>[thank: NOUN, for: NOUN, -PRON-: NOUN, patienc...</td>\n",
       "      <td>[yeah: NOUN, this: NOUN, be: NOUN, crazy: NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@115828 How about you guys figure out my Xbox ...</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>2017-10-31 22:28:00+00:00</td>\n",
       "      <td>@115826 I'm sorry for the wait. You'll receive...</td>\n",
       "      <td>627</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>how about you guys figure out my xbox one x pr...</td>\n",
       "      <td>i am sorry for the wait you will receive an em...</td>\n",
       "      <td>[i: NOUN, be: NOUN, sorry: NOUN, for: NOUN, th...</td>\n",
       "      <td>[how: NOUN, about: NOUN, -PRON-: NOUN, guy: NO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        inbound_text   author_id  \\\n",
       "0  @AmazonHelp 3 different people have given 3 di...  AmazonHelp   \n",
       "1  Way to drop the ball on customer service @1158...  AmazonHelp   \n",
       "2  @115823 I want my amazon payments account CLOS...  AmazonHelp   \n",
       "3  @AmazonHelp @115826 Yeah this is crazy weâ€™re l...  AmazonHelp   \n",
       "4  @115828 How about you guys figure out my Xbox ...  AmazonHelp   \n",
       "\n",
       "                  created_at  \\\n",
       "0  2017-10-31 23:28:00+00:00   \n",
       "1  2017-10-31 22:29:00+00:00   \n",
       "2  2017-10-31 22:28:34+00:00   \n",
       "3  2017-11-01 12:53:34+00:00   \n",
       "4  2017-10-31 22:28:00+00:00   \n",
       "\n",
       "                                       outbound_text response_tweet_id  \\\n",
       "0  @115820 We'd like to take a further look into ...               619   \n",
       "1  @115820 I'm sorry we've let you down! Without ...               616   \n",
       "2  @115822 I am unable to affect your account via...               NaN   \n",
       "3              @115827 Thanks for your patience. ^KM               NaN   \n",
       "4  @115826 I'm sorry for the wait. You'll receive...               627   \n",
       "\n",
       "  inbound_lang inbound_hashtags outbound_hashtags  \\\n",
       "0           en               []                []   \n",
       "1           en               []                []   \n",
       "2           en               []                []   \n",
       "3           en               []                []   \n",
       "4           en               []                []   \n",
       "\n",
       "                                  clean_inbound_text  \\\n",
       "0  different people have given different answers ...   \n",
       "1  way to drop the ball on customer service so pi...   \n",
       "2  i want my amazon payments account closed dm me...   \n",
       "3  yeah this is crazy were less than a week away ...   \n",
       "4  how about you guys figure out my xbox one x pr...   \n",
       "\n",
       "                                 clean_outbound_text  \\\n",
       "0  wed like to take a further look into this with...   \n",
       "1  i am sorry we have let you down without provid...   \n",
       "2  i am unable to affect your account via twitter...   \n",
       "3                        thanks for your patience km   \n",
       "4  i am sorry for the wait you will receive an em...   \n",
       "\n",
       "                                 outbound_tokens_pos  \\\n",
       "0  [-PRON-: NOUN, d: VERB, like: VERB, to: NOUN, ...   \n",
       "1  [i: NOUN, be: NOUN, sorry: NOUN, -PRON-: NOUN,...   \n",
       "2  [i: NOUN, be: NOUN, unable: NOUN, to: NOUN, af...   \n",
       "3  [thank: NOUN, for: NOUN, -PRON-: NOUN, patienc...   \n",
       "4  [i: NOUN, be: NOUN, sorry: NOUN, for: NOUN, th...   \n",
       "\n",
       "                                  inbound_tokens_pos  \n",
       "0  [different: NOUN, people: NOUN, have: NOUN, gi...  \n",
       "1  [way: NOUN, to: NOUN, drop: VERB, the: NOUN, b...  \n",
       "2  [i: NOUN, want: VERB, -PRON-: NOUN, amazon: NO...  \n",
       "3  [yeah: NOUN, this: NOUN, be: NOUN, crazy: NOUN...  \n",
       "4  [how: NOUN, about: NOUN, -PRON-: NOUN, guy: NO...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import processed_v1 data \n",
    "processed_v1_df = pd.read_pickle('../data/processed/processed_v1.pkl').reset_index(drop=True)\n",
    "\n",
    "# An overview \n",
    "processed_v1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding and Vectorization \n",
    "Its necessary to represent the inbound requests in a dense vectorized format so that the clustering algorithms can distinguish/find similarity between different queries/requests\n",
    "\n",
    "- Some popular Embedding Methods \n",
    "    - CountVectorizer \n",
    "    - TfidfVectorizer \n",
    "    - HashingVectorizer \n",
    "    - Word2Vec \n",
    "    - Glove \n",
    "    - FastText \n",
    "    - Doc2Vec \n",
    "\n",
    "- Since I am working on sequences, they are bound to be of different lengths. Word-based embeddings by default generate embeddings of different lengths and though I can pad them to bring to a specific length, the resultant representation isn't accepted by standard statistical clustering algorithms. However, I can experiment the following operations in the **upcoming iterations** and assess whether I get better results: \n",
    "    - **Averaging word embeddings**: Calculate the average of individual word embeddings in a sentence to obtain a single vector representing the sentence. While simple, this method might lose some sentence-specific information \n",
    "    - **Weighted average of word embeddings**: Assign weights to words based on their importance in the sentence (e.g., TFIDF weights) and calculate a weighted average of word embeddings \n",
    "    - **Doc2Vec/Fasttext with tagging**: Use techniques like Word2Vec or Fasttext with additional document tags to infer document-level vectors that encapsulate the meanings of sentences/documents \n",
    "\n",
    "- For the time being, I will stick with 2 sentence level embeddings going ahead, which are `CountVectorization`, `Tfidf`, `HashingVectorizer` and `Doc2Vec` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count Vectorization\n",
    "The easiest way to represent a document as a vector is with a **bag of words Count Vectorizer**. This will turn each document to be a 1-D array, which I think is a good starting point for my clustering algorithms to work. Let's see how it works. \n",
    "\n",
    "The Count Vectorizer exclusively accepts series in string format, not as tokenized lists. Each row, representing a document, must be a string. This ensures that every point in the clustering encapsulates an entire sequence, rather than isolated vectorized words.\n",
    "\n",
    "I set the `min_df` parameter to be equal to 5 to only include terms that occur more than 5 times in my **count vectorized** data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "## Sample CountVectorizer Testing \n",
    "# Sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         different people have given different answers ...\n",
       "1         way to drop the ball on customer service so pi...\n",
       "2         i want my amazon payments account closed dm me...\n",
       "3         yeah this is crazy were less than a week away ...\n",
       "4         how about you guys figure out my xbox one x pr...\n",
       "                                ...                        \n",
       "122335    i sent you guys a dm regarding the status of m...\n",
       "122336    this is happening in my area w prime deliverie...\n",
       "122337    got my at am thanks for fulfilling the order fast\n",
       "122338    no exchange available for i need to exchange m...\n",
       "122339    there should be bonus and gifts for regular cu...\n",
       "Name: clean_inbound_text, Length: 122340, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_v1_df[\"clean_inbound_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 3.8683745861053467 seconds\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing inbound requests with Count Vectorizer \n",
    "start_time = time.time()\n",
    "bag_of_words = CountVectorizer(min_df=5)\n",
    "\n",
    "# Storing count vectorized data and storing them into sparse matrices\n",
    "inbound_cv = bag_of_words.fit_transform(processed_v1_df[\"clean_inbound_text\"])\n",
    "outbound_cv = bag_of_words.fit_transform(processed_v1_df[\"clean_outbound_text\"])\n",
    "\n",
    "end_time = time.time()  \n",
    "print(\"Time Elapsed: {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 13.996706247329712 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tfidf = TfidfVectorizer(min_df=5, ngram_range = (1,3))\n",
    "\n",
    "# Storing tfidf data and transforming them into sparse matrices\n",
    "inbound_tfidf = tfidf.fit_transform(processed_v1_df[\"clean_inbound_text\"])\n",
    "outbound_tfidf = tfidf.fit_transform(processed_v1_df[\"clean_outbound_text\"])\n",
    "end_time = time.time()  \n",
    "print(\"Time Elapsed: {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 93.6 GiB for an array with shape (122340, 102638) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minbound_tfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 93.6 GiB for an array with shape (122340, 102638) and data type float64"
     ]
    }
   ],
   "source": [
    "print(inbound_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained word embeddings\n",
    "While bag-of-words approaches can capture the presence of words in a text, they fail to preserve the crucial aspect of word order. This limitation necessitates the exploration of more sophisticated text vectorization techniques. With the advent of advanced methods, we can now effectively encode intent clusters into meaningful vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Glove "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll explore various word embeddings to encode text differently and assess their effectiveness, beginning with GloVe word embeddingsâ€”an unsupervised learning algorithm for generating word vector representations.\n",
    "\n",
    "Gensim simplifies the use of pretrained word embeddings, offering a specialized data format for easy loading into a numpy array.\n",
    "\n",
    "Note: I won't utilize this method because my clustering algorithms require tweets to be represented in a single format, while this method transforms individual words. I'm keeping this note in my notebook to track progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.89 s\n",
      "Wall time: 9.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"../objects/glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as file: \n",
    "    word_to_vec_map = {} \n",
    "    for line in file: \n",
    "        values = line.split() \n",
    "        curr_word = values[0] \n",
    "        coefs = np.array(values[1:], dtype=np.float32)  # Fix: Convert values to float\n",
    "        word_to_vec_map[curr_word] = coefs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'why is my order at my local courier for the last days and still has not been delivered to me over week late ðŸ˜¡'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_v1_df[\"clean_inbound_text\"][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** \n",
    "- Oh I have forgotten to remove Emojis in my first preprocessing step. Will come back to it again. For the time being, see how other models embed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122340/122340 [00:04<00:00, 29371.80it/s]\n"
     ]
    }
   ],
   "source": [
    "def text_to_word_sequence(text): \n",
    "    return text.split()\n",
    "\n",
    "# Apply embeddings to text data \n",
    "def text_to_embeddings(text): \n",
    "    # Split text into words \n",
    "    words = text_to_word_sequence(text)\n",
    "    # Initialize empty list of embeddings \n",
    "    embeddings_list = []\n",
    "    # Loop through words \n",
    "    for word in words: \n",
    "        # If word exists in embeddings, append to embeddings list \n",
    "        if word in word_to_vec_map: \n",
    "            embeddings_list.append(word_to_vec_map[word])\n",
    "    # Return embeddings list \n",
    "    return embeddings_list  \n",
    "\n",
    "processed_v1_df[\"inbound_text_glove\"] = processed_v1_df[\"clean_inbound_text\"].progress_apply(text_to_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbound_text_glove = processed_v1_df[\"inbound_text_glove\"]\n",
    "\n",
    "# Store inbound_text_glove as a pickle file\n",
    "with open(\"../objects/inbound_text_glove.pkl\", \"wb\") as file: \n",
    "    pickle.dump(inbound_text_glove, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Drop inbound_text_glove column\n",
    "processed_v1_df.drop(columns=[\"inbound_text_glove\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Doc2Vec\n",
    "Main algorithm for my document embeddings, to be included later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hugging Face\n",
    "- Search for Twitter trained sentence embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fast Text \n",
    "- An embedding methodology developed by Facebook AI team\n",
    "- Trained on Wikipedia text, doesn't seem like the ideal choice \n",
    "- Nevertheless, we will try and evaluate if it works well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create scaled versions of our dataset before clustering. This is particularly useful for distance-based clustering techniques. Normally, these vectors don't require scaling, but it could enhance computational efficiency. I apply scaling only to my count vectorized and TF-IDF vectorized data, not to the ones with more meaningful word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and transforming to create standard scald versions of the data through the use of MaxAbsScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "scaled_inbound_cv = scaler.fit_transform(inbound_cv)\n",
    "scaled_outbound_cv = scaler.fit_transform(outbound_cv)\n",
    "scaled_inbound_tfidf = scaler.fit_transform(inbound_tfidf)  \n",
    "scaled_outbound_tfidf = scaler.fit_transform(outbound_tfidf)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inbound_cv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_inbound_cv.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Collection With Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Consideration: Because I will be working with neural networks in the next phase of my project, my objective is to identify approaximately `x` distinct intents in the data through Exploratory Data Analysis(EDA). Each of these should ideally consist of 1000 tweets similar to it to generate your training data for that intent with Gensim's `model.docvecs.similar` something function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aim to employ clustering methods and topic modeling techniques on my dataset to extract prominent themes, intending to manually assign labels to these clusters.\n",
    "\n",
    "Achieving successful outcomes will be more feasible if my dataset pertains uniformly to the domain of customer service. I've taken care to ensure that this Twitter data related to Amazon primarily focuses on this domain. This careful curation aims to enable my upcoming model to capture the subtleties required for intent classification. In essence, the conversational efficacy of the bot largely depends on the language and domain it's been trained on.\n",
    "\n",
    "With respect to the nature of clustering algorithms, I understand that they may not precisely group the data as anticipated beforehand. It's an algorithmic process and might not seamlessly cluster intents together as desired. However, I will back my curiosity to discover insightful trends - if found any!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>1. K Means</color>\n",
    "My first approach for the clustering my word vectors is K-Means, which tends to perform well on blobs.\n",
    "\n",
    "A drawback is that it is very slow, and picking the value for K is hard - I don't even know how many intents there are in the data. This is why I start with larger jumps of K to get a higher level idea of which performs the best, then I dive deeper to finally decide what K works the best for finding the optimal number of intents in my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. K-Means for Count Vectorized and Tfidf data\n",
    "First I cluster the TFIDF and Count Vectorized data. Honestly, I won't really be expecting good results from it, so I won't spend a lot of effort doing this actual clustering on these two. But it's worth a shot to demonstrate an older and suboptimal approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I'm currently running the K-Means algorithm on my entire dataset. Alongside this, I'm performing hyperparameter optimization specifically on the n_clusters parameter. The initial progress bar reflects its progress with the dataset, while the second bar shows its completion concerning different values of n_clusters.\n",
    "\n",
    "The process of applying K-Means across 10 iterations spanning from 10 to 100 for both data types consumed approximately three hours before the scaling step. Thankfully, post-scaling, the training procedure noticeably sped up.\n",
    "\n",
    "Below this section, I've saved my results using Python's serialization package called Pickle to avoid the need to rerun the process again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('scaled_inbound_cv', <122340x8365 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 1941976 stored elements in Compressed Sparse Row format>)\n",
      "1 ('scaled_inbound_tfidf', <122340x102638 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 4094473 stored elements in Compressed Sparse Row format>)\n"
     ]
    }
   ],
   "source": [
    "# Vectorized data\n",
    "vectorized_data = {\"scaled_inbound_cv\": scaled_inbound_cv, \"scaled_inbound_tfidf\": scaled_inbound_tfidf}\n",
    "\n",
    "# Briefly showing the contents of i and\n",
    "for i,j in enumerate(vectorized_data.items()): print(i,j);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# My grand dictionaries that will store all my results\n",
    "wcss_grand = {}\n",
    "labels_grand = {}\n",
    "silhouette_scores_grand = {}\n",
    "n_clusters = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "# Iterating through all the differently embedded data\n",
    "for i,j in tqdm(enumerate(vectorized_data.items())): \n",
    "    name = j[0] # Here j[0] is the name of the dataset\n",
    "    dataset = j[1] # And j[1] is the actual data\n",
    "    \n",
    "    # I store my metrics at these following lists\n",
    "    wcss = []\n",
    "    labels = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    # Looping through values of k\n",
    "    for k in tqdm(n_clusters):    \n",
    "        print(f'Currently fitting {name} with {k} clusters... Please wait')\n",
    "        \n",
    "        # Initializing with k-means++ ensures that you get donâ€™t fall into the random initialization trap.\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state = 10)\n",
    "        kmeans.fit(dataset)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "        # Getting the silhouette score\n",
    "        labels.append(kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_score(dataset, kmeans.labels_))\n",
    "        \n",
    "        # Saving the models\n",
    "        filename = f'models/kmeans/{name}-{k}neighbors.sav'\n",
    "        joblib.dump(kmeans, filename)\n",
    "        \n",
    "    # Updating grand dictionary\n",
    "    wcss_grand[name + '_wcss'] = wcss\n",
    "    labels_grand[name + '_labels'] = labels\n",
    "    silhouette_scores_grand[name + '_silhouettes'] = silhouette_scores\n",
    "\n",
    "# Saving all my results\n",
    "with open(\"../objects/wcss_grand.pkl\", \"wb\") as file:\n",
    "    pickle.dump(wcss_grand, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"../objects/labels_grand.pkl\", \"wb\") as file:\n",
    "    pickle.dump(labels_grand, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"../objects/silhouette_scores_grand.pkl\", \"wb\") as file:\n",
    "    pickle.dump(silhouette_scores_grand, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading back in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing it into objects I can use in this notebook\n",
    "\n",
    "with open(\"../objects/wcss_grand.pkl\", \"rb\") as file:\n",
    "    wcss_grand = pkl.load(file)\n",
    "with open(\"../objects/labels_grand.pkl\", \"rb\") as file: \n",
    "    labels_grand = pkl.load(file)\n",
    "with open(\"../objects/silhouette_scores_grand.pkl\", \"rb\") as file: \n",
    "    silhouette_scores_grand = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_scores_grand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Best K-Means Model\n",
    "I will try creating an elbow plot to see if there exists a clear elbow. I will do so on both the **count vectorized** and **tfidf** features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'inbound_cv_ma_wcss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Elbow Plot count vectorized\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m101\u001b[39m, \u001b[38;5;241m10\u001b[39m), \u001b[43mwcss_grand\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minbound_cv_ma_wcss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmagenta\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElbow Method (Count Vectorized)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of clusters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'inbound_cv_ma_wcss'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Elbow Plot count vectorized\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), wcss_grand['inbound_cv_ma_wcss'], color = 'magenta')\n",
    "plt.title('Elbow Method (Count Vectorized)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Elbow Plot tfidf\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), wcss_grand['inbound_tfidf_ma_wcss'], color = 'magenta')\n",
    "plt.title('Elbow Method (TFIDF)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silouette Plot count vectorized\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), silhouette_scores_grand['inbound_cv_ma_silhouettes'], color = 'red')\n",
    "plt.title('Silhouette Method (Count Vectorized)')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silouette Plot tfidf\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), silhouette_scores_grand['inbound_tfidf_ma_silhouettes'], color = 'red')\n",
    "plt.title('Silhouette Method (TFIDF)')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these plots don't really chaneg much between the TFIDF and Count Vectorized data, furthering my statement above that they aren't really going to be the most useful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing my clusters with t-SNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I try different color maps and choose one so its easier to distinguish between clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available sequential colormaps:\n",
    "```['viridis', 'plasma', 'inferno', 'magma', 'cividis']```\n",
    "\n",
    "```['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']```\n",
    "\n",
    "Available qualitative colormaps:\n",
    "```['Pastel1', 'Pastel2', 'Paired', 'Accent',\n",
    "                        'Dark2', 'Set1', 'Set2', 'Set3',\n",
    "                        'tab10', 'tab20', 'tab20b', 'tab20c']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the shape of current data \n",
    "inbound_cv.shape, inbound_tfidf_ma.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-SNE is a probabilistic model, so it will take some time to run, especially because we have a large no of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Instantiate t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=1, n_jobs=-1)\n",
    "\n",
    "# Fit t-SNE\n",
    "inbound_cv_ma_tsne = tsne.fit_transform(inbound_cv_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting my visualization for each of my n_neighbors with my count vectorized data\n",
    "\n",
    "for k in range(10,101,10):\n",
    "    # Getting the right K-Means cluster labels.\n",
    "    labels = joblib.load(f'models/kmeans/inbound_cv_ma-{str(k)}neighbors.sav').labels_ # See where these labels are saved \n",
    "    \n",
    "    # Visualize high-dimensional data\n",
    "    plt.figure(figsize=(13,12))\n",
    "    plt.scatter(inbound_cv_ma_tsne[:,0], inbound_cv_ma_tsne[:,1], s=20, c = labels, cmap = 'magma')\n",
    "    plt.title(f'2-D t-SNE Representation of my Count Vectorized Inbound data with {k} Clusters')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For now, we are seeing so many clusters embedded into each other without any clear demarcation between them. Should I play with `perspective` parameter to gain any more insights? \n",
    "- Also, at this stage it would be difficult to observe clusters separately since there are many dimensions and we are representing them on a 2D space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting my visualization for each of my n_neighbors with my tfidf data\n",
    "\n",
    "for k in range(10,101,10):\n",
    "    # Getting the right K-Means cluster labels.\n",
    "    labels = joblib.load(f'models/kmeans/inbound_tfidf_ma-{str(k)}neighbors.sav').labels_\n",
    "    \n",
    "    # Visualize high-dimensional data\n",
    "    plt.figure(figsize=(13,12))\n",
    "    plt.scatter(inbound_tfidf_ma_tsne[:,0], inbound_tfidf_ma_tsne[:,1], s=20, c = labels, cmap = 'magma')\n",
    "    plt.title(f'2-D t-SNE Representation of my TFIDF Inbound data with {k} Clusters')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be much difference in the outputs of CountVectorized clusters and TFIDF clusters. Let's check for Doc2Vec and other semantic embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. K-Means for my Doc2Vec data\n",
    "\n",
    "Notice that I did not scale my d2v data on purpose because I do not want to skew the distances that the pretrained model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized data\n",
    "vectorized_data = {'inbound_cv_d2v': inbound_d2v}\n",
    "\n",
    "# Briefly showing the contents of i and j\n",
    "for i, j in enumerate(vectorized_data.items()):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My d2v dictionaries that will store all my results\n",
    "wcss_d2v = {}\n",
    "labels_d2v = {}\n",
    "silhouette_scores_d2v = {}\n",
    "n_clusters = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "# Iterating through all the differently embedded data\n",
    "for i,j in tqdm(enumerate(vectorized_data.items())): \n",
    "    name = j[0] # Here j[0] is the name of the dataset\n",
    "    dataset = j[1] # And j[1] is the actual data\n",
    "    \n",
    "    # I store my metrics at these following lists\n",
    "    wcss = []\n",
    "    labels = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    # Looping through values of k\n",
    "    for k in tqdm(n_clusters):    \n",
    "        print(f'Currently fitting {name} with {k} clusters... Please wait')\n",
    "        \n",
    "        # Initializing with k-means++ ensures that you get donâ€™t fall into the random initialization trap.\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state = 10)\n",
    "        kmeans.fit(dataset)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "        # Getting the silhouette score\n",
    "        labels.append(kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_score(dataset, kmeans.labels_))\n",
    "        \n",
    "        # Saving the models\n",
    "        filename = f'models/kmeans/{name}-{k}neighbors.sav'\n",
    "        joblib.dump(kmeans, filename)\n",
    "        \n",
    "    # Updating d2v dictionary\n",
    "    wcss_d2v[name + '_wcss'] = wcss\n",
    "    labels_d2v[name + '_labels'] = labels\n",
    "    silhouette_scores_d2v[name + '_silhouettes'] = silhouette_scores\n",
    "\n",
    "# Saving all my results, now with a d2v tag\n",
    "with open('objects/wcss_d2v.pkl', 'wb') as handle:\n",
    "    pickle.dump(wcss_d2v, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('objects/labels_d2v.pkl', 'wb') as handle:\n",
    "    pickle.dump(labels_d2v, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('objects/silhouette_scores_d2v.pkl', 'wb') as handle:\n",
    "    pickle.dump(silhouette_scores_d2v, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading back in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing it into objects I can use in this notebook\n",
    "\n",
    "with open('objects/wcss_d2v.pkl', 'rb') as handle:\n",
    "    wcss_d2v = pickle.load(handle)\n",
    "with open('objects/labels_d2v.pkl','rb') as handle:\n",
    "    labels_d2v = pickle.load(handle)\n",
    "with open('objects/silhouette_scores_d2v.pkl','rb') as handle:\n",
    "    silhouette_scores_d2v = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are my plots: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Plot d2v\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), wcss_d2v['inbound_cv_d2v_wcss'], color = 'magenta')\n",
    "plt.title('Elbow Method (Doc2Vec)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silouette Plot d2v\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(10, 101, 10), silhouette_scores_d2v['inbound_cv_d2v_silhouettes'], color = 'red')\n",
    "plt.title('Silhouette Method (Doc2Vec)')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we see that we have a slightly higher silhouette score which isn't negative now. It looks like K = 20 would be the best in this case as it has the highest silhouette score and there is sort of an elbow in the elbow plot, definitely more than at 80 clusters where it seems to be completely smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Instantiate t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=1, n_jobs=-1)\n",
    "\n",
    "# Fit t-SNE\n",
    "inbound_d2v_tsne = tsne.fit_transform(inbound_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting my visualization for each of my n_neighbors, now with Doc2Vec embedded data\n",
    "\n",
    "for k in range(10,101,10):\n",
    "    # Getting the right K-Means cluster labels.\n",
    "    labels = joblib.load(f'models/kmeans/inbound_cv_d2v-{str(k)}neighbors.sav').labels_\n",
    "    \n",
    "    # Visualize high-dimensional data\n",
    "    plt.figure(figsize=(13,12))\n",
    "    plt.scatter(inbound_d2v_tsne[:,0], inbound_d2v_tsne[:,1], s=20, c = labels, cmap = 'magma')\n",
    "    plt.title(f'2-D t-SNE Representation of my Doc2Vec Inbound data with {k} Clusters')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's super hard to tell based on a t-SNE plot, so for that reason, I will evaluate how it clustered at a later section by actually looking at the labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>2. LDA (Latent Dirichlet Allocation) </color>\n",
    "My second approach for the clustering is LDA topic modelling. It basically takes your data and splits it into topics. My goal is still to cluster, but with this method I hope to get more useful, distinct topics.\n",
    "\n",
    "Useful articles:\n",
    "* https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "* https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05\n",
    "\n",
    "There are also newer, deep-learning based methods called LDA2Vec which could be interesting to explore as well.\n",
    "\n",
    "However, due to prorities shifting, I will employ this step as a future step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 5\n",
    "number_words = 10\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "with open(LDAvis_data_filepath, 'w') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time constraint's sake, I decided not to use DBScan because they will achieve a clustering result similar to K-Means. I could have also use Gaussian Mixed Models or Heirarchical Clustering to achieve this clustering result. **Use GMMs and analyze your result on them as well**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Visualizing Intent Differences Between Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved all my models in a folder in this directory called folder. All that I have to do is pick a hyperparamater setting for that model and visualize what words are in those clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_inbound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it's really useful to see the top 10 words in a cluster to get a good idea of what the intents in that cluster is! (Check out the intents...Edit sentences in the final notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
