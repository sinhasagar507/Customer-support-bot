{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-29T11:02:13.668738Z","iopub.status.busy":"2023-11-29T11:02:13.668160Z","iopub.status.idle":"2023-11-29T11:02:14.194260Z","shell.execute_reply":"2023-11-29T11:02:14.193137Z","shell.execute_reply.started":"2023-11-29T11:02:13.668706Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["**Questions** (Ponder over each of them later...) \n","- Can we predict company responses? Given the bounded set of subjects handled by each company, the answer seems yes!!! \n","- Do requests get stale? How quickly do the best companies respond, compared to the worst? \n","- Can we learn high quality dense embeddings or representations of similarity for topical clustering? \n","- How does tone affect the support conversation? Does saying sorry help?: Ponder how I can analyze this...\n","- Can we help companies identify new problems, or ones most affecting their customers? "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-29T11:02:14.197954Z","iopub.status.busy":"2023-11-29T11:02:14.197100Z","iopub.status.idle":"2023-11-29T11:02:15.511735Z","shell.execute_reply":"2023-11-29T11:02:15.510429Z","shell.execute_reply.started":"2023-11-29T11:02:14.197918Z"},"trusted":true},"outputs":[],"source":["# Data visualization \n","import matplotlib.pyplot as plt \n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T06:47:28.391727Z","iopub.status.busy":"2023-11-24T06:47:28.391230Z","iopub.status.idle":"2023-11-24T06:47:29.572874Z","shell.execute_reply":"2023-11-24T06:47:29.571564Z","shell.execute_reply.started":"2023-11-24T06:47:28.391675Z"},"trusted":true},"outputs":[],"source":["## Make an output \"visualizations\" directory for storing images \n","# !cd /kaggle/working/\n","# !mkdir visualizations"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["import nltk\n","import ssl\n","\n","try:\n","    _create_unverified_https_context = ssl._create_unverified_context\n","except AttributeError:\n","    pass\n","else:\n","    ssl._create_default_https_context = _create_unverified_https_context\n","\n","nltk.download()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T10:57:50.334774Z","iopub.status.busy":"2023-11-27T10:57:50.333631Z","iopub.status.idle":"2023-11-27T10:57:50.357283Z","shell.execute_reply":"2023-11-27T10:57:50.355960Z","shell.execute_reply.started":"2023-11-27T10:57:50.334736Z"},"trusted":true},"outputs":[],"source":["# Import necessary libraries\n","import re \n","from collections import Counter \n","import warnings \n","warnings.filterwarnings(\"ignore\")\n","\n","# Object serialization \n","import pickle\n","import sklearn\n","\n","# WordCloud \n","from wordcloud import WordCloud, STOPWORDS\n","\n","# Data Visualization \n","import matplotlib as mp\n","import matplotlib.pyplot as plt \n","%matplotlib inline\n","import seaborn as sns\n","\n","# Pandas pre-profiling \n","from ydata_profiling import ProfileReport \n","\n","# Spelling Checker \n","# from spellchecker import SpellChecker \n","\n","# Import Natural Language Processing (NLP) libraries\n","import nltk\n","# nltk.download(\"stopwords\")\n","# nltk.download(\"wordnet\")\n","# nltk.download(\"punkt\")\n","# nltk.download(\"averaged_perceptron_tagger\")\n","from nltk import word_tokenize, sent_tokenize \n","from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n","from nltk.tokenize import TweetTokenizer\n","from nltk.corpus import stopwords, wordnet \n","\n","# Visualization \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","sns.set(style=\"ticks\", color_codes=True)\n","# from bokeh.io import output_file, show\n","# from bokeh.plotting import figure\n","# from bokeh.models import ColorBar, ColumnDataSource\n","# from bokeh.colors import Color\n","\n","\n","# Import Spacy for advanced natural language processing\n","# import spacy\n","\n","# Fasttext languae detection \n","# from ftlangdetect import detect\n","\n","# Contractions \n","import contractions as cm \n","\n","# Import langdetect for language detection\n","# Note: set seed=0 to enforce consistent results (to be done later)\n","# from langdetect import DetectorFactory, detect \n","# from spacy_langdetect import LanguageDetector\n","from typing import List, Dict \n","\n","# Import scikit-learn utilities\n","from sklearn.preprocessing import FunctionTransformer, LabelEncoder \n","from sklearn.pipeline import Pipeline, FeatureUnion  \n","\n","# Import Spacy tokenizer\n","# from spacy.tokenizer import Tokenizer \n","\n","# Import tqdm for progress bars\n","# Register 'pandas.progress_apply' and 'pandas.Series.map_apply' with 'tqdm' \n","from tqdm import tqdm\n","tqdm.pandas(desc=\"my bar!!!\")\n","\n","# Chi2 test \n","from scipy.stats import chi2_contingency"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inbound_text</th>\n","      <th>author_id</th>\n","      <th>created_at</th>\n","      <th>outbound_text</th>\n","      <th>response_tweet_id</th>\n","      <th>inbound_lang</th>\n","      <th>inbound_hashtags</th>\n","      <th>outbound_hashtags</th>\n","      <th>clean_inbound_text</th>\n","      <th>clean_outbound_text</th>\n","      <th>outbound_tokens_pos</th>\n","      <th>inbound_tokens_pos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>@AmazonHelp 3 different people have given 3 di...</td>\n","      <td>AmazonHelp</td>\n","      <td>2017-10-31 23:28:00+00:00</td>\n","      <td>@115820 We'd like to take a further look into ...</td>\n","      <td>619</td>\n","      <td>en</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>different people have given different answers ...</td>\n","      <td>wed like to take a further look into this with...</td>\n","      <td>[-PRON-: NOUN, d: VERB, like: VERB, to: NOUN, ...</td>\n","      <td>[different: NOUN, people: NOUN, have: NOUN, gi...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Way to drop the ball on customer service @1158...</td>\n","      <td>AmazonHelp</td>\n","      <td>2017-10-31 22:29:00+00:00</td>\n","      <td>@115820 I'm sorry we've let you down! Without ...</td>\n","      <td>616</td>\n","      <td>en</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>way to drop the ball on customer service so pi...</td>\n","      <td>i am sorry we have let you down without provid...</td>\n","      <td>[i: NOUN, be: NOUN, sorry: NOUN, -PRON-: NOUN,...</td>\n","      <td>[way: NOUN, to: NOUN, drop: VERB, the: NOUN, b...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>@115823 I want my amazon payments account CLOS...</td>\n","      <td>AmazonHelp</td>\n","      <td>2017-10-31 22:28:34+00:00</td>\n","      <td>@115822 I am unable to affect your account via...</td>\n","      <td>NaN</td>\n","      <td>en</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>i want my amazon payments account closed dm me...</td>\n","      <td>i am unable to affect your account via twitter...</td>\n","      <td>[i: NOUN, be: NOUN, unable: NOUN, to: NOUN, af...</td>\n","      <td>[i: NOUN, want: VERB, -PRON-: NOUN, amazon: NO...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>@AmazonHelp @115826 Yeah this is crazy weâ€™re l...</td>\n","      <td>AmazonHelp</td>\n","      <td>2017-11-01 12:53:34+00:00</td>\n","      <td>@115827 Thanks for your patience. ^KM</td>\n","      <td>NaN</td>\n","      <td>en</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>yeah this is crazy were less than a week away ...</td>\n","      <td>thanks for your patience km</td>\n","      <td>[thank: NOUN, for: NOUN, -PRON-: NOUN, patienc...</td>\n","      <td>[yeah: NOUN, this: NOUN, be: NOUN, crazy: NOUN...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>@115828 How about you guys figure out my Xbox ...</td>\n","      <td>AmazonHelp</td>\n","      <td>2017-10-31 22:28:00+00:00</td>\n","      <td>@115826 I'm sorry for the wait. You'll receive...</td>\n","      <td>627</td>\n","      <td>en</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>how about you guys figure out my xbox one x pr...</td>\n","      <td>i am sorry for the wait you will receive an em...</td>\n","      <td>[i: NOUN, be: NOUN, sorry: NOUN, for: NOUN, th...</td>\n","      <td>[how: NOUN, about: NOUN, -PRON-: NOUN, guy: NO...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         inbound_text   author_id  \\\n","4   @AmazonHelp 3 different people have given 3 di...  AmazonHelp   \n","5   Way to drop the ball on customer service @1158...  AmazonHelp   \n","6   @115823 I want my amazon payments account CLOS...  AmazonHelp   \n","9   @AmazonHelp @115826 Yeah this is crazy weâ€™re l...  AmazonHelp   \n","10  @115828 How about you guys figure out my Xbox ...  AmazonHelp   \n","\n","                   created_at  \\\n","4   2017-10-31 23:28:00+00:00   \n","5   2017-10-31 22:29:00+00:00   \n","6   2017-10-31 22:28:34+00:00   \n","9   2017-11-01 12:53:34+00:00   \n","10  2017-10-31 22:28:00+00:00   \n","\n","                                        outbound_text response_tweet_id  \\\n","4   @115820 We'd like to take a further look into ...               619   \n","5   @115820 I'm sorry we've let you down! Without ...               616   \n","6   @115822 I am unable to affect your account via...               NaN   \n","9               @115827 Thanks for your patience. ^KM               NaN   \n","10  @115826 I'm sorry for the wait. You'll receive...               627   \n","\n","   inbound_lang inbound_hashtags outbound_hashtags  \\\n","4            en               []                []   \n","5            en               []                []   \n","6            en               []                []   \n","9            en               []                []   \n","10           en               []                []   \n","\n","                                   clean_inbound_text  \\\n","4   different people have given different answers ...   \n","5   way to drop the ball on customer service so pi...   \n","6   i want my amazon payments account closed dm me...   \n","9   yeah this is crazy were less than a week away ...   \n","10  how about you guys figure out my xbox one x pr...   \n","\n","                                  clean_outbound_text  \\\n","4   wed like to take a further look into this with...   \n","5   i am sorry we have let you down without provid...   \n","6   i am unable to affect your account via twitter...   \n","9                         thanks for your patience km   \n","10  i am sorry for the wait you will receive an em...   \n","\n","                                  outbound_tokens_pos  \\\n","4   [-PRON-: NOUN, d: VERB, like: VERB, to: NOUN, ...   \n","5   [i: NOUN, be: NOUN, sorry: NOUN, -PRON-: NOUN,...   \n","6   [i: NOUN, be: NOUN, unable: NOUN, to: NOUN, af...   \n","9   [thank: NOUN, for: NOUN, -PRON-: NOUN, patienc...   \n","10  [i: NOUN, be: NOUN, sorry: NOUN, for: NOUN, th...   \n","\n","                                   inbound_tokens_pos  \n","4   [different: NOUN, people: NOUN, have: NOUN, gi...  \n","5   [way: NOUN, to: NOUN, drop: VERB, the: NOUN, b...  \n","6   [i: NOUN, want: VERB, -PRON-: NOUN, amazon: NO...  \n","9   [yeah: NOUN, this: NOUN, be: NOUN, crazy: NOUN...  \n","10  [how: NOUN, about: NOUN, -PRON-: NOUN, guy: NO...  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_pickle(\"../data/processed/processed_v2.pkl\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T10:26:55.405308Z","iopub.status.busy":"2023-11-27T10:26:55.404943Z","iopub.status.idle":"2023-11-27T10:26:55.412448Z","shell.execute_reply":"2023-11-27T10:26:55.411127Z","shell.execute_reply.started":"2023-11-27T10:26:55.405273Z"},"trusted":true},"outputs":[],"source":["#constants\n","eng_stopwords = set(stopwords.words(\"english\"))\n","\n","#settings\n","warnings.filterwarnings(\"ignore\")\n","lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T10:26:59.509545Z","iopub.status.busy":"2023-11-27T10:26:59.508747Z","iopub.status.idle":"2023-11-27T10:27:26.431994Z","shell.execute_reply":"2023-11-27T10:27:26.430762Z","shell.execute_reply.started":"2023-11-27T10:26:59.509495Z"},"trusted":true},"outputs":[],"source":["%%time\n","# Reading in all the Twitter data\n","all_data = pd.read_csv('/kaggle/input/customer-support-on-twitter/twcs/twcs.csv')\n","\n","# Finding the distribution of all authors\n","count = all_data.groupby(\"author_id\")[\"text\"].count()\n","\n","# Showing only the authors that appear more than 15000 times\n","c = count[count>15000].plot(kind='bar',figsize=(15, 5), color='#00acee')\n","c.set_xlabel('')\n","c.set_ylabel('Frequency')\n","plt.title('Most of the Data is AmazonHelp')\n","\n","# Saving my plot\n","# plt.savefig('/kaggle/working/visualizations/authors.png')\n","\n","plt.show()\n","\n","# There are 108 unique authors that are companies and 702669 customers\n","authors_count = (len(all_data[all_data.inbound == False].author_id.unique()), len(all_data[all_data.inbound == True].author_id.unique()))\n","\n","# Visualizing it\n","plt.figure(figsize =(25,5))\n","x = ['Unique Companies','Unique Customers']\n","ax = plt.barh(x, authors_count, color = '#00acee')\n","plt.title('There are way more unique customers than unique companies')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Dictionary "]},{"cell_type":"markdown","metadata":{},"source":["## Data Dictionary:\n","* Tweet_id: The unique ID for this tweet\n","* Author_id: The unique ID for this tweet author (anonymized for non-company users)\n","* Inbound: Whether or not the tweet was sent (inbound) to a company\n","* Created_at: When the tweet was created\n","* Text: The text content of the tweet\n","* Response_tweet_id: The tweet that responded to this one, if any\n","* In_response_to_tweet_id: The tweet this tweet was in response to, if any"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:35:18.767512Z","iopub.status.busy":"2023-11-25T15:35:18.767064Z","iopub.status.idle":"2023-11-25T15:35:19.385241Z","shell.execute_reply":"2023-11-25T15:35:19.384340Z","shell.execute_reply.started":"2023-11-25T15:35:18.767476Z"},"trusted":true},"outputs":[],"source":["# All data shape\n","print('All data has shape {}'.format(all_data.shape))\n","\n","# Converting columns to the right datatypes\n","all_data['text'] = all_data['text'].astype('string')\n","all_data['author_id'] = all_data['author_id'].astype('string')\n","\n","# This function utilizes Panda's Style to helps me encode data type and \n","# magnitude information to my dataframe view\n","show = lambda x: x.tail(5).style.set_properties(**{'background-color': 'black',                                                   \n","                                    'color': 'lawngreen',                       \n","                                    'border-color': 'white'})\\\n",".applymap(lambda x: f\"color: {'lawngreen' if isinstance(x,str) else 'cyan'}\")\\\n",".background_gradient(cmap='Blues')\n","\n","show(all_data)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Wrangling \n","- **Requirements**: \n","    - **Inbound**: Since the first step I want to achieve is intent classification and to figure out what the customer is saying to the company, I only want data that goes from the customer to the company (inbound data).\n","    - **Just Amazon Customer Data**: My first modeling step is to do some sort of clustering to classify the intents for each of these tweets, doing so might be easier and more fruitful if I narrow the data to just the **amazon support data**. I can expand this later to other datasets and domains. \n","    \n","    - {{Later, I would like my chatbot to be able to respond to make *follow up* responses to my customers.}}: Investigate this "]},{"cell_type":"markdown","metadata":{},"source":["**Note** \n","- See if I can use some **topic modeling** based **similarity matching** technique to label the intents of other \n","- Any classification-based approach???"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:34:20.450513Z","iopub.status.busy":"2023-11-25T15:34:20.450142Z","iopub.status.idle":"2023-11-25T15:34:21.006877Z","shell.execute_reply":"2023-11-25T15:34:21.005768Z","shell.execute_reply.started":"2023-11-25T15:34:20.450485Z"},"trusted":true},"outputs":[],"source":["amazon_data = all_data[all_data[\"author_id\"]==\"AmazonHelp\"]\n","amazon_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["- Later, I would like my bot to make *follow-up* responses to my customers  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:33:17.975209Z","iopub.status.busy":"2023-11-25T15:33:17.974770Z","iopub.status.idle":"2023-11-25T15:33:18.063698Z","shell.execute_reply":"2023-11-25T15:33:18.062540Z","shell.execute_reply.started":"2023-11-25T15:33:17.975172Z"},"trusted":true},"outputs":[],"source":["amazon_data.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T03:05:46.114869Z","iopub.status.busy":"2023-11-25T03:05:46.114452Z","iopub.status.idle":"2023-11-25T03:05:46.124210Z","shell.execute_reply":"2023-11-25T03:05:46.122969Z","shell.execute_reply.started":"2023-11-25T03:05:46.114838Z"},"trusted":true},"outputs":[],"source":["amazon_data.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T06:59:19.275293Z","iopub.status.busy":"2023-11-24T06:59:19.274785Z","iopub.status.idle":"2023-11-24T06:59:19.286289Z","shell.execute_reply":"2023-11-24T06:59:19.284858Z","shell.execute_reply.started":"2023-11-24T06:59:19.275254Z"},"trusted":true},"outputs":[],"source":["amazon_data[\"created_at\"].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:34:25.657082Z","iopub.status.busy":"2023-11-25T15:34:25.656635Z","iopub.status.idle":"2023-11-25T15:34:29.762186Z","shell.execute_reply":"2023-11-25T15:34:29.761024Z","shell.execute_reply.started":"2023-11-25T15:34:25.657024Z"},"trusted":true},"outputs":[],"source":["# Change the data types of \"author_id\" and \"text\" to object, and \"created_at\" to datetime \n","amazon_data.loc[ : , [\"author_id\", \"text\"]] = amazon_data.loc[ : , [\"author_id\", \"text\"]].astype(\"object\")\n","amazon_data.loc[ : , \"created_at\"] = pd.to_datetime(amazon_data.loc[ : , \"created_at\"], format=\"%a %b %d %H:%M:%S %z %Y\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:55:36.089547Z","iopub.status.busy":"2023-11-24T11:55:36.089078Z","iopub.status.idle":"2023-11-24T11:55:36.100269Z","shell.execute_reply":"2023-11-24T11:55:36.099380Z","shell.execute_reply.started":"2023-11-24T11:55:36.089508Z"},"trusted":true},"outputs":[],"source":["amazon_data.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T07:56:23.199852Z","iopub.status.busy":"2023-11-24T07:56:23.199394Z","iopub.status.idle":"2023-11-24T07:56:23.208470Z","shell.execute_reply":"2023-11-24T07:56:23.207058Z","shell.execute_reply.started":"2023-11-24T07:56:23.199819Z"},"trusted":true},"outputs":[],"source":["# Shape of Amazon Data \n","amazon_data.shape"]},{"cell_type":"markdown","metadata":{},"source":["There are 169840 rows of Amazon data, which is 6 percent of the entire data. These are all data from Amazon to a customer, I want the data from the customer to Amazon for each of these Amazon customer service responses, and this can be found in the reponse_tweet_id.\n","\n","Now I drop the rows with a NaN value for the in response to tweet id column, meaning that the original customer inquiry was not available for that interaction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:34:29.764237Z","iopub.status.busy":"2023-11-25T15:34:29.763858Z","iopub.status.idle":"2023-11-25T15:34:29.786652Z","shell.execute_reply":"2023-11-25T15:34:29.785486Z","shell.execute_reply.started":"2023-11-25T15:34:29.764208Z"},"trusted":true},"outputs":[],"source":["# Inspect data samples where \"in_response_to_tweet_id\" is null\n","amazon_data[amazon_data[\"in_response_to_tweet_id\"].isnull()]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:34:32.888543Z","iopub.status.busy":"2023-11-25T15:34:32.888150Z","iopub.status.idle":"2023-11-25T15:34:32.925604Z","shell.execute_reply":"2023-11-25T15:34:32.924496Z","shell.execute_reply.started":"2023-11-25T15:34:32.888513Z"},"trusted":true},"outputs":[],"source":["amazon_data.dropna(subset = [\"in_response_to_tweet_id\"],inplace = True)\n","amazon_data[\"inbound\"].value_counts(), amazon_data.shape"]},{"cell_type":"markdown","metadata":{},"source":["**Observations** \n","- The above procedure let's go of 553 rows, maybe the origin inbound requests were deleted by the poster. Since I am interested in atleast single-level request response from the company, I deleted them since they won't serve me any good purpose "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:33:43.306836Z","iopub.status.busy":"2023-11-25T15:33:43.306422Z","iopub.status.idle":"2023-11-25T15:33:45.571688Z","shell.execute_reply":"2023-11-25T15:33:45.570294Z","shell.execute_reply.started":"2023-11-25T15:33:43.306805Z"},"trusted":true},"outputs":[],"source":["!pwd\n","!mkdir objects "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:34:36.768135Z","iopub.status.busy":"2023-11-25T15:34:36.766871Z","iopub.status.idle":"2023-11-25T15:34:38.079763Z","shell.execute_reply":"2023-11-25T15:34:38.078615Z","shell.execute_reply.started":"2023-11-25T15:34:36.768024Z"},"trusted":true},"outputs":[],"source":["# Converting the IDs to integers to match the tweet_id column for joining \n","amazon_data[\"in_response_to_tweet_id\"] = amazon_data[\"in_response_to_tweet_id\"].astype(\"int64\")\n","\n","# Doing an inner join to get the respective tweets back from all_data and get the response tweets \n","amazon_data = pd.merge(all_data[[\"tweet_id\", \"text\"]], amazon_data, left_on=\"tweet_id\", right_on=\"in_response_to_tweet_id\")\n","\n","# Dropping the columns that doesn't encode useful information and renaming the columns nicely for interpretability \n","amazon_data.drop([\"tweet_id_x\", \"tweet_id_y\", \"in_response_to_tweet_id\", \"inbound\"], axis=1, inplace=True)\n","\n","# Renaming the columns for interpretability \n","amazon_data.rename(columns={\"text_x\": \"inbound_text\", \"text_y\": \"outbound_text\"}, inplace=True)\n","\n","# Subsetting to inbound and outboudn text \n","text = amazon_data[[\"inbound_text\", \"outbound_text\"]]\n","\n","# Previewing \n","print(text.head())\n","\n","# Saving the real, raw inbound data\n","text.to_pickle(\"/kaggle/working/objects/raw_text.pkl\")\n","\n","inbound = text[\"inbound_text\"]\n","outbound = text[\"outbound_text\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:35:34.994277Z","iopub.status.busy":"2023-11-25T15:35:34.993794Z","iopub.status.idle":"2023-11-25T15:35:35.010591Z","shell.execute_reply":"2023-11-25T15:35:35.009354Z","shell.execute_reply.started":"2023-11-25T15:35:34.994240Z"},"trusted":true},"outputs":[],"source":["# Showing the results of data wrangling\n","show(amazon_data)"]},{"cell_type":"markdown","metadata":{},"source":["- Great. So now we are going to build upon this **amazon_data** from now on \n","- **Since I am working with unsupervised data for the first part of my chatbot, I do not need a train-test split. My test set would also just end up needing labels as well. For this reason, I will just subset my data to the inbound/outbound data without employing the concept of unseen data.** Pay great attention to this part "]},{"cell_type":"markdown","metadata":{},"source":["## Text Preprocessing Pipeline \n","- In this process, it's expected that I will find much edge cases, and I don't have to address them all. However, in the end, I felt like my prerprocessing pipeline is relatively complete given this dataset \n","- Also, this was an iterative process because I fit my models in the next notebook just to notice that my tweets still had '\\u200d', '#', '', and '@' even though I still did a lot of fitting already. These are just extra emojis and extra punctuation I decided at a later stage to remove, but in the end I just modified my pipeline function \n","\n","- Before that, I did a bit of EDA with bag of words to see what words are being used by the dataset to better preprocess the data and prevent working in too much granularity. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:35:41.051500Z","iopub.status.busy":"2023-11-25T15:35:41.051098Z","iopub.status.idle":"2023-11-25T15:35:41.057077Z","shell.execute_reply":"2023-11-25T15:35:41.055817Z","shell.execute_reply.started":"2023-11-25T15:35:41.051470Z"},"trusted":true},"outputs":[],"source":["# Import Count Vectorizer \n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:37:04.609225Z","iopub.status.busy":"2023-11-25T15:37:04.607960Z","iopub.status.idle":"2023-11-25T15:37:04.618348Z","shell.execute_reply":"2023-11-25T15:37:04.617348Z","shell.execute_reply.started":"2023-11-25T15:37:04.609178Z"},"trusted":true},"outputs":[],"source":["# Writing a function to plot the counts that are in the data \n","def top10_bag_of_words(data, output_name, title):\n","    \"\"\" Taking as input the data and plots the top 10 words based on counts in this text data \"\"\"\n","    bag_of_words = CountVectorizer()\n","    inbound = bag_of_words.fit_transform(data)\n","    \n","    # Output will be a sparse matrix \n","    word_counts = np.array(np.sum(inbound, axis=0)).reshape((-1, ))\n","    words = np.array(bag_of_words.get_feature_names_out())\n","    words_df = pd.DataFrame({\"word\": words, \n","                             \"count\": word_counts})\n","    words_rank = words_df.sort_values(by=\"count\", ascending=False)\n","    \n","    # Storing it in a CSV so that I can inspect it myself \n","    words_rank.to_csv(\"words_rank.csv\") # Run this for only the first time and then comment it out \n","    words_rank.head()\n","    \n","    # Visualizing top 10 words \n","    plt.figure(figsize=(12, 6))\n","    sns.barplot(x=words_rank[\"word\"][ :10], y=words_rank[\"count\"][ :10].astype(int), data=amazon_data, palette=\"inferno\")\n","    plt.title(title)\n","    \n","    # Save the figure \n","    # plt.savefig(f\"/kaggle/working/visualizations/{output_name}.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:37:06.314764Z","iopub.status.busy":"2023-11-25T15:37:06.314368Z","iopub.status.idle":"2023-11-25T15:37:12.782649Z","shell.execute_reply":"2023-11-25T15:37:12.781513Z","shell.execute_reply.started":"2023-11-25T15:37:06.314727Z"},"trusted":true},"outputs":[],"source":["top10_bag_of_words(inbound, \"most_common_before\" , \"Top 10 Most Common Words in My Inbound Data Before Preprocessing\")"]},{"cell_type":"markdown","metadata":{},"source":["We see that after preprocessing, we will remove the handles which is what applesupport and 115858 is in the visualization above. For now, I briefly visualized the top 10 most common tokens to show that my data as it currently is quite dirty, they will definitely skew and ruin my results if I don't do preprocessing!"]},{"cell_type":"markdown","metadata":{},"source":["### Emoji Analysis\n","You can leave emojis in and have model learn embeddings for the emojis. Emoji are used in very specific instances, if they use emojis it will be in a useful sense. If the emojis are used for inside jokes, then it's different story, but that is likely not the case for this dataset because these are customer service tweets.\n","\n","There's actually an embedding called emoji2vec, but that's something I can explore down the line. Right now, I think doc2vec won't be able to handle these emojis in the best way and it might skew my results if I leave them in.\n","\n","Extracted from [this kernel](https://www.kaggle.com/psbots/customer-support-meets-spacy-universe)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Analysis will go here "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Emoji Visualization goes here "]},{"cell_type":"markdown","metadata":{},"source":["## My Entire Preliminary Text Preprocessing Pipeline\n","\n","I wanted all the preprocessing to be an iterative process, so I compiled it all in one function so that it runs at the same time and hence there is an enhanced sense of organization. At every preprocessing step, I visualize the lengths of each tokens of the data. In general, things like removing stop-words will shift the distribution to the left because we have fewer and fewer tokens at every preprocessing step. I also provide a peek to the head of the data at each step so that it clearly shows what processing is being done at each step."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T10:29:57.000293Z","iopub.status.busy":"2023-11-27T10:29:56.999837Z","iopub.status.idle":"2023-11-27T10:29:57.024941Z","shell.execute_reply":"2023-11-27T10:29:57.023729Z","shell.execute_reply.started":"2023-11-27T10:29:57.000260Z"},"trusted":true},"outputs":[],"source":["# Building tokenizer goes here \n","# from spacy_cld import LanguageDetector\n","\n","### Variable storage\n","punct_base = ['?']\n","\n","# Punctuations I want to remove, including the empty token\n","puncts = ['\\u200d', '?', '....','..','...','','@','#', ',', '.', '\"', ':', ')', '(', '-', '!', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '*', '+', '\\\\', \n","    'â€¢', '~', 'Â£', 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',  'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', \n","    'Â½', 'Ã ', 'â€¦', 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', \n","    'â€”', 'â€¹', 'â”€', 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²', 'Ã¨', 'Â¸', 'Â¾', \n","    'Ãƒ', 'â‹…', 'â€˜', 'âˆž', 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', \n","    'Â¹', 'â‰¤', 'â€¡', 'âˆš', '!','ðŸ…°','ðŸ…±']\n","\n","# Using NLTK's stop words corpus\n","stopwords.words('english');\n","stop_words = set(stopwords.words('english')) \n","\n","# Found a dictionary of common contractions and colloquial language\n","contraction_colloq_dict = {\n","                           \"btw\": \"by the way\", \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n","                           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n","                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n","                           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n","                           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n","                           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n","                           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n","                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\", \"i'm\": \"i am\", \n","                           \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n","                           \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n","                           \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n","                           \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n","                           \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n","                           \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n","                           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n","                           \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n","                           \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n","                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n","                           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n","                           \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n","                           \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n","                           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \n","                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n","                           \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n","                           \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n","                           \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n","                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n","                           \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n","                           \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \n","                           \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n","                           \"you'd\": \"you would\", \"you'd've\": \"you would have\"\n","}\n","\n","# Initializing the lemmatizer \n","# Initializing spacy objects \n","# nlp_cld = spacy.load(\"en_core_web_md\", exclude=[\"tagger\", \"ner\"])\n","# lang_detector = spacy_cld.LanguageDetector()\n","# nlp_cld.add_pipe(lang_detector)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T14:17:34.760193Z","iopub.status.busy":"2023-11-27T14:17:34.758923Z","iopub.status.idle":"2023-11-27T14:17:34.769590Z","shell.execute_reply":"2023-11-27T14:17:34.768326Z","shell.execute_reply.started":"2023-11-27T14:17:34.760138Z"},"trusted":true},"outputs":[],"source":["import contractions as cm\n","cm.fix(\"Can't you believe how it's getting harder to find a decent place to eat? I've been searching for a good restaurant that doesn't cost an arm and a leg. It's not just about the food; it's also about the ambiance, you know? I'd love to try that new place, but it's always packed! I'm thinking we should probably make a reservation in advance. Don't you think so too?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T14:06:58.160499Z","iopub.status.busy":"2023-11-27T14:06:58.159926Z","iopub.status.idle":"2023-11-27T14:06:58.206429Z","shell.execute_reply":"2023-11-27T14:06:58.205013Z","shell.execute_reply.started":"2023-11-27T14:06:58.160454Z"},"trusted":true},"outputs":[],"source":["list(\"I am a boy with a big fat butt!!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T11:00:30.906214Z","iopub.status.busy":"2023-11-27T11:00:30.905656Z","iopub.status.idle":"2023-11-27T11:00:30.942077Z","shell.execute_reply":"2023-11-27T11:00:30.940884Z","shell.execute_reply.started":"2023-11-27T11:00:30.906172Z"},"trusted":true},"outputs":[],"source":["# My preprocessing functions (defining them here so that I could access them from anywhere in the notebook)\n","# Capture the hashtags and/or usertags \n","# Clean comment text \n","\n","tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n","### Take care of positional parameters \n","def clean_text(\n","        text, words=True, stops=True, urls=True, tags=True, \n","        newLine=True, ellipsis=True, special_chars=True, condensed=True, non_breaking_space=True, \n","        character_encodings=True, stopwords=True, only_words=True) -> str:\n","    \n","    \"\"\" Clean tweets after extracting all hashtags and username tags\n","    Not comprehensive enough to capture all idiosyncrasies, but works for most of the time\n","    \"\"\"\n","    \n","    # Capture only words and no numbers\n","    if words:\n","        pattern = r\"\\d\"\n","        text = re.sub(pattern, \"\", text)\n","        \n","    # Remove URLs \n","    if urls:\n","        pattern = \"(https\\:)*\\/*\\/*(www\\.)?(\\w+)(\\.\\w+)\\/*\\w*\"\n","        text = re.sub(pattern, \"\", text)\n","        \n","    # Remove tags \n","    if tags:\n","        text = re.sub(\"@\\w+\", \"\", text)\n","        \n","    # Replacing one or more occurrences of '\\n' with ''\n","    # Replacing multiple occurrences, i.e., >=2 occurrences with '.'\n","    if newLine:\n","        text = re.sub(\"\\n+\", \"\", text)\n","        text = re.sub(r'\\.\\s+', '.', text)\n","        \n","    # Fix contractions\n","    if condensed:\n","        try:\n","            text = cm.fix(text)\n","        except: \n","            print(text)\n","        \n","    # Remove non-breaking space \n","    if non_breaking_space: \n","        pattern = r\"(\\xa0|&nbsp)\"\n","        text = re.sub(pattern, \"\", text)\n","        \n","    # Remove stopwords\n","    if stopwords:\n","        text = text.lower()\n","        # print(f\"Original Shape of the Data is {.shape}\")\n","        \n","        # Splitting with NLTK's Tweet tokenizer. This limits repeated characters to \n","        # three with the reduce lens parameter and strips all the \"@'s\". It also splits \n","        # it into 1-gram tokens         \n","        words = tokenizer.tokenize(text)\n","        filtered_words = [word for word in words if word not in eng_stopwords]\n","        text = \" \".join(words)\n","        text = text.strip()  # Add further checks for cleaning \n","    \n","    return text\n","\n","    # Only words\n","#     if only_words:\n","#         text = re.sub(r\"[^\\w\\n\\.]+\", \" \", text)\n","#         text = text.strip()\n","        \n","\n","def extract_hashtags(text: str) -> List[str]:\n","    \"\"\" Returns all Twitter hashtags from the text\"\"\"\n","    hashtags_ls = re.findall(\"#\\w+\", text)\n","    return list(set(hashtags_ls))\n","\n","# Function to detect language using langdetect\n","## Check if I have to perform sentence level tokenization first \n","def detect_language(text: str) -> str:\n","    \"\"\"\n","    Detect the language of the given text using langdetect library.\n","\n","    Parameters:\n","    - text (str): The input text for language detection.\n","\n","    Returns:\n","    - str: The detected language code (e.g., 'en' for English).\n","           If language detection fails, returns 'unknown'.\n","    \"\"\"\n","    try:\n","        # Attempt to detect the language using langdetect\n","        return detect(text)[\"lang\"]\n","    \n","    except Exception:\n","        # Return 'unknown' if language detection fails\n","        return \"unknown\"\n","\n","### Apply the custom TWEET Tokenizer if required. Check for it \n","# End-to-end tokenizer function \n","def tokenize_lemmatize(text: str) -> List[str]: \n","    ''' I am making my own end-to-end tokenizer'''\n","    \n","    # Convert all to lower case \n","    data = data.lower()\n","    print(f\"Original Shape of the Data is {data.shape}\")\n","    \n","    # Splitting with NLTK's Tweet tokenizer. This limits repeated characters to \n","    # three with the reduce lens parameter and strips all the \"@'s\". It also splits \n","    # it into 1-gram tokens \n","    doc = nlp(text)\n","    \n","    # Extract lemmatized text and store it back into a string \n","    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n","    return lemmatized_text \n","    \n","    \n","def remove_items(tokens: List[str], items_to_remove: List[str]) -> List[str]:\n","    \"\"\"\n","    This function removes a list of items from another list.\n","\n","    Parameters:\n","    original_list (list): The list from which items are to be removed.\n","    items_to_remove (list): The list of items to be removed from the original list.\n","\n","    Returns:\n","    list: A new list that contains only the items from the original list that are not in the items to remove.\n","    \"\"\"\n","    \n","    # Use list comprehension to create a new list that contains only the items from the original list\n","    # that are not in the items to remove\n","    return [item for item in original_list if item not in items_to_remove]\n","\n","\n","def correct_spellings(tokens: List[str]) -> List[str]:\n","    \"\"\"\n","    This function takes as input a list of words and outputs a list of the corrected spelling for each word.\n","\n","    Parameters:\n","    tokens (List[str]): The list of words for which the spelling is to be corrected.\n","\n","    Returns:\n","    List[str]: A list of corrected words.\n","    \"\"\"\n","    \n","    # Initialize an empty list to store the corrected words\n","    corrected_text = []\n","\n","    # Iterate over each word in the input list\n","    for word in x:\n","        \n","        # Append the corrected word to the corrected_text list\n","        corrected_text.append(spell.correction(word))\n","    \n","    # Return the list of corrected words\n","    return corrected_text\n","\n","\n","def get_wordnet_pos(word: str):\n","    \"\"\"\n","    This function determines the WordNet part of speech (POS) tag for a given word.\n","    This is often used in the process of lemmatization, where words are reduced to their base or root form.\n","\n","    Parameters:\n","    word (str): The word for which the POS tag is to be determined.\n","\n","    Returns:\n","    str: The WordNet POS tag for the input word.\n","    \"\"\"\n","    \n","    # Use NLTK's pos_tag function to determine the part of speech of the word\n","    # pos_tag returns a list of tuples, where each tuple contains a word and its POS tag\n","    # We're only interested in the POS tag of the first word, so we use [0][1][0] to extract the first character of the POS tag\n","    # The upper() function is used to convert this character to uppercase\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","\n","    # This dictionary maps the first character of the POS tag to a corresponding WordNet POS tag\n","    # The keys \"J\", \"N\", \"V\", and \"R\" represent adjective, noun, verb, and adverb, respectively\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","\n","    # Return the WordNet POS tag corresponding to the input word's POS tag\n","    # If the first character of the POS tag is not in tag_dict, default to wordnet.NOUN\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","def lemmatize_list(tokens: List[str]) -> List[str]:\n","    '''This lemmatizer function should work on a single list of tokenized data'''\n","    # Returning a list again\n","    return [(lemmatizer.lemmatize(token), get_wordnet_pos(token)) for token in tokens]\n","\n","def extract_emojis(): \n","    \"\"\"Details to be added later\"\"\"\n","    pass \n","\n","def limit_length(tokens: List[str], min_tokens: int, max_tokens: int) -> float: \n","    ''' Inputs a list and drops it out of the document if \n","        the document has more than the max and less than the min'''\n","    output = x\n","    if len(x) <= min_tokens:\n","        output = np.nan\n","        \n","    if len(x) > max_tokens:\n","        output = np.nan\n","    return output\n","\n","def remove_from_list(original_list: List[str], items_to_remove: List[str]): \n","    \"\"\"\n","    Remove a list of items from another list. \n","    \n","    This function takes two lists as input: `original_list` and `items_to_remove`.\n","    It returns a new list that contains all items from `original_list` that are not in `items_to_remove`.\n","    \n","    The function uses a set for `items_to_remove` to optimize the removal operation.\n","    \n","    Parameters: \n","    original_list (list): The original list \n","    items_to_remove (list): The list of items to remove from the original list \n","    \n","    Returns: \n","    list: A new list with the items removed. \n","\n","    \"\"\"\n","    # Convert items_to_remove to a set for faster lookup\n","    items_to_remove_set = set(items_to_remove)\n","    \n","    # Use a list comprehension to create a new list that includes only the items from original_list\n","    # that are not in items_to_remove_set\n","    return [item for item in original_list if item not in items_to_remove_set]\n","\n","\n","def visualize_lengths(data: list, title: str) -> None:\n","    '''Visualizing lengths of tokens in each tweet'''\n","    lengths = [len(i) for i in data]\n","    plt.figure(figsize=(13, 6))\n","    plt.hist(lengths, bins=40)\n","    plt.title(title)\n","    plt.show()\n","\n","def validate(): \n","    # Keep function validation out of scope for the time being \n","    pass \n","\n","\n","#     # Exapanding contractions and colloquial language \n","#     text = text.progress_apply(replace_from_dict, dic=contraction_colloq_dict)\n","#     print(f\"Expanded contractions into extra tokens. Shape is still {text.shape} .\\n \\n Peek: \\n {text.head()}\")\n","#     visualize_lengths(text)\n","    \n","#     # Removing non-English tweets with Spacy \n","#     text = text[only_english(text)]\n","#     print(f\"Remove all non-English tweets. Shape is {text.shape}. Clearly less than before! \\n \\n Peek: \\n {data.head()}\")\n","    \n","#     # Lemmatization \n","#     text = text.progress_apply(lemmatize_list) \n","#     print(f\"Lemmatized the tokens. Shape is still {text.shape}. \\n \\n Peek: \\n {text.head()}\")\n","#     visualize_lengths(text, \"Length of tokens after step 8\")\n","    \n","#     # Remove again to make sure I get everything \n","#     text = text.progress_apply(remove_from_list, stuff_to_remove=puncts)\n","    \n","#     # Using progress bar to show the progress bar \n","#     data = data.progress_apply(tokenizer.tokenize)\n","    \n","#     # Limiting length of tweets \n","#     max_tokens = 50 \n","#     min_tokens = 5 \n","#     data = data.progress_apply(limit_length, min_tokens=min_tokens, max_tokens=max_tokens)\n","#     # Dropping all NaN values, which are the token limits that didn't meet the thresholding requirements \n","#     data = da"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T11:00:37.951509Z","iopub.status.busy":"2023-11-27T11:00:37.951078Z","iopub.status.idle":"2023-11-27T11:00:37.960552Z","shell.execute_reply":"2023-11-27T11:00:37.959410Z","shell.execute_reply.started":"2023-11-27T11:00:37.951472Z"},"trusted":true},"outputs":[],"source":["clean_text(\"Hey @user123, great meeting you at the #TechConference2023! Can't believe we discussed over 100 new ideas! #InnovationOverload #TechIsLife'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T15:52:16.198258Z","iopub.status.busy":"2023-11-25T15:52:16.197794Z","iopub.status.idle":"2023-11-25T15:52:16.350686Z","shell.execute_reply":"2023-11-25T15:52:16.349173Z","shell.execute_reply.started":"2023-11-25T15:52:16.198221Z"},"trusted":true},"outputs":[],"source":["print(type(wordnet.ADJ))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T16:56:08.730147Z","iopub.status.busy":"2023-11-25T16:56:08.729685Z","iopub.status.idle":"2023-11-25T16:56:12.420691Z","shell.execute_reply":"2023-11-25T16:56:12.419478Z","shell.execute_reply.started":"2023-11-25T16:56:08.730108Z"},"trusted":true},"outputs":[],"source":["amazon_data[\"inbound_lang\"] = amazon_data[\"inbound_text\"].apply(detect_language)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T17:08:25.138456Z","iopub.status.busy":"2023-11-25T17:08:25.137562Z","iopub.status.idle":"2023-11-25T17:08:25.175384Z","shell.execute_reply":"2023-11-25T17:08:25.174239Z","shell.execute_reply.started":"2023-11-25T17:08:25.138415Z"},"trusted":true},"outputs":[],"source":["len(list(amazon_data[\"inbound_text\"])[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T17:07:30.059182Z","iopub.status.busy":"2023-11-25T17:07:30.058735Z","iopub.status.idle":"2023-11-25T17:07:31.184540Z","shell.execute_reply":"2023-11-25T17:07:31.183702Z","shell.execute_reply.started":"2023-11-25T17:07:30.059146Z"},"trusted":true},"outputs":[],"source":["amazon_data = amazon_data[amazon_data[\"inbound_lang\"]==\"en\"]\n","# Visualize data lengths after first pass \n","visualize_lengths(list(amazon_data[\"inbound_text\"]), \"Lengths post inbound processing\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T16:46:05.954163Z","iopub.status.busy":"2023-11-25T16:46:05.953719Z","iopub.status.idle":"2023-11-25T16:46:05.980302Z","shell.execute_reply":"2023-11-25T16:46:05.978932Z","shell.execute_reply.started":"2023-11-25T16:46:05.954119Z"},"trusted":true},"outputs":[],"source":["amazon_data[\"inbound_hashtags\"] = amazon_data[\"inbound_text\"].progress_apply(extract_hashtags)\n","amazon_data[\"outbound_hashtags\"] = amazon_data[\"outbound_text\"].progress_apply(extract_hashtags)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["amazon_data.to_picke(\"\")"]},{"cell_type":"markdown","metadata":{},"source":["**Observations**\n","- Okay, if I filter for any English rows, the follow-up responses besides the given \"inbound_text\" and \"outbound_text\" will also be in English \n","- After the process, I only have 122340 rows left "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# end-to-end \n","amazon_data[\"hashtags\"] = amazon_data[\"\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T15:33:25.565465Z","iopub.status.busy":"2023-11-24T15:33:25.564879Z","iopub.status.idle":"2023-11-24T15:33:25.662730Z","shell.execute_reply":"2023-11-24T15:33:25.661144Z","shell.execute_reply.started":"2023-11-24T15:33:25.565424Z"},"trusted":true},"outputs":[],"source":["from spellchecker import SpellChecker"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T17:32:54.947811Z","iopub.status.busy":"2023-11-25T17:32:54.947349Z","iopub.status.idle":"2023-11-25T17:32:55.567837Z","shell.execute_reply":"2023-11-25T17:32:55.566863Z","shell.execute_reply.started":"2023-11-25T17:32:54.947774Z"},"trusted":true},"outputs":[],"source":["# Saving all my results\n","with open('proceesed_v1.pkl', 'wb') as handle:\n","    pickle.dump(amazon_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T10:27:56.267913Z","iopub.status.busy":"2023-11-27T10:27:56.267485Z","iopub.status.idle":"2023-11-27T10:27:57.308448Z","shell.execute_reply":"2023-11-27T10:27:57.307244Z","shell.execute_reply.started":"2023-11-27T10:27:56.267880Z"},"trusted":true},"outputs":[],"source":["amazon_data = pd.read_pickle(\"/kaggle/input/working-data/proceesed_v1.pkl\")\n","amazon_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T10:35:08.325970Z","iopub.status.busy":"2023-11-27T10:35:08.325516Z","iopub.status.idle":"2023-11-27T10:36:17.599178Z","shell.execute_reply":"2023-11-27T10:36:17.598017Z","shell.execute_reply.started":"2023-11-27T10:35:08.325931Z"},"trusted":true},"outputs":[],"source":["# Applying data cleaning \n","amazon_data[\"cleaned_inbound_text\"] = amazon_data[\"inbound_text\"].progress_apply(clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-27T10:36:42.673823Z","iopub.status.busy":"2023-11-27T10:36:42.673422Z","iopub.status.idle":"2023-11-27T10:36:42.694696Z","shell.execute_reply":"2023-11-27T10:36:42.693452Z","shell.execute_reply.started":"2023-11-27T10:36:42.673792Z"},"trusted":true},"outputs":[],"source":["amazon_data.head()"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Unit Testing for preprocessing pipeline "]},{"cell_type":"markdown","metadata":{},"source":["# Length of Words \n","# "]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4133,"sourceId":8841,"sourceType":"datasetVersion"},{"datasetId":4067285,"sourceId":7064202,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":".venv","language":"python","name":".venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
