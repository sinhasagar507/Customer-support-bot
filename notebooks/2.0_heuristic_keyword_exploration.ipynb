{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Approach: Heuristic Intent Distribution Exploration\n",
    "I need to get an insight into the true intents inside my Twitter data. Doing it by keyword might prove to be a good baseline way to do this. \n",
    "\n",
    "\n",
    "Using keywords as a starting point might offer a strong foundational approach for this task. I'm building upon this concept by employing heuristic clustering to organize my intents, aiming to minimize overlaps between them. The goal is to distill a set of intents that are both distinct and exclusive, enhancing Eve bot's ability to differentiate between them.\n",
    "\n",
    "I drew inspiration from observing other solutions, particularly the \"semantic fingerprint\" concept implemented by cortex, although the specific workings were undisclosed. This prompted me to devise my approach. Initially, I considered branching off into manual selection of 1000 examples, but I realized this was impractical and overly laborious.\n",
    "\n",
    "This notebook serves as a strategy for generating training data for intent classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(f'pandas: {pd.__version__}')\n",
    "import numpy as np\n",
    "print(f'numpy: {np.__version__}')\n",
    "# Visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "# Making my visualizations pretty\n",
    "sns.set_style('whitegrid')\n",
    "# Combination exploration\n",
    "import itertools\n",
    "import yaml\n",
    "\n",
    "# Loading back processed data\n",
    "processed = pd.read_pickle('objects/processed.pkl') # Load data from 1.0_ipynb file \n",
    "print(f'\\ninbound:\\n{processed.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Keyword Search EDA\n",
    "Using this as a tool to look at Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search by keywords (single keyword filter)\n",
    "keyword = 'info'\n",
    "\n",
    "# Seeing what the processed Tweets look like\n",
    "filt = [(i, comment_txt) for i, comment_txt in enumerate(processed[\"Processed Inbound\"]) if keyword in comment_txt]\n",
    "filtered = processed.iloc[[i[0] for i in filt]]\n",
    "print(f'{len(filtered)} Tweets contain the keyword {keyword}')\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # code script for donwloading table data from wikipedia page\n",
    "\n",
    "def calculate_indirect_features(df, eng_stopwords):\n",
    "    \"\"\"\n",
    "    Calculate indirect features for a DataFrame based on the 'comment_text' column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data.\n",
    "    eng_stopwords (set): Set of English stopwords.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): DataFrame with the new features added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sentence count in each comment\n",
    "    df['count_sent'] = df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\", str(x))) + 1)\n",
    "\n",
    "    # Word count in each comment\n",
    "    df['count_word'] = df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    # Unique word count\n",
    "    df['count_unique_word'] = df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Letter count\n",
    "    df['count_letters'] = df[\"comment_text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "    # Punctuation count\n",
    "    df[\"count_punctuations\"] = df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "    # Upper case words count\n",
    "    df[\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "    # Title case words count\n",
    "    df[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "    # Number of stopwords\n",
    "    df[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "    # Average length of the words\n",
    "    df[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-Hoc Intents: I find the keywords that is associated with a particular intent and search based on these keywords \n",
    "\n",
    "# Version 1: Initial Iteration: Make a dictionary to store intents and the predefined responses \n",
    "\n",
    "intents = [\"Greeting\": [\"Hi there!\", \"Hello\"], \n",
    "           \"Closing\": \"Thanks for talking\", \n",
    "           \"Promotion\": \"\", \n",
    "           \"Complaint\": \"\",\n",
    "            \"Scenarios\": {\"Last Payment\": \"\", \"Account Details\": \"\", \n",
    "                          \"Account Confirmation: \"\"}, \n",
    "            \"Location\": \"\"}\n",
    "          ]\n",
    "\n",
    "# Version-2: For showing progress \n",
    "intents = {'greeting': ['hi', 'hello', 'hey','yo'], 'app': ['app', 'application'],\n",
    "          'iphone': ['iphone', 'i phone'], 'icloud': ['icloud', 'i cloud'],\n",
    "          'ios': ['io'], 'battery': ['battery'], 'watch': ['watch'], 'mac': \n",
    "           ['mac', 'macbook', 'laptop', 'computer'], 'update': ['update'],\n",
    "          'troubleshooting': ['problem', 'trouble', 'error'],\n",
    "          'settings': ['settings', 'setting'], 'music': ['music', 'song', 'playlist'],\n",
    "          'payment': ['credit','card','payment','pay'], 'bug':['bug'], 'watch': ['tv', 'show'],\n",
    "          'network': ['internet','connection','network']}\n",
    "          \n",
    "# Intents that require all words within it to be contained in the list (alternative filtering method)\n",
    "intents_all = {'ios update': ['io', 'update'], 'app update': ['app','update']}\n",
    "\n",
    "# Version 3\n",
    "intents = {'update': ['update'], 'battery': ['battery', 'power'], 'forgot_password':['password','account','login'],\n",
    "          'repair':['repair','fix','broken'],  \n",
    "           'payment': ['credit','card','payment','pay']}\n",
    "\n",
    "# Storing it to YAML file\n",
    "with open('objects/intents.yml', 'w') as outfile:\n",
    "    yaml.dump(intents, outfile, default_flow_style=False)\n",
    "\n",
    "print('INTENTS FOR KEYWORD EDA BELOW:\\n ------------------------')\n",
    "for i in intents.items():\n",
    "    print('Intent: {} \\n   Keywords: {}'.format(i[0], \" + \".join(i[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions for Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_tweets(series, keywords): \n",
    "    '''\n",
    "    Takes as input the list of keywords and outputs the Tweets that contains at least\n",
    "    one of these keywords\n",
    "    '''\n",
    "    keyword_tweets = []\n",
    "    for tweet in series:\n",
    "        for word in keywords:\n",
    "            if word in tweet:\n",
    "                keyword_tweets.append(tweet)\n",
    "    return keyword_tweets\n",
    "\n",
    "def get_key_tweets_all(series, keywords):\n",
    "    '''\n",
    "    Takes as input the list of keywords and outputs the Tweets that contains all\n",
    "    of these keywords\n",
    "    '''\n",
    "    keyword_tweets = []\n",
    "    for tweet in series:\n",
    "        if all(word in tweet for word in keywords):\n",
    "            keyword_tweets.append(tweet)\n",
    "    return keyword_tweets"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
