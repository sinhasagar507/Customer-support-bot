{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(string_data, max_epochs, vec_size, alpha):\n",
    "     \n",
    "    # Tagging each of the documents with a unique ID\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(string_data)]\n",
    "    \n",
    "    # Instantiating my model \n",
    "    model = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1) # dm=1 means 'distributed memory' (PV-DM)\n",
    "    \n",
    "    # Building the vocabulary table\n",
    "    model.build_vocab(tagged_data)\n",
    "    \n",
    "    for epoch in range(max_epochs): # Run for max_epochs\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs) # This statement trains the model on the current epoch\n",
    "        # Decreasing the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # Fixing the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "        \n",
    "    # Saving model\n",
    "    model.save(\"models/d2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "# Training\n",
    "train_doc2vec(processed_inbound_extra, max_epochs = 100, vec_size = 20, alpha = 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a synthetic dataset - generating N Tweets resembling a mock tweet\n",
    "# This will subsequently be merged with the existing inbound data for inclusion in the doc2vec training process\n",
    "\n",
    "# Version 1 - will be improved in future iterations\n",
    "ideal = {\n",
    "            \"order track\": \"@AmazonHelo Hi, could you provide an update on the order? Its been days since the product has moved from its last location \", # change intent to \"order tracking\"???\n",
    "            \"product inquiry\": \"@AmazonHelp Looking for more info on the product. Can you share details or direct me to a reliable source?\", # product inquire??? \n",
    "            \"return refund\": \"@AmazonHelp How can I start a return process? The item I received doesn't match the description.\",\n",
    "            \"account management\": \"@AmazonHelp Hi, I am having trouble logging into my account. Can you help me reset my password?\", \n",
    "            \"promotion discount\": \"@AmazonHelp Are there any ongoing promotions or deals in the ongoing festive season? Looking to buy a few items.\",\n",
    "            \"shipping\": \"@AmazonHelp Hi, My address has changed. Can you help me update the shipping address for my order?\",\n",
    "            \"technical support\": \"@AmazonHelp Encountering errors during checkout. Can you help me troubleshoot the issue?\",\n",
    "            \"payment issue\": \"@AmazonHelp My payment method isn't going through. Any suggestions on how to resolve this?\",\n",
    "            \"general query\": \"@AmazonHelp Hi, I have a general question regarding the product. Can you help me with this?\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra(current_tokenized_data, extra_tweets):\n",
    "    '''Adding extra tweets to the current tokenized data'''\n",
    "    \n",
    "    # Convert the extra tweets into a pandas Series\n",
    "    extra_tweets = pd.Series(extra_tweets)\n",
    "\n",
    "    # Convert the current tokenized data into a single string\n",
    "    print('Converting to string...')\n",
    "    string_processed_data = current_tokenized_data.progress_apply(\" \".join)\n",
    "\n",
    "    # Concatenate the extra tweets to the current data\n",
    "    string_processed_data = pd.concat([string_processed_data, extra_tweets], axis = 0)\n",
    "\n",
    "    # Tokenize the combined data\n",
    "    tknzr = TweetTokenizer(strip_handles = True, reduce_len = True)\n",
    "    print('Tokenizing...')\n",
    "    tokenized_data = string_processed_data.progress_apply(tknzr.tokenize)\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "# Add the extra tweets to the current data\n",
    "processed_inbound_extra = add_extra(processed['Processed Inbound'], list(ideal.values()))\n",
    "\n",
    "# Save the updated data to a pickle file\n",
    "processed_inbound_extra.to_pickle('objects/processed_inbound_extra.pkl')\n",
    "\n",
    "processed_inbound_extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's is the catch. Have intent buckets inplace already in place. You would need to supply this to the below function to get your top N tweets corresponding to the current tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping the intent to the row index \n",
    "intent_itags = {\n",
    "\n",
    "}\n",
    "\n",
    "def generate_intent(nsim, idx_tag): \n",
    "    '''Function that maps an index tag to an intent and returns nsim number of similar tweets'''\n",
    "    sim_docs = model.docvecs.most_similar(idx_tag, topn = nsim)\n",
    "    \n",
    "    # Getting just the indexes \n",
    "    indexes = [int(i[0]) for i in sim_docs]\n",
    "    \n",
    "    # Actually seeing the top 1000 tweets similar to 0th tweet which seems to be about updates \n",
    "    # print(processed_inbound_extra[indexes])\n",
    "    return indexes\n",
    "    \n",
    "# Create a dictionary mapping the intent to the row index of tweets\n",
    "index_intents = {}\n",
    "for intent, tag in intent_itags.items():\n",
    "    print('Intent: ', intent)\n",
    "    index_indents[intent] = generate_intent(1000, tag)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now map the index to each row of the preprocessed inbound data\n",
    "preprocessed_inbound[\"intent\"] = processed_inbounnd.index.map(index_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent classification with Keras \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
