{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from numpy import triu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     different people have given different answers ...\n",
       "5     way to drop the ball on customer service so pi...\n",
       "6     i want my amazon payments account closed dm me...\n",
       "9     yeah this is crazy were less than a week away ...\n",
       "10    how about you guys figure out my xbox one x pr...\n",
       "Name: clean_inbound_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the processed data and the processed inbound dataset \n",
    "processed_df = pd.read_pickle(\"../data/processed/processed_v2.pkl\")\n",
    "processed_inbound_extra = processed_df[\"clean_inbound_text\"]\n",
    "processed_inbound_extra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index. Set it properly in this iteration \n",
    "processed_df = processed_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inbound_text               0\n",
       "author_id                  0\n",
       "created_at                 0\n",
       "outbound_text              0\n",
       "response_tweet_id      60327\n",
       "inbound_lang               0\n",
       "inbound_hashtags           0\n",
       "outbound_hashtags          0\n",
       "clean_inbound_text         0\n",
       "clean_outbound_text        0\n",
       "outbound_tokens_pos        0\n",
       "inbound_tokens_pos         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for some null values \n",
    "processed_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "intents:\n",
      "{'track': ['tracking', 'order', 'shipment', 'late', 'status', 'carrier', 'update', 'number', 'info', 'received', 'details'], 'support': ['service'], 'quality': ['quality', 'product', 'damaged', 'received', 'refund', 'return', 'issue', 'order', 'packaging', 'proper', 'working', 'expected', 'different'], 'discount': ['prime', 'product', 'offer', 'price', 'sale'], 'account': ['email', 'orders', 'details', 'bank', 'access']}\n",
      "\n",
      "processed:\n",
      "                                        inbound_text   author_id  \\\n",
      "0  @AmazonHelp 3 different people have given 3 di...  AmazonHelp   \n",
      "1  Way to drop the ball on customer service @1158...  AmazonHelp   \n",
      "2  @115823 I want my amazon payments account CLOS...  AmazonHelp   \n",
      "3  @AmazonHelp @115826 Yeah this is crazy weâ€™re l...  AmazonHelp   \n",
      "4  @115828 How about you guys figure out my Xbox ...  AmazonHelp   \n",
      "\n",
      "                  created_at  \\\n",
      "0  2017-10-31 23:28:00+00:00   \n",
      "1  2017-10-31 22:29:00+00:00   \n",
      "2  2017-10-31 22:28:34+00:00   \n",
      "3  2017-11-01 12:53:34+00:00   \n",
      "4  2017-10-31 22:28:00+00:00   \n",
      "\n",
      "                                       outbound_text response_tweet_id  \\\n",
      "0  @115820 We'd like to take a further look into ...               619   \n",
      "1  @115820 I'm sorry we've let you down! Without ...               616   \n",
      "2  @115822 I am unable to affect your account via...               NaN   \n",
      "3              @115827 Thanks for your patience. ^KM               NaN   \n",
      "4  @115826 I'm sorry for the wait. You'll receive...               627   \n",
      "\n",
      "  inbound_lang inbound_hashtags outbound_hashtags  \\\n",
      "0           en               []                []   \n",
      "1           en               []                []   \n",
      "2           en               []                []   \n",
      "3           en               []                []   \n",
      "4           en               []                []   \n",
      "\n",
      "                                  clean_inbound_text  \\\n",
      "0  different people have given different answers ...   \n",
      "1  way to drop the ball on customer service so pi...   \n",
      "2  i want my amazon payments account closed dm me...   \n",
      "3  yeah this is crazy were less than a week away ...   \n",
      "4  how about you guys figure out my xbox one x pr...   \n",
      "\n",
      "                                 clean_outbound_text  \\\n",
      "0  wed like to take a further look into this with...   \n",
      "1  i am sorry we have let you down without provid...   \n",
      "2  i am unable to affect your account via twitter...   \n",
      "3                        thanks for your patience km   \n",
      "4  i am sorry for the wait you will receive an em...   \n",
      "\n",
      "                                 outbound_tokens_pos  \\\n",
      "0  [-PRON-: NOUN, d: VERB, like: VERB, to: NOUN, ...   \n",
      "1  [i: NOUN, be: NOUN, sorry: NOUN, -PRON-: NOUN,...   \n",
      "2  [i: NOUN, be: NOUN, unable: NOUN, to: NOUN, af...   \n",
      "3  [thank: NOUN, for: NOUN, -PRON-: NOUN, patienc...   \n",
      "4  [i: NOUN, be: NOUN, sorry: NOUN, for: NOUN, th...   \n",
      "\n",
      "                                  inbound_tokens_pos  \n",
      "0  [different: NOUN, people: NOUN, have: NOUN, gi...  \n",
      "1  [way: NOUN, to: NOUN, drop: VERB, the: NOUN, b...  \n",
      "2  [i: NOUN, want: VERB, -PRON-: NOUN, amazon: NO...  \n",
      "3  [yeah: NOUN, this: NOUN, be: NOUN, crazy: NOUN...  \n",
      "4  [how: NOUN, about: NOUN, -PRON-: NOUN, guy: NO...  \n"
     ]
    }
   ],
   "source": [
    "# Read in the intents back \n",
    "with open(r\"../objects/intents_amazon_support.yml\") as file:\n",
    "    intents = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Previewing\n",
    "print(f'\\nintents:\\n{intents}')\n",
    "print(f'\\nprocessed:\\n{processed_df.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Collection with Doc2Vec \n",
    "I can use my Doc2Vec representation to find top 1000 Tweets most similar to a generalized intent version of a Tweet based on it's cosine similarity. \n",
    "\n",
    "Heuristic search refers to a search strategy that attempts to optimize a problem by iteratively improving the solution based on a given heuristic function or a cost measure. My cost measure is trying to get the closest cosine distances.\n",
    "\n",
    "So I basically trained my doc2vec model with my training data, which is the `processed_inbound`. I can actually compute a vector based on my training data to vectorize that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "def train_doc2vec(string_data, max_epochs, vec_size, alpha):\n",
    "     \n",
    "    # Tagging each of the documents with a unique ID\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(string_data)]\n",
    "    \n",
    "    # Instantiating my model \n",
    "    model = Doc2Vec(vector_size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1) # dm=1 means 'distributed memory' (PV-DM)\n",
    "    \n",
    "    # Building the vocabulary table\n",
    "    model.build_vocab(tagged_data)\n",
    "    \n",
    "    for epoch in range(max_epochs): # Run for max_epochs\n",
    "        print('iteration {0}'.format(epoch))    \n",
    "        model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs) # This statement trains the model on the current epoch\n",
    "        # Decreasing the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # Fixing the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "        \n",
    "    # Saving model\n",
    "    model.save(\"../models/d2v.model\")\n",
    "    print(\"Model Saved\")        \n",
    "    \n",
    "# Training\n",
    "train_doc2vec(processed_inbound_extra, max_epochs=100, vec_size=20, alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a synthetic dataset - generating N Tweets resembling a mock tweet\n",
    "# This will subsequently be merged with the existing inbound data for inclusion in the doc2vec training process\n",
    "\n",
    "# Version 1 - will be improved in future iterations\n",
    "ideal = {\n",
    "            \"order track\": \"@AmazonHelo Hi, could you provide an update on the order? Its been days since the product has moved from its last location \", # change intent to \"order tracking\"???\n",
    "            \"product inquiry\": \"@AmazonHelp Looking for more info on the product. Can you share details or direct me to a reliable source?\", # product inquire??? \n",
    "            \"return refund\": \"@AmazonHelp How can I start a return process? The item I received doesn't match the description.\",\n",
    "            \"account management\": \"@AmazonHelp Hi, I am having trouble logging into my account. Can you help me reset my password?\", \n",
    "            \"promotion discount\": \"@AmazonHelp Are there any ongoing promotions or deals in the ongoing festive season? Looking to buy a few items.\",\n",
    "            \"shipping\": \"@AmazonHelp Hi, My address has changed. Can you help me update the shipping address for my order?\",\n",
    "            \"technical support\": \"@AmazonHelp Encountering errors during checkout. Can you help me troubleshoot the issue?\",\n",
    "            \"payment issue\": \"@AmazonHelp My payment method isn't going through. Any suggestions on how to resolve this?\",\n",
    "            \"general query\": \"@AmazonHelp Hi, I have a general question regarding the product. Can you help me with this?\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra(current_tokenized_data, extra_tweets):\n",
    "    '''Adding extra tweets to the current tokenized data'''\n",
    "    \n",
    "    # Convert the extra tweets into a pandas Series\n",
    "    extra_tweets = pd.Series(extra_tweets)\n",
    "\n",
    "    # Convert the current tokenized data into a single string\n",
    "    print('Converting to string...')\n",
    "    string_processed_data = current_tokenized_data.progress_apply(\" \".join)\n",
    "\n",
    "    # Concatenate the extra tweets to the current data\n",
    "    string_processed_data = pd.concat([string_processed_data, extra_tweets], axis = 0)\n",
    "\n",
    "    # Tokenize the combined data\n",
    "    tknzr = TweetTokenizer(strip_handles = True, reduce_len = True)\n",
    "    print('Tokenizing...')\n",
    "    tokenized_data = string_processed_data.progress_apply(tknzr.tokenize)\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "# Add the extra tweets to the current data\n",
    "processed_inbound_extra = add_extra(processed['Processed Inbound'], list(ideal.values()))\n",
    "\n",
    "# Save the updated data to a pickle file\n",
    "processed_inbound_extra.to_pickle('objects/processed_inbound_extra.pkl')\n",
    "\n",
    "processed_inbound_extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's is the catch. Have intent buckets inplace already in place. You would need to supply this to the below function to get your top N tweets corresponding to the current tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping the intent to the row index \n",
    "intent_itags = {\n",
    "\n",
    "}\n",
    "\n",
    "def generate_intent(nsim, idx_tag): \n",
    "    '''Function that maps an index tag to an intent and returns nsim number of similar tweets'''\n",
    "    sim_docs = model.docvecs.most_similar(idx_tag, topn = nsim)\n",
    "    \n",
    "    # Getting just the indexes \n",
    "    indexes = [int(i[0]) for i in sim_docs]\n",
    "    \n",
    "    # Actually seeing the top 1000 tweets similar to 0th tweet which seems to be about updates \n",
    "    # print(processed_inbound_extra[indexes])\n",
    "    return indexes\n",
    "    \n",
    "# Create a dictionary mapping the intent to the row index of tweets\n",
    "index_intents = {}\n",
    "for intent, tag in intent_itags.items():\n",
    "    print('Intent: ', intent)\n",
    "    index_indents[intent] = generate_intent(1000, tag)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now map the index to each row of the preprocessed inbound data\n",
    "preprocessed_inbound[\"intent\"] = processed_inbounnd.index.map(index_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent classification with Keras \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
